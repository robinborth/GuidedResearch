{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_features: int = 64,\n",
    "        depth: int = 4,\n",
    "        size: int = 256,\n",
    "        mode: str = \"point\",  # \"point\", \"normal\", \"point_normal\"\n",
    "        max_weight: float = 100.0,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_weight = max_weight        \n",
    "\n",
    "        self.mode = mode\n",
    "        assert mode in [\"point\", \"normal\", \"point_normal\"]\n",
    "        out_channels = 1\n",
    "        in_channels = 6\n",
    "        if mode == \"point_normal\":\n",
    "            in_channels = 12        \n",
    "\n",
    "        self.depth = depth\n",
    "        self.size = size\n",
    "        features = init_features\n",
    "\n",
    "        # Contracting Path (Encoder)\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.encoders.append(UNet._block(in_channels, features))\n",
    "            self.pools.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            in_channels = features\n",
    "            features *= 2\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = UNet._block(features // 2, features)\n",
    "\n",
    "        # Expansive Path (Decoder)\n",
    "        self.upconvs = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            features //= 2\n",
    "            self.upconvs.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    features * 2,\n",
    "                    features,\n",
    "                    kernel_size=2,\n",
    "                    stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.decoders.append(UNet._block(features * 2, features))\n",
    "\n",
    "        # Final Convolution\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        s_point: torch.Tensor,\n",
    "        s_normal: torch.Tensor,\n",
    "        t_point: torch.Tensor,\n",
    "        t_normal: torch.Tensor,\n",
    "    ):\n",
    "        # prepare input\n",
    "        if self.mode == \"point_normal\":  # (B, H, W, 12)\n",
    "            x = torch.cat([s_point, s_normal, t_point, t_normal], dim=-1)\n",
    "        elif self.mode == \"point\":\n",
    "            x = torch.cat([s_point, t_point], dim=-1)  # (B, H, W, 6)\n",
    "        elif self.mode == \"normal\":\n",
    "            x = torch.cat([s_normal, t_normal], dim=-1)  # (B, H, W, 6)\n",
    "        else:\n",
    "            raise AttributeError(f\"No {self.mode} that works.\")\n",
    "        x = x.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "        B, C, H, W = x.shape \n",
    "        x = self._pad(x, height=H, width=W)\n",
    "\n",
    "        # B, H, W, C\n",
    "        encoders_output = []\n",
    "        for i in range(self.depth):\n",
    "            x = self.encoders[i](x)\n",
    "            encoders_output.append(x)\n",
    "            x = self.pools[i](x)\n",
    "\n",
    "        bottleneck = self.bottleneck(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            x = self.upconvs[i](bottleneck if i == 0 else x)\n",
    "            enc_output = encoders_output[-(i + 1)]\n",
    "            x = torch.cat((x, enc_output), dim=1)\n",
    "            x = self.decoders[i](x)\n",
    "        x = torch.exp(self.conv(x))\n",
    "        x = self._unpad(x, height=H, width=W)\n",
    "        x = x.permute(0, 2, 3, 1)  # (B, W, H, 1)\n",
    "\n",
    "        return x \n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels: int, features: int):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=features,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=features,\n",
    "                out_channels=features,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def _pad(self, x: torch.Tensor, height: int, width: int):\n",
    "        # Desired output dimensions\n",
    "        target_height = self.size\n",
    "        target_width = self.size\n",
    "\n",
    "        # Calculate padding for height and width\n",
    "        pad_height = target_height - height\n",
    "        pad_width = target_width - width\n",
    "\n",
    "        # Pad equally on both sides\n",
    "        padding = [\n",
    "            pad_width // 2,\n",
    "            pad_width - pad_width // 2,\n",
    "            pad_height // 2,\n",
    "            pad_height - pad_height // 2,\n",
    "        ]  # (left, right, top, bottom)\n",
    "\n",
    "        # Apply padding\n",
    "        return F.pad(x, padding)\n",
    "\n",
    "    def _unpad(self, x: torch.Tensor, height: int, width: int):\n",
    "        # Desired output dimensions\n",
    "        target_height = self.size\n",
    "        target_width = self.size\n",
    "\n",
    "        # Calculate padding for height and width\n",
    "        pad_height = target_height - height\n",
    "        pad_width = target_width - width\n",
    "\n",
    "        # Slice back to the original shape (135, 240)\n",
    "        start_height = pad_height // 2\n",
    "        end_height = start_height + height \n",
    "\n",
    "        start_width = pad_width // 2\n",
    "        end_width = start_width +  width\n",
    "\n",
    "        return x[:, :, start_height:end_height, start_width:end_width]\n",
    "\n",
    "\n",
    "model = UNet(init_features=32, depth=4, mode=\"point_normal\", device=\"cpu\")\n",
    "# print(unet)\n",
    "\n",
    "# Example input tensor (batch_size, channels, height, width)\n",
    "s_point = torch.zeros((1, 135, 240, 3))\n",
    "t_point = torch.zeros((1, 135, 240, 3))\n",
    "s_normal = torch.zeros((1, 135, 240, 3))\n",
    "t_normal = torch.zeros((1, 135, 240, 3))\n",
    "out = model(\n",
    "    s_point =s_point,\n",
    "    t_point=t_point,\n",
    "    s_normal=s_normal,\n",
    "    t_normal=t_normal,\n",
    ")\n",
    "out[\"bottleneck\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of usage\n",
    "model = UNet(in_channels=1, out_channels=1, init_features=32)\n",
    "# print(model)\n",
    "\n",
    "# Example input tensor (batch_size, channels, height, width)\n",
    "x = torch.zeros((1, 1, 135, 240))\n",
    "out = model(x)\n",
    "out[\"weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input tensor of shape (batch_size, channels, height, width)\n",
    "x = torch.randn((1, 1, 135, 240), requires_grad=True)\n",
    "\n",
    "\n",
    "# Desired output dimensions\n",
    "target_height = 256\n",
    "target_width = 256\n",
    "\n",
    "# Calculate padding for height and width\n",
    "pad_height = target_height - x.shape[2]\n",
    "pad_width = target_width - x.shape[3]\n",
    "\n",
    "# Pad equally on both sides\n",
    "padding = [pad_width // 2, pad_width - pad_width // 2, pad_height // 2, pad_height - pad_height // 2]  # (left, right, top, bottom)\n",
    "\n",
    "# Apply padding\n",
    "x_padded = F.pad(x, padding)\n",
    "\n",
    "print(\"Padded shape:\", x_padded.shape)  # Should be (1, 1, 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input tensor of shape (batch_size, channels, height, width)\n",
    "x = torch.randn((1, 1, 135, 240), requires_grad=True)\n",
    "\n",
    "# Desired output dimensions\n",
    "target_height = 256\n",
    "target_width = 256\n",
    "\n",
    "# Calculate padding for height and width\n",
    "pad_height = target_height - x.shape[2]\n",
    "pad_width = target_width - x.shape[3]\n",
    "\n",
    "# Pad equally on both sides\n",
    "padding = [pad_width // 2, pad_width - pad_width // 2, pad_height // 2, pad_height - pad_height // 2]  # (left, right, top, bottom)\n",
    "\n",
    "# Apply padding\n",
    "x_padded = F.pad(x, padding)\n",
    "\n",
    "print(\"Padded shape:\", x_padded.shape)  # Should be (1, 1, 256, 256)\n",
    "\n",
    "# Slice back to the original shape (135, 240)\n",
    "start_height = pad_height // 2\n",
    "end_height = start_height + x.shape[2]\n",
    "\n",
    "start_width = pad_width // 2\n",
    "end_width = start_width + x.shape[3]\n",
    "\n",
    "x_original_shape = x_padded[:, :, start_height:end_height, start_width:end_width]\n",
    "\n",
    "print(\"Shape after slicing:\", x_original_shape.shape)  # Should be (1, 1, 135, 240)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_original_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "xs = []\n",
    "for i in range(120):\n",
    "    x = torch.load(f\"/home/borth/GuidedResearch/data/dphm_kinect/christoph_mouthmove/params/{i:05}.pt\")\n",
    "    xs.append(x[\"transl\"])\n",
    "xs = torch.cat(xs)\n",
    "(xs[1:] - xs[:-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.optimizer.solver import PytorchCholeskySolver, LinearSystemSolver, PytorchSolver, PytorchLSTSQSolver\n",
    "\n",
    "x = torch.load(\"/home/borth/GuidedResearch/temp/_tracked_innocenzo_fulgintl_rotatemouth/linsys/0000000.pt\")\n",
    "A = x[\"A\"].requires_grad_(True)\n",
    "b = x[\"b\"].requires_grad_(True)\n",
    "solver = PytorchLSTSQSolver()\n",
    "x_gt, _ = solver(A, b)\n",
    "x_gt.mean().backward()\n",
    "print(A.grad.mean(), A.grad.median(), A.grad.max(), A.grad.min())\n",
    "print(b.grad.mean(), b.grad.median(), b.grad.max(), b.grad.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.cond(A + torch.eye(A.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.cond(A + 1.0 * torch.diag_embed(A.diag()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import time\n",
    "\n",
    "# Define the directory containing the images\n",
    "color_dir = Path(\"/home/borth/GuidedResearch/data/dphm_kinect/ali_kocal_eyeblink/color\")\n",
    "\n",
    "# List all the .png files in the directory\n",
    "image_paths = sorted([p for p in color_dir.iterdir() if str(p).endswith('.png')])\n",
    "imgs = [Image.open(image_path) for image_path in image_paths]\n",
    "\n",
    "# Iterate through each image, load it with PIL, and display it like a video\n",
    "for img in imgs:\n",
    "    # Clear the previous output\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # Display the image in the notebook\n",
    "    display.display(img)\n",
    "    \n",
    "    # # Pause briefly to simulate frame rate (e.g., 24 frames per second)\n",
    "    # time.sleep(1/24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import time\n",
    "\n",
    "# Define the directory containing the images\n",
    "color_dir = Path(\"/home/borth/GuidedResearch/data/dphm_kinect/ali_kocal_eyeblink/color\")\n",
    "\n",
    "# List all the .png files in the directory\n",
    "image_paths = sorted([p for p in color_dir.iterdir() if str(p).endswith('.png')])\n",
    "\n",
    "# Set the scaling factor (4 times smaller)\n",
    "scale_factor = 4\n",
    "\n",
    "# Iterate through each image, load it with PIL, rescale, and display it like a video\n",
    "imgs = []\n",
    "for image_path in image_paths:\n",
    "    # Load image using PIL\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Get original image size and compute new size (scaled by factor of 4)\n",
    "    new_size = (img.width // scale_factor, img.height // scale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    img_resized = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    imgs.append(img_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in imgs:\n",
    "    # Clear the previous output\n",
    "    display.clear_output(wait=True)\n",
    "    # Display the resized image\n",
    "    display.display(img)\n",
    "    # Pause briefly to simulate frame rate (e.g., 24 frames per second)\n",
    "    time.sleep(1/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.utils.visualize import load_pcd\n",
    "import open3d as o3d\n",
    "\n",
    "x = torch.load(\"/home/borth/GuidedResearch/data/debug_synthetic/s00000/vertices/00000.pt\")\n",
    "p = load_pcd(x.reshape(-1, 3))\n",
    "o3d.visualization.draw_plotly([p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"/home/borth/GuidedResearch/data/dphm_kinect/ali_kocal_mouthmove/landmark/00000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.model.flame.flame import Flame\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.rasterizer import Rasterizer\n",
    "from lib.renderer.renderer import Renderer\n",
    "from lib.renderer.camera import Camera\n",
    "\n",
    "# settings\n",
    "data_dir = \"/home/borth/GuidedResearch/data/dphm_kinect/christoph_mouthmove\"\n",
    "flame_dir = \"/home/borth/GuidedResearch/checkpoints/flame2023_no_jaw\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# setup camera, rasterizer and renderer\n",
    "K = load_intrinsics(data_dir=data_dir, return_tensor=\"pt\")\n",
    "camera = Camera(K=K, width=1920, height=1080, scale=1)\n",
    "rasterizer = Rasterizer(width=camera.width, height=camera.height)\n",
    "renderer = Renderer(rasterizer=rasterizer, camera=camera)\n",
    "\n",
    "# setup flame optimizer\n",
    "flame = Flame(\n",
    "    flame_dir=flame_dir,\n",
    "    vertices_mask=\"full\",\n",
    "    expression_params=50,\n",
    "    shape_params=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.config import load_config\n",
    "from lib.optimizer.framework import OptimizerFramework, NeuralOptimizer\n",
    "from lib.data.loader import load_intrinsics\n",
    "import hydra\n",
    "import torch\n",
    "from lib.model.flame.flame import Flame\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.rasterizer import Rasterizer\n",
    "from lib.renderer.renderer import Renderer\n",
    "from lib.renderer.camera import Camera\n",
    "\n",
    "cfg = load_config(\"train\")\n",
    "K = load_intrinsics(data_dir=cfg.data.intrinsics_dir, return_tensor=\"pt\")\n",
    "camera = Camera(\n",
    "    K=K,\n",
    "    width=cfg.data.width,\n",
    "    height=cfg.data.height,\n",
    "    near=cfg.data.near,\n",
    "    far=cfg.data.far,\n",
    "    scale=cfg.data.scale,\n",
    ")\n",
    "rasterizer = Rasterizer(width=camera.width, height=camera.height)\n",
    "renderer = Renderer(rasterizer=rasterizer, camera=camera)\n",
    "flame = hydra.utils.instantiate(cfg.model)\n",
    "\n",
    "datamodule = hydra.utils.instantiate(cfg.data, renderer=renderer)\n",
    "datamodule.setup(\"fit\")\n",
    "dataloader = datamodule.train_dataloader()\n",
    "\n",
    "path = \"/home/borth/GuidedResearch/logs/2024-09-25/16-42-40_train/checkpoints/last.ckpt\"\n",
    "optimizer = NeuralOptimizer.load_from_checkpoint(path, renderer=renderer, flame=flame)\n",
    "batch = next(iter(dataloader))\n",
    "batch = optimizer.transfer_batch_to_device(batch, \"cuda\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = optimizer(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
