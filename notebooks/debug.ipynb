{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 16, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_features: int = 64,\n",
    "        depth: int = 4,\n",
    "        size: int = 256,\n",
    "        mode: str = \"point\",  # \"point\", \"normal\", \"point_normal\"\n",
    "        max_weight: float = 100.0,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_weight = max_weight        \n",
    "\n",
    "        self.mode = mode\n",
    "        assert mode in [\"point\", \"normal\", \"point_normal\"]\n",
    "        out_channels = 1\n",
    "        in_channels = 6\n",
    "        if mode == \"point_normal\":\n",
    "            in_channels = 12        \n",
    "\n",
    "        self.depth = depth\n",
    "        self.size = size\n",
    "        features = init_features\n",
    "\n",
    "        # Contracting Path (Encoder)\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.encoders.append(UNet._block(in_channels, features))\n",
    "            self.pools.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            in_channels = features\n",
    "            features *= 2\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = UNet._block(features // 2, features)\n",
    "\n",
    "        # Expansive Path (Decoder)\n",
    "        self.upconvs = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            features //= 2\n",
    "            self.upconvs.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    features * 2,\n",
    "                    features,\n",
    "                    kernel_size=2,\n",
    "                    stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.decoders.append(UNet._block(features * 2, features))\n",
    "\n",
    "        # Final Convolution\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        s_point: torch.Tensor,\n",
    "        s_normal: torch.Tensor,\n",
    "        t_point: torch.Tensor,\n",
    "        t_normal: torch.Tensor,\n",
    "    ):\n",
    "        # prepare input\n",
    "        if self.mode == \"point_normal\":  # (B, H, W, 12)\n",
    "            x = torch.cat([s_point, s_normal, t_point, t_normal], dim=-1)\n",
    "        elif self.mode == \"point\":\n",
    "            x = torch.cat([s_point, t_point], dim=-1)  # (B, H, W, 6)\n",
    "        elif self.mode == \"normal\":\n",
    "            x = torch.cat([s_normal, t_normal], dim=-1)  # (B, H, W, 6)\n",
    "        else:\n",
    "            raise AttributeError(f\"No {self.mode} that works.\")\n",
    "        x = x.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "        B, C, H, W = x.shape \n",
    "        x = self._pad(x, height=H, width=W)\n",
    "\n",
    "        # B, H, W, C\n",
    "        encoders_output = []\n",
    "        for i in range(self.depth):\n",
    "            x = self.encoders[i](x)\n",
    "            encoders_output.append(x)\n",
    "            x = self.pools[i](x)\n",
    "\n",
    "        bottleneck = self.bottleneck(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            x = self.upconvs[i](bottleneck if i == 0 else x)\n",
    "            enc_output = encoders_output[-(i + 1)]\n",
    "            x = torch.cat((x, enc_output), dim=1)\n",
    "            x = self.decoders[i](x)\n",
    "        x = torch.exp(self.conv(x))\n",
    "        x = self._unpad(x, height=H, width=W)\n",
    "        x = x.permute(0, 2, 3, 1)  # (B, W, H, 1)\n",
    "\n",
    "        return x \n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels: int, features: int):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=features,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=features,\n",
    "                out_channels=features,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def _pad(self, x: torch.Tensor, height: int, width: int):\n",
    "        # Desired output dimensions\n",
    "        target_height = self.size\n",
    "        target_width = self.size\n",
    "\n",
    "        # Calculate padding for height and width\n",
    "        pad_height = target_height - height\n",
    "        pad_width = target_width - width\n",
    "\n",
    "        # Pad equally on both sides\n",
    "        padding = [\n",
    "            pad_width // 2,\n",
    "            pad_width - pad_width // 2,\n",
    "            pad_height // 2,\n",
    "            pad_height - pad_height // 2,\n",
    "        ]  # (left, right, top, bottom)\n",
    "\n",
    "        # Apply padding\n",
    "        return F.pad(x, padding)\n",
    "\n",
    "    def _unpad(self, x: torch.Tensor, height: int, width: int):\n",
    "        # Desired output dimensions\n",
    "        target_height = self.size\n",
    "        target_width = self.size\n",
    "\n",
    "        # Calculate padding for height and width\n",
    "        pad_height = target_height - height\n",
    "        pad_width = target_width - width\n",
    "\n",
    "        # Slice back to the original shape (135, 240)\n",
    "        start_height = pad_height // 2\n",
    "        end_height = start_height + height \n",
    "\n",
    "        start_width = pad_width // 2\n",
    "        end_width = start_width +  width\n",
    "\n",
    "        return x[:, :, start_height:end_height, start_width:end_width]\n",
    "\n",
    "\n",
    "model = UNet(init_features=32, depth=4, mode=\"point_normal\", device=\"cpu\")\n",
    "# print(unet)\n",
    "\n",
    "# Example input tensor (batch_size, channels, height, width)\n",
    "s_point = torch.zeros((1, 135, 240, 3))\n",
    "t_point = torch.zeros((1, 135, 240, 3))\n",
    "s_normal = torch.zeros((1, 135, 240, 3))\n",
    "t_normal = torch.zeros((1, 135, 240, 3))\n",
    "out = model(\n",
    "    s_point =s_point,\n",
    "    t_point=t_point,\n",
    "    s_normal=s_normal,\n",
    "    t_normal=t_normal,\n",
    ")\n",
    "out[\"bottleneck\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256, 256])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of usage\n",
    "model = UNet(in_channels=1, out_channels=1, init_features=32)\n",
    "# print(model)\n",
    "\n",
    "# Example input tensor (batch_size, channels, height, width)\n",
    "x = torch.zeros((1, 1, 135, 240))\n",
    "out = model(x)\n",
    "out[\"weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded shape: torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input tensor of shape (batch_size, channels, height, width)\n",
    "x = torch.randn((1, 1, 135, 240), requires_grad=True)\n",
    "\n",
    "\n",
    "# Desired output dimensions\n",
    "target_height = 256\n",
    "target_width = 256\n",
    "\n",
    "# Calculate padding for height and width\n",
    "pad_height = target_height - x.shape[2]\n",
    "pad_width = target_width - x.shape[3]\n",
    "\n",
    "# Pad equally on both sides\n",
    "padding = [pad_width // 2, pad_width - pad_width // 2, pad_height // 2, pad_height - pad_height // 2]  # (left, right, top, bottom)\n",
    "\n",
    "# Apply padding\n",
    "x_padded = F.pad(x, padding)\n",
    "\n",
    "print(\"Padded shape:\", x_padded.shape)  # Should be (1, 1, 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded shape: torch.Size([1, 1, 256, 256])\n",
      "Shape after slicing: torch.Size([1, 1, 135, 240])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input tensor of shape (batch_size, channels, height, width)\n",
    "x = torch.randn((1, 1, 135, 240), requires_grad=True)\n",
    "\n",
    "# Desired output dimensions\n",
    "target_height = 256\n",
    "target_width = 256\n",
    "\n",
    "# Calculate padding for height and width\n",
    "pad_height = target_height - x.shape[2]\n",
    "pad_width = target_width - x.shape[3]\n",
    "\n",
    "# Pad equally on both sides\n",
    "padding = [pad_width // 2, pad_width - pad_width // 2, pad_height // 2, pad_height - pad_height // 2]  # (left, right, top, bottom)\n",
    "\n",
    "# Apply padding\n",
    "x_padded = F.pad(x, padding)\n",
    "\n",
    "print(\"Padded shape:\", x_padded.shape)  # Should be (1, 1, 256, 256)\n",
    "\n",
    "# Slice back to the original shape (135, 240)\n",
    "start_height = pad_height // 2\n",
    "end_height = start_height + x.shape[2]\n",
    "\n",
    "start_width = pad_width // 2\n",
    "end_width = start_width + x.shape[3]\n",
    "\n",
    "x_original_shape = x_padded[:, :, start_height:end_height, start_width:end_width]\n",
    "\n",
    "print(\"Shape after slicing:\", x_original_shape.shape)  # Should be (1, 1, 135, 240)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0182, -0.2634,  1.2838,  ..., -0.5184,  0.2893,  0.1558],\n",
       "          [-0.3295, -0.1763, -0.4001,  ..., -0.4550, -0.6738, -0.6903],\n",
       "          [-0.5878, -0.4613, -0.0079,  ...,  1.0670, -1.4411,  0.4294],\n",
       "          ...,\n",
       "          [-0.2986,  1.8018, -0.8441,  ..., -1.4393,  1.9679,  0.3780],\n",
       "          [-0.7267, -1.0729, -0.5690,  ..., -0.0377,  1.1575, -0.7883],\n",
       "          [-0.6154,  0.8982, -1.1257,  ...,  0.6682,  1.5402,  0.4965]]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_original_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0182, -0.2634,  1.2838,  ..., -0.5184,  0.2893,  0.1558],\n",
       "          [-0.3295, -0.1763, -0.4001,  ..., -0.4550, -0.6738, -0.6903],\n",
       "          [-0.5878, -0.4613, -0.0079,  ...,  1.0670, -1.4411,  0.4294],\n",
       "          ...,\n",
       "          [-0.2986,  1.8018, -0.8441,  ..., -1.4393,  1.9679,  0.3780],\n",
       "          [-0.7267, -1.0729, -0.5690,  ..., -0.0377,  1.1575, -0.7883],\n",
       "          [-0.6154,  0.8982, -1.1257,  ...,  0.6682,  1.5402,  0.4965]]]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2209e-06)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "xs = []\n",
    "for i in range(120):\n",
    "    x = torch.load(f\"/home/borth/GuidedResearch/data/dphm_kinect/christoph_mouthmove/params/{i:05}.pt\")\n",
    "    xs.append(x[\"transl\"])\n",
    "xs = torch.cat(xs)\n",
    "(xs[1:] - xs[:-1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1526)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.5607e-07) tensor(3.2350e-07) tensor(0.0004) tensor(-0.0004)\n",
      "tensor(0.0004) tensor(-1.6172e-06) tensor(0.0100) tensor(-0.0008)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from lib.optimizer.solver import PytorchCholeskySolver, LinearSystemSolver, PytorchSolver, PytorchLSTSQSolver\n",
    "\n",
    "x = torch.load(\"/home/borth/GuidedResearch/temp/_tracked_innocenzo_fulgintl_rotatemouth/linsys/0000000.pt\")\n",
    "A = x[\"A\"].requires_grad_(True)\n",
    "b = x[\"b\"].requires_grad_(True)\n",
    "solver = PytorchLSTSQSolver()\n",
    "x_gt, _ = solver(A, b)\n",
    "x_gt.mean().backward()\n",
    "print(A.grad.mean(), A.grad.median(), A.grad.max(), A.grad.min())\n",
    "print(b.grad.mean(), b.grad.median(), b.grad.max(), b.grad.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.2022e-02, -1.6498e-02,  1.6422e-02,  8.3287e-03,  1.8885e-01,\n",
       "        -1.1039e-01, -2.6702e-01,  5.2789e-02, -9.4564e-02, -1.7879e-01,\n",
       "        -2.8307e-02, -2.4503e-01, -2.4458e-01,  6.2571e-02, -2.1742e-01,\n",
       "         5.7680e-02, -3.0225e-01, -1.0088e-01, -1.5052e-03, -8.2054e-03,\n",
       "        -3.3691e-01, -2.4137e-02, -1.6685e-01,  5.2742e-01, -1.1675e-01,\n",
       "        -4.3037e-01,  4.1128e-01, -1.2757e-01, -4.6779e-01, -2.1549e-01,\n",
       "        -2.5377e-01,  2.5678e-01,  1.4873e-01,  2.8678e-01,  5.6243e-02,\n",
       "         1.8464e-02, -2.5776e-03, -2.2784e-01, -5.6792e-03, -5.7189e-01,\n",
       "         2.1492e-01, -1.1935e-01, -2.1601e-01, -2.4235e-01,  3.4376e-01,\n",
       "         2.6661e-01, -1.9733e-01,  4.8864e-03,  1.6744e-01,  1.0434e-01,\n",
       "        -3.0153e-04,  3.2145e-03,  3.1760e-03,  1.7339e-04,  3.7922e-05,\n",
       "         7.3675e-05, -2.1500e-04, -1.6379e-03, -5.2934e-03],\n",
       "       grad_fn=<LinalgSolveExBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(59782876., grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.1731e-03,  1.1354e-03, -3.3658e-05,  ...,  1.0128e-01,\n",
       "         -5.0421e-03, -5.1304e-03],\n",
       "        [ 1.1354e-03,  2.1057e-03, -6.0995e-05,  ...,  1.2770e-02,\n",
       "          1.9029e-03,  1.7827e-04],\n",
       "        [-3.3658e-05, -6.0995e-05,  2.6531e-04,  ..., -6.9914e-04,\n",
       "         -6.3254e-03, -5.7062e-03],\n",
       "        ...,\n",
       "        [ 1.0128e-01,  1.2770e-02, -6.9914e-04,  ...,  5.1271e+00,\n",
       "         -9.5530e-02, -3.2283e-01],\n",
       "        [-5.0421e-03,  1.9029e-03, -6.3254e-03,  ..., -9.5530e-02,\n",
       "          2.2648e+00,  1.0380e-01],\n",
       "        [-5.1304e-03,  1.7827e-04, -5.7062e-03,  ..., -3.2283e-01,\n",
       "          1.0380e-01,  1.9050e+00]], requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.1731e-03, 2.1057e-03, 2.6531e-04, 9.0030e-04, 4.0633e-04, 4.1603e-04,\n",
       "        1.4641e-04, 3.8512e-04, 1.4818e-04, 1.8286e-04, 2.0913e-04, 1.7092e-04,\n",
       "        6.2570e-05, 2.0327e-04, 8.8231e-05, 1.3383e-04, 1.7895e-04, 5.1430e-05,\n",
       "        1.7467e-04, 7.6409e-05, 5.9546e-05, 5.3258e-05, 3.6550e-05, 6.1496e-05,\n",
       "        4.3800e-05, 3.9916e-05, 3.2244e-05, 3.4983e-05, 2.7346e-05, 3.5999e-05,\n",
       "        3.9655e-05, 2.7206e-05, 4.1970e-05, 3.4752e-05, 3.0459e-05, 4.0527e-05,\n",
       "        4.2741e-05, 2.3114e-05, 2.0822e-05, 3.6277e-05, 2.9260e-05, 2.3219e-05,\n",
       "        2.9221e-05, 2.6685e-05, 2.1644e-05, 2.3351e-05, 1.9196e-05, 2.4129e-05,\n",
       "        1.9122e-05, 1.9676e-05, 4.0554e+01, 7.4488e+00, 2.3953e+01, 1.1929e+03,\n",
       "        8.0555e+02, 3.1684e+03, 5.1271e+00, 2.2648e+00, 1.9050e+00],\n",
       "       grad_fn=<DiagonalBackward0_copy>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3369.5950, grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.cond(A + torch.eye(A.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(59782876., grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(51664336., grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.cond(A + 1.0 * torch.diag_embed(A.diag()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
