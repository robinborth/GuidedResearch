{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hydra\n",
    "from lib.utils.config import load_config\n",
    "from lib.model.flame import FLAME\n",
    "from lib.data.datamodule import DPHMDataModule\n",
    "from lib.trainer.logger import FlameLogger\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.rasterizer import Rasterizer\n",
    "from lib.renderer.camera import Camera\n",
    "from lib.utils.config import set_configs\n",
    "\n",
    "cfg = load_config(\n",
    "    \"optimize\",\n",
    "    overrides=[\n",
    "        \"optimizer=gauss_newton\",\n",
    "        \"joint_trainer.init_idxs=[0]\",\n",
    "        \"joint_trainer.max_iters=1\",\n",
    "        \"joint_trainer.max_optims=1\",\n",
    "        \"joint_trainer.scheduler.milestones=[0]\",\n",
    "        \"joint_trainer.scheduler.params=[[global_pose,transl]]\",\n",
    "        \"joint_trainer.coarse2fine.milestones=[0]\",\n",
    "        \"joint_trainer.coarse2fine.scales=[8]\",\n",
    "        \"sequential_trainer=null\",\n",
    "    ],\n",
    ")\n",
    "cfg = set_configs(cfg)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "K = load_intrinsics(data_dir=cfg.data.data_dir, return_tensor=\"pt\")\n",
    "camera = Camera(\n",
    "    K=K,\n",
    "    width=cfg.data.width,\n",
    "    height=cfg.data.height,\n",
    "    near=cfg.data.near,\n",
    "    far=cfg.data.far,\n",
    ")\n",
    "rasterizer = Rasterizer(width=camera.width, height=camera.height)\n",
    "datamodule: DPHMDataModule = hydra.utils.instantiate(cfg.data, devie=device)\n",
    "# logger: FlameLogger = hydra.utils.instantiate(cfg.logger)\n",
    "model: FLAME = hydra.utils.instantiate(cfg.model).to(device)\n",
    "# coarse2fine = hydra.utils.instantiate(cfg.joint_trainer.coarse2fine)\n",
    "# scheduler = hydra.utils.instantiate(cfg.joint_trainer.scheduler)\n",
    "# optimizer = hydra.utils.instantiate(cfg.optimizer)\n",
    "\n",
    "# datamodule.setup()\n",
    "model.init_renderer(camera=camera, rasterizer=rasterizer)\n",
    "# coarse2fine.init_scheduler(camera=camera, rasterizer=rasterizer)\n",
    "# model.init_logger(logger=logger)\n",
    "# optimizer.init_logger(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.mesh import vertex_normals\n",
    "\n",
    "out = model()\n",
    "vertices = out[\"vertices\"]\n",
    "faces = model.faces.data\n",
    "fragments = model.renderer.rasterize(vertices, faces)\n",
    "\n",
    "\n",
    "def infer():\n",
    "    model()\n",
    "\n",
    "\n",
    "def normals(_vertices, _faces):\n",
    "    vertex_normals(_vertices, _faces)\n",
    "\n",
    "\n",
    "def interpolate(\n",
    "    vertices_idx: torch.Tensor,\n",
    "    bary_coords: torch.Tensor,\n",
    "    attributes: torch.Tensor,\n",
    "):\n",
    "    model.renderer.interpolate(vertices_idx, bary_coords, attributes)\n",
    "\n",
    "\n",
    "def timed(fn):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    result = fn()\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return result, start.elapsed_time(end)\n",
    "\n",
    "\n",
    "infer_opt = torch.compile(infer, mode=\"default\")\n",
    "normals_opt = torch.compile(normals, mode=\"default\")\n",
    "interpolate_opt = torch.compile(interpolate, mode=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.mesh import vertex_normals\n",
    "\n",
    "# vertex_normals(torch.rand((1, 5023, 3)).cuda(), torch.randn((9976, 3)).cuda())\n",
    "vertex_normals(vertices, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.trainer.timer import TimeTracker\n",
    "\n",
    "N_ITER = 100\n",
    "tracker = TimeTracker()\n",
    "for _ in range(N_ITER):\n",
    "    tracker.start(\"model_inference\")\n",
    "    interpolate(\n",
    "        vertices_idx=fragments.vertices_idx,\n",
    "        bary_coords=fragments.bary_coords,\n",
    "        attributes=vertices,\n",
    "    )\n",
    "    tracker.stop()\n",
    "for _ in range(N_ITER):\n",
    "    tracker.start(\"model_opt_inference\")\n",
    "    interpolate_opt(\n",
    "        vertices_idx=fragments.vertices_idx,\n",
    "        bary_coords=fragments.bary_coords,\n",
    "        attributes=vertices,\n",
    "    )\n",
    "    tracker.stop()\n",
    "print(tracker.print_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    timed(\n",
    "        lambda: interpolate(\n",
    "            vertices_idx=fragments.vertices_idx,\n",
    "            bary_coords=fragments.bary_coords,\n",
    "            attributes=vertices,\n",
    "        )\n",
    "    )[1]\n",
    ")\n",
    "print(\n",
    "    timed(\n",
    "        lambda: interpolate_opt(\n",
    "            vertices_idx=fragments.vertices_idx,\n",
    "            bary_coords=fragments.bary_coords,\n",
    "            attributes=vertices,\n",
    "        )\n",
    "    )[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timed(lambda: normals(vertices, faces))[1])\n",
    "print(timed(lambda: normals_opt(vertices, faces))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch single batch\n",
    "iter_step = 0\n",
    "c2fs.schedule_dataset(datamodule=datamodule, iter_step=iter_step)\n",
    "fts.param_groups(model, iter_step=iter_step)\n",
    "dataloader = datamodule.train_dataloader()\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.configure_optimizer(\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    batch=batch,\n",
    "    iter_step=iter_step,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "signature = inspect.signature(model.forward)\n",
    "param_names = [param.name for param in signature.parameters.values()]\n",
    "param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# n = 100\n",
    "A = torch.rand((700, 700))\n",
    "B = torch.rand(700)\n",
    "X = torch.linalg.solve(A, B)\n",
    "x = torch.zeros((700), requires_grad=True)\n",
    "# def foo(x):\n",
    "#     return (A @ x - B)\n",
    "# J = torch.autograd.functional.jacobian(foo, x)\n",
    "# J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.rand(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "\n",
    "def exp_reducer(x):\n",
    "    return x.exp().sum(dim=1)\n",
    "\n",
    "\n",
    "# jacobian(exp_reducer, inputs, strategy=\"forward-mode\", vectorize=True)\n",
    "jacobian(\n",
    "    exp_reducer, inputs, strategy=\"reverse-mode\", vectorize=True, create_graph=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(2, 2)\n",
    "inputs.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import jacrev, vmap, jacfwd\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return x.sin().sum(dim=-1)\n",
    "\n",
    "\n",
    "v = vmap(jacrev(torch.exp))(inputs)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian(exp_reducer, inputs, strategy=\"forward-mode\", vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "max_steps = 100000\n",
    "x = torch.zeros((n), requires_grad=True)\n",
    "# optimizer = torch.optim.Adam([x], lr=1.0) \n",
    "optimizer = torch.optim.LBFGS([x) \n",
    "\n",
    "for step in tqdm(range(max_steps)):\n",
    "    optimizer.zero_grad()\n",
    "    F = A @ x - B\n",
    "    loss = torch.pow(F, 2).sum()\n",
    "    # print(f\"{step}) {loss}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f\"{step}) {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.inverse() @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A.T @ A).inverse() @ (A.T @ B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.solve(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([6.0, 4.0], requires_grad=True)\n",
    "Q = 3 * a**3 - b**2\n",
    "external_grad = torch.tensor([1.0, 1.0])\n",
    "Q.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5, requires_grad=True)\n",
    "b = 2 * a\n",
    "c = b**2  # replace this with c = b + 2 and the autograd error will go away\n",
    "print(b._version)\n",
    "b = b + 1\n",
    "print(b._version)\n",
    "b += 1  # inplace operation!\n",
    "print(b._version)\n",
    "# c.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
