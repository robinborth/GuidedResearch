{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hydra\n",
    "from lib.utils.config import load_config\n",
    "from lib.model.flame import FLAME\n",
    "from lib.data.datamodule import DPHMDataModule\n",
    "from lib.data.scheduler import CoarseToFineScheduler, FinetuneScheduler\n",
    "from lib.model.flame import FLAME\n",
    "from lib.model.logger import FlameLogger\n",
    "from lib.model.loss import calculate_point2plane\n",
    "\n",
    "cfg = load_config(\n",
    "    \"optimize\",\n",
    "    overrides=[\n",
    "        \"model.init_mode=kinect\",\n",
    "        \"model.vertices_mask=full\",\n",
    "        \"model.optimize_frames=10\",\n",
    "        \"data=kinect\",\n",
    "        \"data.scale=8\",\n",
    "        \"data.batch_size=10\",\n",
    "        \"data.start_frame_idx=19\",\n",
    "    ],\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "datamodule: DPHMDataModule = hydra.utils.instantiate(cfg.data, devie=device)\n",
    "datamodule.setup()  # this now contains the intrinsics, camera and rasterizer\n",
    "model: FLAME = hydra.utils.instantiate(cfg.model) \n",
    "model.init_renderer(camera=datamodule.camera, rasterizer=datamodule.rasterizer)\n",
    "model = model.to(device)\n",
    "fts: FinetuneScheduler = hydra.utils.instantiate(cfg.scheduler.finetune)\n",
    "c2fs: CoarseToFineScheduler = hydra.utils.instantiate(cfg.scheduler.coarse2fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch single batch\n",
    "iter_step = 0\n",
    "c2fs.schedule_dataset(datamodule=datamodule, iter_step=iter_step)\n",
    "fts.param_groups(model, iter_step=iter_step)\n",
    "dataloader = datamodule.train_dataloader()\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts.state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "signature = inspect.signature(model.forward)\n",
    "param_names = [param.name for param in signature.parameters.values()]\n",
    "param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# n = 100\n",
    "A = torch.rand((700, 700))\n",
    "B = torch.rand(700)\n",
    "X = torch.linalg.solve(A, B)\n",
    "x = torch.zeros((700), requires_grad=True)\n",
    "# def foo(x):\n",
    "#     return (A @ x - B)\n",
    "# J = torch.autograd.functional.jacobian(foo, x)\n",
    "# J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_B = B.unsqueeze(-1)\n",
    "(1/_B).T @ (A @ _B).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "max_steps = 100000\n",
    "x = torch.zeros((n), requires_grad=True)\n",
    "# optimizer = torch.optim.Adam([x], lr=1.0) \n",
    "optimizer = torch.optim.LBFGS([x) \n",
    "\n",
    "for step in tqdm(range(max_steps)):\n",
    "    optimizer.zero_grad()\n",
    "    F = A @ x - B\n",
    "    loss = torch.pow(F, 2).sum()\n",
    "    # print(f\"{step}) {loss}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(f\"{step}) {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.inverse() @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A.T @ A).inverse() @ (A.T @ B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.solve(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = 3*a**3 - b**2\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5, requires_grad=True)\n",
    "b = 2 * a\n",
    "c = b ** 2  # replace this with c = b + 2 and the autograd error will go away\n",
    "print(b._version)\n",
    "b = b + 1\n",
    "print(b._version)\n",
    "b += 1  # inplace operation!\n",
    "print(b._version)\n",
    "# c.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
