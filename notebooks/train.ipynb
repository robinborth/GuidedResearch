{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model.flame.flame import Flame\n",
    "import torch\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.rasterizer import Rasterizer\n",
    "from lib.renderer.renderer import Renderer\n",
    "from lib.renderer.camera import Camera\n",
    "from lib.model.weighting import ResidualWeightModule, DummyWeightModule\n",
    "from lib.model.correspondence import (\n",
    "    ProjectiveCorrespondenceModule,\n",
    "    OpticalFlowCorrespondenceModule,\n",
    ")\n",
    "from lib.utils.visualize import visualize_grid, visualize_params\n",
    "from lib.optimizer.residuals import Point2PlaneResiduals, VertexResiduals\n",
    "from lib.optimizer.newton import GaussNewton\n",
    "from lib.optimizer.framework import VertexOptimizer\n",
    "from lib.data.dataset import SplitDataset \n",
    "from torch.utils.data import DataLoader\n",
    "from lib.optimizer.solver import PytorchSolver\n",
    "\n",
    "# settings\n",
    "data_dir = \"/home/borth/GuidedResearch/data/dphm_christoph_mouthmove\"\n",
    "flame_dir = \"/home/borth/GuidedResearch/checkpoints/flame2023_no_jaw\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# setup camera, rasterizer and renderer\n",
    "K = load_intrinsics(data_dir=data_dir, return_tensor=\"pt\")\n",
    "camera = Camera(K=K, width=1920, height=1080, scale=8)\n",
    "rasterizer = Rasterizer(width=camera.width, height=camera.height)\n",
    "renderer = Renderer(rasterizer=rasterizer, camera=camera)\n",
    "\n",
    "# setup flame optimizer\n",
    "flame = Flame(\n",
    "    flame_dir=flame_dir,\n",
    "    vertices_mask=\"full\",\n",
    "    expression_params=50,\n",
    "    shape_params=100,\n",
    ")\n",
    "\n",
    "# creaste gt_params\n",
    "# gt_params = flame.generate_default_params()\n",
    "# gt_params[\"transl\"] = torch.tensor([[0.043, -0.003, -0.528]], device=device)\n",
    "\n",
    "# initlize params close to gt_params\n",
    "# params = flame.generate_default_params()\n",
    "# params[\"transl\"] = torch.tensor([[0.055, -0.01, -0.548]], device=device)\n",
    "# params[\"expression_params\"] = torch.rand((1, 50),device=device) * 2.0\n",
    "# params[\"global_pose\"] = torch.tensor([[-0.012, -0.018, -0.005]], device=device)\n",
    "# params[\"shape_params\"] = torch.rand((1, 100),device=device) * 0.5\n",
    "# params = flame.generate_default_params()\n",
    "# params[\"transl\"] = torch.tensor([[0.042, -0.001, -0.538]], device=device)\n",
    "\n",
    "params = torch.load(\"/home/borth/GuidedResearch/logs/2024-08-29_07-20-23_optimize/params/00000.pt\")\n",
    "params = {k: v.to(\"cuda\") for k,v in params.items()}\n",
    "\n",
    "# visualize_params(flame, renderer, gt_params, color=0)\n",
    "# visualize_params(flame, renderer, params, color=2)\n",
    "with torch.no_grad():\n",
    "    m_out = flame(**params)\n",
    "    out = flame.render(renderer=renderer, params=params)\n",
    "print(m_out[\"vertices\"].mean())\n",
    "print(params[\"shape_params\"].mean())\n",
    "print(params[\"expression_params\"].mean())\n",
    "print(params[\"transl\"].mean())\n",
    "print(params[\"global_pose\"].mean())\n",
    "print(params[\"neck_pose\"].mean())\n",
    "print(out[\"point\"].mean())\n",
    "visualize_params(flame, renderer, params, color=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_optimizer = VertexOptimizer(\n",
    "    flame=flame,\n",
    "    residuals=VertexResiduals(),\n",
    "    optimizer=GaussNewton(PytorchSolver()),\n",
    "    max_iters=2,\n",
    ")\n",
    "new_params = vertex_optimizer(s_vertices=batch[\"vertices\"].to(\"cuda\"), params=params)\n",
    "visualize_params(flame, renderer, new_params, color=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flame_optimizer = FlameOptimizer(\n",
    "    flame=flame,\n",
    "    correspondence_module=ProjectiveCorrespondenceModule(),\n",
    "    weighting_module=DummyWeightModule(),\n",
    "    renderer=renderer,\n",
    "    residuals=Point2PlaneResiduals(),\n",
    "    optimizer=GaussNewton(PytorchSolver()),\n",
    "    max_iters=5,\n",
    "    max_optims=1,\n",
    ")\n",
    "new_params = flame_optimizer(\n",
    "    s_point=batch[\"point\"].to(\"cuda\"),\n",
    "    s_normal=batch[\"normal\"].to(\"cuda\"),\n",
    "    s_mask=batch[\"mask\"].to(\"cuda\"),\n",
    "    params=params,\n",
    ")\n",
    "visualize_params(flame, renderer, new_params, color=1)\n",
    "print(flame_optimizer.optimizer.time_tracker.print_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flame_optimizer = FlameOptimizer(\n",
    "    flame=flame,\n",
    "    correspondence_module=OpticalFlowCorrespondenceModule(device=device),\n",
    "    weighting_module=ResidualWeightModule(device=device),\n",
    "    renderer=renderer,\n",
    "    residuals=Point2PlaneResiduals(),\n",
    "    optimizer=GaussNewton(PytorchSolver(), step_size=0.00001, strategy=\"forward-mode\"),\n",
    "    max_iters=1,\n",
    "    max_optims=1,\n",
    ")\n",
    "new_params = flame_optimizer(\n",
    "    s_point=batch[\"point\"].to(\"cuda\"),\n",
    "    s_normal=batch[\"normal\"].to(\"cuda\"),\n",
    "    s_mask=batch[\"mask\"].to(\"cuda\"),\n",
    "    params=params,\n",
    ")\n",
    "visualize_params(flame, renderer, new_params, color=1)\n",
    "print(flame_optimizer.optimizer.time_tracker.print_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def loss_step(params, gt_params):\n",
    "    loss = []\n",
    "    for p_names in params.keys():\n",
    "        l1_loss = torch.abs(params[p_names] - gt_params[p_names])\n",
    "        loss.append(l1_loss)\n",
    "    loss = torch.cat(loss, dim=-1).sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def clone_params(params):\n",
    "    out = {}\n",
    "    for k, v in params.items():\n",
    "        out[k] = v.detach().clone()\n",
    "    return out\n",
    "\n",
    "\n",
    "max_steps = 1\n",
    "flame_optimizer = FlameOptimizer(\n",
    "    flame=flame,\n",
    "    correspondence_module=OpticalFlowCorrespondenceModule(device=device),\n",
    "    weighting_module=ResidualWeightModule(device=device),\n",
    "    renderer=renderer,\n",
    "    residuals=Point2PlaneResiduals(),\n",
    "    optimizer=GaussNewton(PytorchSolver(), step_size=0.01),\n",
    "    max_iters=4,\n",
    "    max_optims=3,\n",
    ")\n",
    "optimizer = torch.optim.Adam(\n",
    "    flame_optimizer.w_module.parameters(),\n",
    "    lr=1e-03,\n",
    ")\n",
    "\n",
    "progress = tqdm(total=max_steps, desc=\"Loop\")\n",
    "for step in range(max_steps):\n",
    "    optimizer.zero_grad()\n",
    "    params, gt_params = clone_params(params), clone_params(gt_params)\n",
    "    batch = next(iter(dataloader))\n",
    "    new_params = flame_optimizer(\n",
    "        s_point=batch[\"point\"].to(\"cuda\"),\n",
    "        s_normal=batch[\"normal\"].to(\"cuda\"),\n",
    "        s_mask=batch[\"mask\"].to(\"cuda\"),\n",
    "        params=params,\n",
    "    )\n",
    "    default_loss = loss_step(params=params, gt_params=gt_params)\n",
    "    loss = loss_step(params=new_params, gt_params=gt_params)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    progress.update(1)\n",
    "    progress.set_postfix(\n",
    "        {\n",
    "            \"loss\": float(loss.item()),\n",
    "            \"default_loss\": float(default_loss.item()),\n",
    "        }\n",
    "    )\n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.state[optimizer.param_groups[0][\"params\"][0]].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import jacobian\n",
    "from torch.func import jacrev, vmap, jacfwd\n",
    "import torch.utils.benchmark as benchmark\n",
    "from lib.utils.mesh import vertex_normals\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_step(\n",
    "    batch: dict,\n",
    "    params: dict,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        m_out = flame.model_step(**params)\n",
    "        r_out = flame.render_step(\n",
    "            renderer=renderer,\n",
    "            vertices=m_out[\"vertices\"],\n",
    "        )\n",
    "        c_out = flame.correspondence_step(\n",
    "            s_point=batch[\"point\"],\n",
    "            s_mask=batch[\"mask\"],\n",
    "            s_normal=batch[\"normal\"],\n",
    "            t_point=r_out[\"point\"],\n",
    "            t_mask=r_out[\"r_mask\"],\n",
    "            t_normal=r_out[\"normal\"],\n",
    "        )\n",
    "        mask = c_out[\"mask\"]\n",
    "\n",
    "    b_depth = batch[\"point\"][..., 2:]  # (B, H, W, 1)\n",
    "    c_depth = r_out[\"point\"][..., 2:]  # (B, H, W, 1)\n",
    "    c_normal = r_out[\"normal\"]  # (B, H, W, 3)\n",
    "    rw_input = torch.cat([b_depth, c_depth, c_normal], dim=-1)\n",
    "    rw_input = rw_input.permute(0, 3, 1, 2)  # (B, 5, H, W)\n",
    "    weight_map = residual_weight(rw_input)\n",
    "\n",
    "    def closure(*args):\n",
    "        _params = {k: v for k, v in zip(params.keys(), args)}\n",
    "        m_out = flame.model_step(**_params)\n",
    "        t_point = interpolate(\n",
    "            vertices_idx=r_out[\"vertices_idx\"],\n",
    "            bary_coords=r_out[\"bary_coords\"],\n",
    "            attributes=m_out[\"vertices\"],\n",
    "        )\n",
    "        t_point = t_point[mask]\n",
    "        s_point = batch[\"point\"][mask]\n",
    "        t_normal = r_out[\"normal\"][mask]\n",
    "        weight = weight_map[mask]\n",
    "        point2plane = ((s_point - t_point) * t_normal).sum(-1)  # (C)\n",
    "        if \"expression_params\" in _params:\n",
    "            regularization = _params[\"expression_params\"].flatten()\n",
    "            F = torch.cat([weight * point2plane, regularization])\n",
    "        else:\n",
    "            F = weight * point2plane\n",
    "        return F, F\n",
    "\n",
    "    jacobian_fn = jacfwd(closure, has_aux=True, argnums=tuple(range(len(params))))\n",
    "    jacobian, F = jacobian_fn(*params.values())\n",
    "    J = torch.cat([j.flatten(-2) for j in jacobian], dim=-1)  # (M, N)\n",
    "\n",
    "    # solve for delta\n",
    "    H = 2 * J.T @ J\n",
    "    grad_f = 2 * J.T @ F\n",
    "    delta = -torch.linalg.solve(H, grad_f) * 1e-03\n",
    "\n",
    "    offset = 0\n",
    "    out = {}\n",
    "    for k, param in params.items():\n",
    "        numel = param.numel()\n",
    "        d = delta[offset : offset + numel].view_as(param)\n",
    "        out[k] = param + d\n",
    "        offset += numel\n",
    "\n",
    "    return out, weight_map\n",
    "\n",
    "\n",
    "def loss_step(params, gt_params):\n",
    "    loss = []\n",
    "    for p_names in params.keys():\n",
    "        l1_loss = torch.abs(params[p_names] - gt_params[p_names])\n",
    "        loss.append(l1_loss)\n",
    "    loss = torch.cat(loss, dim=-1).sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def clone_params(params):\n",
    "    out = {}\n",
    "    for k, v in params.items():\n",
    "        out[k] = v.detach().clone()\n",
    "    return out\n",
    "\n",
    "\n",
    "def loss_step1(params, batch):\n",
    "    m_out = flame.model_step(**params)\n",
    "    r_out = flame.render_step(\n",
    "        renderer=renderer,\n",
    "        vertices=m_out[\"vertices\"],\n",
    "    )\n",
    "    c_out = flame.correspondence_step(\n",
    "        s_point=batch[\"point\"],\n",
    "        s_mask=batch[\"mask\"],\n",
    "        s_normal=batch[\"normal\"],\n",
    "        t_point=r_out[\"point\"],\n",
    "        t_mask=r_out[\"r_mask\"],\n",
    "        t_normal=r_out[\"normal\"],\n",
    "    )\n",
    "    mask = c_out[\"mask\"]\n",
    "    t_point = interpolate(\n",
    "        vertices_idx=r_out[\"vertices_idx\"],\n",
    "        bary_coords=r_out[\"bary_coords\"],\n",
    "        attributes=m_out[\"vertices\"],\n",
    "    )\n",
    "    t_point = t_point[mask]\n",
    "    s_point = batch[\"point\"][mask]\n",
    "    t_normal = r_out[\"normal\"][mask]\n",
    "    point2plane = ((s_point - t_point) * t_normal).sum(-1)  # (C)\n",
    "    return (point2plane**2).sum()\n",
    "\n",
    "\n",
    "max_steps = 1000\n",
    "optimizer = torch.optim.Adam(residual_weight.parameters(), lr=1e-03)\n",
    "\n",
    "progress = tqdm(total=max_steps, desc=\"Loop\")\n",
    "for step in range(max_steps):\n",
    "    optimizer.zero_grad()\n",
    "    params, gt_params = clone_params(params), clone_params(gt_params)\n",
    "    batch = create_batch(gt_params)\n",
    "    new_params, weight_map = train_step(batch=batch, params=params)\n",
    "    default_loss = loss_step(params=params, gt_params=gt_params)\n",
    "    loss = loss_step(params=new_params, gt_params=gt_params)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    progress.update(1)\n",
    "    progress.set_postfix(\n",
    "        {\n",
    "            \"loss\": float(loss.item()),\n",
    "            \"default_loss\": float(default_loss.item()),\n",
    "        }\n",
    "    )\n",
    "progress.close()\n",
    "\n",
    "visualize_params(params, 0)\n",
    "visualize_params(new_params, 1)\n",
    "visualize_params(gt_params, 2)\n",
    "visualize_weight_map(weight_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterStore:\n",
    "    def generate_random(self, seed: int = 1, sigma: float = 1.0):\n",
    "        params = {}\n",
    "        for p_name in self.full_p_names:\n",
    "            if param := init_config.get(p_name):\n",
    "                np.random.seed(seed)\n",
    "                delta = np.random.normal(0, sigma, len(param))\n",
    "                params[p_name] = [p + d for p, d in zip(param, delta)]\n",
    "        self.init_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_module = OpticalFlowCorrespondenceModule(device=device)\n",
    "m_out = flame(**params)\n",
    "r_out = renderer.render_full(\n",
    "    vertices=m_out[\"vertices\"],  # (B, V, 3)\n",
    "    faces=flame.faces,  # (F, 3)\n",
    ")\n",
    "c_out = c_module.predict(\n",
    "    s_point=batch[\"point\"].to(\"cuda\"),\n",
    "    s_normal=batch[\"normal\"].to(\"cuda\"),\n",
    "    t_point=r_out[\"point\"],\n",
    "    t_normal=r_out[\"normal\"],\n",
    ")\n",
    "value = r_out[\"point\"]\n",
    "grid = c_module.create_grid(c_out[\"s_delta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "def sample_grid1(grid: torch.Tensor, value: torch.Tensor):\n",
    "    B, H, W, C = value.shape\n",
    "\n",
    "    # scale grid values from [-1, 1] to [0, W-1] for x and [0, H-1] for y\n",
    "    x = ((grid[..., 0] + 1) * (W - 1)) / 2\n",
    "    y = ((grid[..., 1] + 1) * (H - 1)) / 2\n",
    "\n",
    "    # ...\n",
    "\n",
    "    # bilinear interpolate the cornes\n",
    "    interpolated_values = (\n",
    "        wa.unsqueeze(-1) * Ia\n",
    "        + wb.unsqueeze(-1) * Ib\n",
    "        + wc.unsqueeze(-1) * Ic\n",
    "        + wd.unsqueeze(-1) * Id\n",
    "    )\n",
    "\n",
    "    # zero out the where we need to clamp\n",
    "    zero_tensor = torch.zeros_like(interpolated_values)\n",
    "    interpolated_values[clamp_mask] = zero_tensor[clamp_mask]\n",
    "\n",
    "    return interpolated_values\n",
    "\n",
    "\n",
    "def sample_grid2(grid: torch.Tensor, value: torch.Tensor):\n",
    "    value = value.permute(0, 3, 1, 2)\n",
    "    samples = torch.nn.functional.grid_sample(\n",
    "        input=value,\n",
    "        grid=grid,\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "        padding_mode=\"zeros\",\n",
    "    )\n",
    "    samples = samples.permute(0, 2, 3, 1)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def foo():\n",
    "    sample_grid2(grid, value)\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt=\"foo()\",\n",
    "    setup=\"from __main__ import foo\",\n",
    "    globals=globals(),\n",
    ")\n",
    "print(t0.timeit(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.tracker.logger import FlameLogger\n",
    "\n",
    "logger = FlameLogger(mode=\"joint\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
