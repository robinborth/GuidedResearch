{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from lib.utils.config import load_config\n",
    "from lib.optimizer.framework import NeuralOptimizer\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.rasterizer import Rasterizer\n",
    "from lib.renderer.renderer import Renderer\n",
    "from lib.renderer.camera import Camera\n",
    "from lib.tracker.timer import TimeTracker\n",
    "from lib.utils.progress import reset_progress, close_progress\n",
    "\n",
    "\n",
    "def path_to_abblation(path):\n",
    "    return \"_\".join(path.split(\"/\")[-3].split(\"_\")[1:])\n",
    "\n",
    "\n",
    "def eval_iterations(\n",
    "    optimizer, datamodule, N: int = 1, value: str = \"loss_param\", mode=\"iters\"\n",
    "):\n",
    "    optimizer.max_iters = 1\n",
    "    optimizer.max_optims = 1\n",
    "\n",
    "    outer_progress = tqdm(total=N + 1, desc=\"Iter Loop\", position=0)\n",
    "    total_evals = len(datamodule.val_dataset)\n",
    "    inner_progress = tqdm(total=total_evals, desc=\"Eval Loop\", leave=True, position=1)\n",
    "\n",
    "    iters_p_loss = {}\n",
    "    iters_v_loss = {}\n",
    "    iters_time = {}\n",
    "\n",
    "    # initial evaluation no optimization\n",
    "    reset_progress(inner_progress, total_evals)\n",
    "    p_loss = []\n",
    "    v_loss = []\n",
    "    for batch in datamodule.val_dataloader():\n",
    "        with torch.no_grad():\n",
    "            batch = optimizer.transfer_batch_to_device(batch, \"cuda\", 0)\n",
    "            out = optimizer(batch)\n",
    "            out[\"params\"] = batch[\"init_params\"]\n",
    "            loss_info = optimizer.compute_loss(batch=batch, out=out)\n",
    "            p_loss.append(loss_info[\"loss_param\"])\n",
    "            v_loss.append(loss_info[\"loss_vertices\"])\n",
    "        inner_progress.update(1)\n",
    "    iters_p_loss[0] = torch.stack(p_loss)\n",
    "    iters_v_loss[0] = torch.stack(v_loss)\n",
    "    iters_time[0] = torch.zeros_like(iters_p_loss[0])\n",
    "    outer_progress.update(1)\n",
    "\n",
    "    # evaluation after some optimization\n",
    "    for iters in range(1, N + 1):\n",
    "        reset_progress(inner_progress, total_evals)\n",
    "        if mode == \"iters\":\n",
    "            optimizer.max_iters = iters\n",
    "        else:\n",
    "            optimizer.max_optims = iters\n",
    "        time_tracker = TimeTracker()\n",
    "        p_loss = []\n",
    "        v_loss = []\n",
    "        for batch in datamodule.val_dataloader():\n",
    "            with torch.no_grad():\n",
    "                batch = optimizer.transfer_batch_to_device(batch, \"cuda\", 0)\n",
    "                time_tracker.start(\"optimize\")\n",
    "                out = optimizer(batch)\n",
    "                time_tracker.stop(\"optimize\")\n",
    "                loss_info = optimizer.compute_loss(batch=batch, out=out)\n",
    "                p_loss.append(loss_info[\"loss_param\"])\n",
    "                v_loss.append(loss_info[\"loss_vertices\"])\n",
    "            inner_progress.update(1)\n",
    "        iters_p_loss[iters] = torch.stack(p_loss)\n",
    "        iters_v_loss[iters] = torch.stack(v_loss)\n",
    "        iters_time[iters] = torch.stack(\n",
    "            [torch.tensor(t.time_ms) for t in list(time_tracker.tracks.values())[0]]\n",
    "        )\n",
    "        outer_progress.update(1)\n",
    "    close_progress([outer_progress, inner_progress])\n",
    "    return iters_p_loss, iters_v_loss, iters_time\n",
    "\n",
    "\n",
    "def load_flame_renderer():\n",
    "    # instanciate similar to training\n",
    "    cfg = load_config(\"train\", [\"data=synthetic\"])\n",
    "    K = load_intrinsics(data_dir=cfg.data.intrinsics_dir, return_tensor=\"pt\")\n",
    "    camera = Camera(\n",
    "        K=K,\n",
    "        width=cfg.data.width,\n",
    "        height=cfg.data.height,\n",
    "        near=cfg.data.near,\n",
    "        far=cfg.data.far,\n",
    "        scale=cfg.data.scale,\n",
    "    )\n",
    "    rasterizer = Rasterizer(width=camera.width, height=camera.height)\n",
    "    renderer = Renderer(rasterizer=rasterizer, camera=camera)\n",
    "    flame = hydra.utils.instantiate(cfg.model)\n",
    "    return flame, renderer\n",
    "\n",
    "\n",
    "def load_neural_optimizer(flame, renderer, path, override=[]):\n",
    "    cfg = load_config(\"train\", [\"data=synthetic\"]+override)\n",
    "    correspondence = hydra.utils.instantiate(cfg.correspondence)\n",
    "    weighting = hydra.utils.instantiate(cfg.weighting)\n",
    "    residuals = hydra.utils.instantiate(cfg.residuals)\n",
    "    regularize = hydra.utils.instantiate(cfg.regularize)\n",
    "    neural_optimizer = NeuralOptimizer.load_from_checkpoint(\n",
    "        path,\n",
    "        renderer=renderer,\n",
    "        flame=flame,\n",
    "        correspondence=correspondence,\n",
    "        regularize=regularize,\n",
    "        residuals=residuals,\n",
    "        weighting=weighting,\n",
    "    )\n",
    "    return neural_optimizer\n",
    "\n",
    "def load_neural_optimizer1(flame, renderer, path):\n",
    "    cfg = load_config(\"train\", [\"data=synthetic\", \"regularize=dummy\"])\n",
    "    correspondence = hydra.utils.instantiate(cfg.correspondence)\n",
    "    weighting = hydra.utils.instantiate(cfg.weighting)\n",
    "    regularize = hydra.utils.instantiate(cfg.regularize)\n",
    "    residuals = hydra.utils.instantiate(cfg.residuals)\n",
    "    neural_optimizer = NeuralOptimizer.load_from_checkpoint(\n",
    "        path,\n",
    "        renderer=renderer,\n",
    "        flame=flame,\n",
    "        correspondence=correspondence,\n",
    "        regularize=regularize,\n",
    "        residuals=residuals,\n",
    "        weighting=weighting,\n",
    "    )\n",
    "    return neural_optimizer\n",
    "\n",
    "\n",
    "def load_icp_optimizer(flame, renderer, overrides):\n",
    "    cfg = load_config(\n",
    "        \"train\", [\"data=synthetic\", \"optimizer.output_dir=none\"] + overrides\n",
    "    )\n",
    "    correspondence = hydra.utils.instantiate(cfg.correspondence)\n",
    "    weighting = hydra.utils.instantiate(cfg.weighting)\n",
    "    residuals = hydra.utils.instantiate(cfg.residuals)\n",
    "    optimizer = hydra.utils.instantiate(cfg.optimizer)\n",
    "    regularize = hydra.utils.instantiate(cfg.regularize)\n",
    "    icp_optimizer = hydra.utils.instantiate(\n",
    "        cfg.framework,\n",
    "        flame=flame,\n",
    "        logger=None,\n",
    "        renderer=renderer,\n",
    "        correspondence=correspondence,\n",
    "        regularize=regularize,\n",
    "        residuals=residuals,\n",
    "        optimizer=optimizer,\n",
    "        weighting=weighting,\n",
    "    )\n",
    "    return icp_optimizer.to(\"cuda\")\n",
    "\n",
    "\n",
    "# setup the datamodule\n",
    "def load_datamodule(renderer, start_frame, end_frame):\n",
    "    cfg = load_config(\"train\", [\"data=synthetic\"])\n",
    "    datamodule = hydra.utils.instantiate(\n",
    "        cfg.data,\n",
    "        renderer=renderer,\n",
    "        val_dataset=dict(\n",
    "            start_frame=start_frame,\n",
    "            end_frame=end_frame,\n",
    "        ),\n",
    "    )\n",
    "    datamodule.setup(\"fit\")\n",
    "    return datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "N = 3\n",
    "value = \"loss_param\"  #  loss_vertices, loss_param\n",
    "start_frame = 10\n",
    "end_frame = 18\n",
    "\n",
    "# checkpoints\n",
    "ours = \"/home/borth/GuidedResearch/logs/2024-10-06/06-55-55_abblation_ours_ckpt/checkpoints/last.ckpt\"\n",
    "wo_neural_prior = \"/home/borth/GuidedResearch/logs/2024-10-04/22-44-20_abblation_wo_neural_prior/checkpoints/last.ckpt\"\n",
    "w_single_corresp = \"/home/borth/GuidedResearch/logs/2024-10-03/09-54-41_abblation_w_single_corresp/checkpoints/last.ckpt\"\n",
    "w_single_optim = \"/home/borth/GuidedResearch/logs/2024-10-06/12-55-40_abblation_w_single_optim/checkpoints/last.ckpt\"\n",
    "wo_neural_weights = \"/home/borth/GuidedResearch/logs/2024-10-03/09-54-41_abblation_wo_neural_weights/checkpoints/last.ckpt\"\n",
    "w_multi5_optim = \"/home/borth/GuidedResearch/logs/2024-10-08/14-59-53_abblation_w_multi5_optim_ckpt/checkpoints/last.ckpt\"\n",
    "\n",
    "# loadings\n",
    "times = {}\n",
    "p_losses = {}\n",
    "v_losses = {}\n",
    "flame, renderer = load_flame_renderer()\n",
    "datamodule = load_datamodule(renderer, start_frame, end_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ours\n",
    "optimizer = load_neural_optimizer(flame, renderer, path)\n",
    "p_loss, v_loss, time = eval_iterations(optimizer, datamodule, N=N, value=value, mode=\"iters\")\n",
    "key = path_to_abblation(path)\n",
    "times[key] = time[N].median().item()\n",
    "p_losses[key] = p_loss[N].mean().item()\n",
    "v_losses[key] = v_loss[N].mean().item()\n",
    "print(f\"{key}: p_loss={p_losses[key]:.03f} v_loss={v_losses[key]:.03f} time={times[key]:.03f}ms\")\n",
    "\n",
    "path = wo_neural_weights\n",
    "optimizer = load_neural_optimizer(flame, renderer, path)\n",
    "p_loss, v_loss, time = eval_iterations(optimizer, datamodule, N=N, value=value, mode=\"iters\")\n",
    "key = path_to_abblation(path)\n",
    "times[key] = time[N].median().item()\n",
    "p_losses[key] = p_loss[N].mean().item()\n",
    "v_losses[key] = v_loss[N].mean().item()\n",
    "print(f\"{key}: p_loss={p_losses[key]:.03f} v_loss={v_losses[key]:.03f} time={times[key]:.03f}ms\")\n",
    "\n",
    "path = wo_neural_prior\n",
    "optimizer = load_neural_optimizer1(flame, renderer, path)\n",
    "p_loss, v_loss, time = eval_iterations(optimizer, datamodule, N=N, value=value, mode=\"iters\")\n",
    "key = path_to_abblation(path)\n",
    "times[key] = time[N].median().item()\n",
    "p_losses[key] = p_loss[N].mean().item()\n",
    "v_losses[key] = v_loss[N].mean().item()\n",
    "print(f\"{key}: p_loss={p_losses[key]:.03f} v_loss={v_losses[key]:.03f} time={times[key]:.03f}ms\")\n",
    "\n",
    "path = w_single_corresp\n",
    "optimizer = load_neural_optimizer(flame, renderer, path)\n",
    "p_loss, v_loss, time = eval_iterations(optimizer, datamodule, N=N, value=value, mode=\"optims\")\n",
    "key = path_to_abblation(path)\n",
    "times[key] = time[N].median().item()\n",
    "p_losses[key] = p_loss[N].mean().item()\n",
    "v_losses[key] = v_loss[N].mean().item()\n",
    "print(f\"{key}: p_loss={p_losses[key]:.03f} v_loss={v_losses[key]:.03f} time={times[key]:.03f}ms\")\n",
    "\n",
    "path = w_single_optim\n",
    "optimizer = load_neural_optimizer(flame, renderer, path)\n",
    "p_loss, v_loss, time = eval_iterations(optimizer, datamodule, N=N, value=value, mode=\"iters\")\n",
    "key = \"abblation_wo_end_to_end\"\n",
    "times[key] = time[N].median().item()\n",
    "p_losses[key] = p_loss[N].mean().item()\n",
    "v_losses[key] = v_loss[N].mean().item()\n",
    "print(f\"{key}: p_loss={p_losses[key]:.03f} v_loss={v_losses[key]:.03f} time={times[key]:.03f}ms\")\n",
    "key = path_to_abblation(path)\n",
    "times[key] = time[1].median().item()\n",
    "p_losses[key] = p_loss[1].mean().item()\n",
    "v_losses[key] = v_loss[1].mean().item()\n",
    "print(f\"{key}: p_loss={p_losses[key]:.03f} v_loss={v_losses[key]:.03f} time={times[key]:.03f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = wo_neural_weights\n",
    "optimizer = load_neural_optimizer(flame, renderer, path, [\"weighting.dummy_weight=True\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times[key] = time[1].median().item()\n",
    "p_losses[key] = p_loss[1].mean().item()\n",
    "v_losses[key] = v_loss[1].mean().item()\n",
    "print(f\"{key}: p_loss={p_losses[key]:.03f} v_loss={v_losses[key]:.03f} time={times[key]:.03f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "step_size = 0.3\n",
    "\n",
    "optimizer = load_icp_optimizer(flame, renderer, [\"residuals=face2face\", \"weighting=dummy\", \"regularize=dummy\"])\n",
    "optimizer.optimizer.step_size=step_size\n",
    "loss, time = eval_iterations(optimizer, datamodule, N=N, value=value)\n",
    "f2f_loss = loss\n",
    "print(\"icp-f2f\")\n",
    "print({k: v.mean().item() for k, v in loss.items()})\n",
    "\n",
    "optimizer = load_icp_optimizer(flame, renderer, [\"residuals=point2plane\", \"weighting=dummy\", \"regularize=dummy\"])\n",
    "optimizer.optimizer.step_size=step_size\n",
    "loss, time = eval_iterations(optimizer, datamodule, N=N, value=value)\n",
    "p2p_loss = loss\n",
    "print(\"icp-p2p\")\n",
    "print({k: v.mean().item() for k, v in loss.items()})\n",
    "\n",
    "optimizer = load_neural_optimizer(flame, renderer, ours)\n",
    "optimizer.optimizer.step_size=step_size\n",
    "loss, time = eval_iterations(optimizer, datamodule, N=N, value=value, mode=\"iters\")\n",
    "ours_loss = loss\n",
    "print(\"ours\")\n",
    "print({k: v.mean().item() for k, v in loss.items()})\n",
    "\n",
    "optimizer = load_neural_optimizer(flame, renderer, w_single_optim)\n",
    "optimizer.optimizer.step_size=step_size\n",
    "loss, time = eval_iterations(optimizer, datamodule, N=N, value=value, mode=\"iters\")\n",
    "w_single_optim_loss = loss\n",
    "print(\"w_single_optim\")\n",
    "print({k: v.mean().item() for k, v in loss.items()})\n",
    "\n",
    "optimizer = load_neural_optimizer(flame, renderer, w_multi5_optim)\n",
    "optimizer.optimizer.step_size=step_size\n",
    "loss, time = eval_iterations(optimizer, datamodule, N=N, value=value, mode=\"iters\")\n",
    "w_single_optim_loss = loss\n",
    "print(\"w_multi5_optim\")\n",
    "print({k: v.mean().item() for k, v in loss.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1\n",
    "for key in [1.5, 1.2, 1.0, 0.5, 0.3, 0.1]:\n",
    "    optimizer = load_icp_optimizer(\n",
    "        flame,\n",
    "        renderer,\n",
    "        [\n",
    "            \"residuals=face2face\",\n",
    "            \"weighting=dummy\",\n",
    "            \"regularize=dummy\",\n",
    "            f\"optimizer.step_size={key}\",\n",
    "        ],\n",
    "    )\n",
    "    loss, time = eval_iterations(optimizer, datamodule, N=N, value=value)\n",
    "    time = time[N].median().item()\n",
    "    loss = loss[N].mean().item()\n",
    "    print(f\"{key}: loss={loss:.03f} time={time:.03f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = load_icp_optimizer(\n",
    "    flame,\n",
    "    renderer,\n",
    "    [\n",
    "        \"residuals=face2face\",\n",
    "        \"weighting=dummy\",\n",
    "        \"regularize=dummy\",\n",
    "        \"optimizer.step_size=0.3\",\n",
    "    ],\n",
    ")\n",
    "loss, time = eval_iterations(optimizer, datamodule, N=10, value=value)\n",
    "print({k: v.mean().item() for k, v in loss.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = load_icp_optimizer(\n",
    "    flame,\n",
    "    renderer,\n",
    "    [\n",
    "        \"residuals=face2face\",\n",
    "        \"weighting=dummy\",\n",
    "        \"regularize=dummy\",\n",
    "        \"optimizer.step_size=0.3\",\n",
    "        \"residuals=point2plane\",\n",
    "    ],\n",
    ")\n",
    "loss, time = eval_iterations(optimizer, datamodule, N=10, value=value)\n",
    "print({k: v.mean().item() for k, v in loss.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/borth/GuidedResearch/logs/2024-10-06/06-55-55_abblation_ours_ckpt/checkpoints/last.ckpt\"\n",
    "optimizer = load_neural_optimizer(flame, renderer, path)\n",
    "loss, time = eval_iterations(optimizer, datamodule, N=10, value=value, mode=\"iters\")\n",
    "print({k: v.mean().item() for k, v in loss.items()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
