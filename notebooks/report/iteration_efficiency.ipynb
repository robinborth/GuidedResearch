{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from lib.utils.config import load_config\n",
    "from lib.optimizer.framework import NeuralOptimizer, NeuralOptimizer2\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.rasterizer import Rasterizer\n",
    "from lib.renderer.renderer import Renderer\n",
    "from lib.renderer.camera import Camera\n",
    "from lib.tracker.timer import TimeTracker\n",
    "from lib.utils.progress import reset_progress, close_progress\n",
    "\n",
    "def path_to_abblation(path):\n",
    "    return \"_\".join(path.split(\"/\")[-3].split(\"_\")[1:])\n",
    "\n",
    "def eval_iterations(optimizer, datamodule, N: int = 1, value: str = \"loss_param\", mode=\"iters\"):\n",
    "    optimizer.max_iters = 1\n",
    "    optimizer.max_optims = 1\n",
    "\n",
    "    outer_progress = tqdm(total=N+1, desc=\"Iter Loop\", position=0)\n",
    "    total_evals = len(datamodule.val_dataset)\n",
    "    inner_progress = tqdm(total=total_evals, desc=\"Eval Loop\", leave=True, position=1)\n",
    "\n",
    "    iters_loss = {}\n",
    "    iters_time = {}\n",
    "\n",
    "    # initial evaluation no optimization\n",
    "    reset_progress(inner_progress, total_evals)\n",
    "    loss = []\n",
    "    for batch in datamodule.val_dataloader():\n",
    "        with torch.no_grad():\n",
    "            batch = optimizer.transfer_batch_to_device(batch, \"cuda\", 0)\n",
    "            out = optimizer(batch)\n",
    "            out[\"params\"] = batch[\"init_params\"]\n",
    "            loss_info = optimizer.compute_loss(batch=batch, out=out)\n",
    "            loss.append(loss_info[value])\n",
    "        inner_progress.update(1)\n",
    "    iters_loss[0] = torch.stack(loss)\n",
    "    iters_time[0] = torch.zeros_like(iters_loss[0])\n",
    "    outer_progress.update(1)\n",
    "        \n",
    "    # evaluation after some optimization\n",
    "    for iters in range(1, N+1):\n",
    "        reset_progress(inner_progress, total_evals)\n",
    "        if mode == \"iters\":\n",
    "            optimizer.max_iters = iters\n",
    "        else:\n",
    "            optimizer.max_optims = iters\n",
    "        time_tracker = TimeTracker()\n",
    "        loss = []\n",
    "        for batch in datamodule.val_dataloader():\n",
    "            with torch.no_grad():\n",
    "                batch = optimizer.transfer_batch_to_device(batch, \"cuda\", 0)\n",
    "                time_tracker.start(\"optimize\")\n",
    "                out = optimizer(batch)\n",
    "                time_tracker.stop(\"optimize\")\n",
    "                loss_info = optimizer.compute_loss(batch=batch, out=out)\n",
    "                loss.append(loss_info[value])\n",
    "            inner_progress.update(1)\n",
    "        loss = torch.stack(loss)\n",
    "        iters_loss[iters] = loss\n",
    "        iters_time[iters] = torch.stack([torch.tensor(t.time_ms) for t in list(time_tracker.tracks.values())[0]])\n",
    "        outer_progress.update(1)\n",
    "    close_progress([outer_progress, inner_progress])\n",
    "    return iters_loss, iters_time\n",
    "\n",
    "def load_flame_renderer():\n",
    "    # instanciate similar to training\n",
    "    cfg = load_config(\"train\", [\"data=synthetic\"])\n",
    "    K = load_intrinsics(data_dir=cfg.data.intrinsics_dir, return_tensor=\"pt\")\n",
    "    camera = Camera(\n",
    "        K=K,\n",
    "        width=cfg.data.width,\n",
    "        height=cfg.data.height,\n",
    "        near=cfg.data.near,\n",
    "        far=cfg.data.far,\n",
    "        scale=cfg.data.scale,\n",
    "    )\n",
    "    rasterizer = Rasterizer(width=camera.width, height=camera.height)\n",
    "    renderer = Renderer(rasterizer=rasterizer, camera=camera)\n",
    "    flame = hydra.utils.instantiate(cfg.model)\n",
    "    return flame, renderer\n",
    "\n",
    "\n",
    "def load_neural_optimizer(flame, renderer, path):\n",
    "    cfg = load_config(\"train\", [\"data=synthetic\"])\n",
    "    correspondence = hydra.utils.instantiate(cfg.correspondence)\n",
    "    weighting = hydra.utils.instantiate(cfg.weighting)\n",
    "    residuals = hydra.utils.instantiate(cfg.residuals)\n",
    "    regularize = hydra.utils.instantiate(cfg.regularize)\n",
    "    neural_optimizer = NeuralOptimizer.load_from_checkpoint(\n",
    "        path,\n",
    "        renderer=renderer,\n",
    "        flame=flame,\n",
    "        correspondence=correspondence,\n",
    "        regularize=regularize,\n",
    "        residuals=residuals,\n",
    "        weighting=weighting,\n",
    "    )\n",
    "    return neural_optimizer\n",
    "\n",
    "def load_icp_optimizer(flame, renderer, overrides, neural_optimizer2: bool = False):\n",
    "    cfg = load_config(\"train\", [\"data=synthetic\", \"optimizer.output_dir=none\"]+overrides)\n",
    "    correspondence = hydra.utils.instantiate(cfg.correspondence)\n",
    "    weighting = hydra.utils.instantiate(cfg.weighting)\n",
    "    residuals = hydra.utils.instantiate(cfg.residuals)\n",
    "    optimizer = hydra.utils.instantiate(cfg.optimizer)\n",
    "    regularize = hydra.utils.instantiate(cfg.regularize)\n",
    "    if neural_optimizer2:\n",
    "        cfg.framework._target_ = \"lib.optimizer.framework.NeuralOptimizer2\"\n",
    "    icp_optimizer = hydra.utils.instantiate(\n",
    "        cfg.framework,\n",
    "        flame=flame,\n",
    "        logger=None,\n",
    "        renderer=renderer,\n",
    "        correspondence=correspondence,\n",
    "        regularize=regularize,\n",
    "        residuals=residuals,\n",
    "        optimizer=optimizer,\n",
    "        weighting=weighting,\n",
    "    )\n",
    "    return icp_optimizer.to(\"cuda\")\n",
    "\n",
    "# setup the datamodule\n",
    "def load_datamodule(renderer, start_frame, end_frame):\n",
    "    cfg = load_config(\"train\", [\"data=synthetic\"])\n",
    "    datamodule = hydra.utils.instantiate(\n",
    "        cfg.data,\n",
    "        renderer=renderer,\n",
    "        val_dataset=dict(\n",
    "            start_frame=start_frame,\n",
    "            end_frame=end_frame,\n",
    "        ),\n",
    "    )\n",
    "    datamodule.setup(\"fit\")\n",
    "    return datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "N = 3\n",
    "value = \"loss_param\"  #  loss_vertices, loss_param \n",
    "start_frame = 10\n",
    "end_frame = 18\n",
    "\n",
    "# checkpoints\n",
    "ours = \"/home/borth/GuidedResearch/logs/2024-10-01/03-31-59_abblation_ours/checkpoints/last.ckpt\"\n",
    "wo_neural_prior = \"/home/borth/GuidedResearch/logs/2024-10-01/00-59-15_abblation_wo_neural_prior/checkpoints/last.ckpt\"\n",
    "w_single_corresp = \"/home/borth/GuidedResearch/logs/2024-10-01/22-07-11_abblation_w_single_corresp/checkpoints/last.ckpt\"\n",
    "w_single_optim = \"/home/borth/GuidedResearch/logs/2024-10-01/22-07-11_abblation_w_single_optim/checkpoints/last.ckpt\"\n",
    "wo_neural_weights = \"/home/borth/GuidedResearch/logs/2024-10-01/23-50-37_abblation_wo_neural_weights/checkpoints/last.ckpt\"\n",
    "\n",
    "# loadings\n",
    "times = {}\n",
    "losses = {}\n",
    "flame, renderer = load_flame_renderer()\n",
    "datamodule = load_datamodule(renderer, start_frame, end_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [ours, wo_neural_prior, wo_neural_weights]:\n",
    "    optimizer = load_neural_optimizer(flame, renderer, path)\n",
    "    loss, time = eval_iterations(optimizer, datamodule, N=N, value=value, mode=\"iters\")\n",
    "    key = path_to_abblation(path)\n",
    "    times[key] = time[N].median().item()\n",
    "    losses[key] = loss[N].mean().item()\n",
    "    print(f\"{key}: loss={losses[key]:.03f} time={times[key]:.03f}ms\")\n",
    "\n",
    "path = w_single_corresp\n",
    "optimizer = load_neural_optimizer(flame, renderer, path)\n",
    "loss, time = eval_iterations(optimizer, datamodule, N=N, value=value, mode=\"optims\")\n",
    "key = path_to_abblation(path)\n",
    "times[key] = time[N].median().item()\n",
    "losses[key] = loss[N].mean().item()\n",
    "print(f\"{key}: loss={losses[key]:.03f} time={times[key]:.03f}ms\")\n",
    "\n",
    "path = w_single_optim \n",
    "optimizer = load_neural_optimizer(flame, renderer, path)\n",
    "loss, time = eval_iterations(optimizer, datamodule, N=N, value=value, mode=\"optims\")\n",
    "key = \"abblation_wo_end_to_end\"\n",
    "times[key] = time[N].median().item()\n",
    "losses[key] = loss[N].mean().item()\n",
    "print(f\"{key}: loss={losses[key]:.03f} time={times[key]:.03f}ms\")\n",
    "key = path_to_abblation(path)\n",
    "times[key] = time[1].median().item()\n",
    "losses[key] = loss[1].mean().item()\n",
    "print(f\"{key}: loss={losses[key]:.03f} time={times[key]:.03f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = load_icp_optimizer(flame, renderer, [\"residuals=face2face\", \"weighting=dummy\", \"regularize=dummy\"])\n",
    "loss, time = eval_iterations(optimizer, datamodule, N=N, value=value)\n",
    "print(\"icp-baseline\")\n",
    "print(optimizer.time_tracker.print_summary())\n",
    "print({k: v.median().item() for k, v in time.items()})\n",
    "print({k: v.mean().item() for k, v in loss.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = load_icp_optimizer(flame, renderer, [\"regularize.latent_scale=1\"])\n",
    "# loss, time = eval_iterations(optimizer, datamodule, N=1, value=value)\n",
    "# print(optimizer.time_tracker.print_summary())\n",
    "# print({k: v.median().item() for k, v in time.items()})\n",
    "# print({k: v.mean().item() for k, v in loss.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = load_icp_optimizer(flame, renderer, [\"weighting.features=64\"])\n",
    "# loss, time = eval_iterations(optimizer, datamodule, N=1, value=value)\n",
    "# print(optimizer.time_tracker.print_summary())\n",
    "# print({k: v.median().item() for k, v in time.items()})\n",
    "# print({k: v.mean().item() for k, v in loss.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = load_icp_optimizer(flame, renderer, [\"regularize.latent_scale=4\"], True)\n",
    "# loss, time = eval_iterations(optimizer, datamodule, N=1, value=value)\n",
    "# print(optimizer.time_tracker.print_summary())\n",
    "# print({k: v.median().item() for k, v in time.items()})\n",
    "# print({k: v.mean().item() for k, v in loss.items()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
