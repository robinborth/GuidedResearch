{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from lib.utils.config import load_config\n",
    "from lib.optimizer.framework import NeuralOptimizer\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.rasterizer import Rasterizer\n",
    "from lib.renderer.renderer import Renderer\n",
    "from lib.renderer.camera import Camera\n",
    "from lib.tracker.timer import TimeTracker\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def path_to_abblation(path):\n",
    "    return \"_\".join(path.split(\"/\")[-3].split(\"_\")[1:])\n",
    "\n",
    "\n",
    "def eval_iterations(optimizer, datamodule, N: int = 3):\n",
    "    optimizer.max_iters = N\n",
    "    optimizer.max_optims = 1\n",
    "    time_tracker = TimeTracker()\n",
    "    n_threshold =  optimizer.c_module.n_threshold\n",
    "    d_threshold = optimizer.c_module.d_threshold\n",
    "    # initial evaluation no optimization\n",
    "    p_loss = []\n",
    "    v_loss = []\n",
    "    g_loss = []\n",
    "    for batch in datamodule.val_dataloader():\n",
    "        with torch.no_grad():\n",
    "            batch = optimizer.transfer_batch_to_device(batch, \"cuda\", 0)\n",
    "            time_tracker.start(\"optimize\")\n",
    "            out = optimizer(batch)\n",
    "            time_tracker.stop(\"optimize\")\n",
    "            optimizer.c_module.n_threshold = -1.0\n",
    "            optimizer.c_module.d_threshold = 1e-02  # 1cm\n",
    "            loss_info = optimizer.compute_loss(batch=batch, out=out)\n",
    "            optimizer.c_module.n_threshold = n_threshold\n",
    "            optimizer.c_module.d_threshold = d_threshold  # 1cm\n",
    "            p_loss.append(loss_info[\"loss_param\"])\n",
    "            v_loss.append(loss_info[\"loss_vertices\"])\n",
    "            g_loss.append(loss_info[\"loss_geometric_point2point\"])\n",
    "    iters_p_loss = torch.stack(p_loss).mean().item()\n",
    "    iters_g_loss = torch.stack(g_loss).mean().item()\n",
    "    iters_v_loss = torch.stack(v_loss).mean().item()\n",
    "    t_perf = [torch.tensor(t.time_ms) for t in list(time_tracker.tracks.values())[0]]\n",
    "    iters_time = torch.stack(t_perf).min().item()\n",
    "    return iters_p_loss, iters_g_loss, iters_v_loss, iters_time\n",
    "\n",
    "\n",
    "def load_flame_renderer():\n",
    "    # instanciate similar to training\n",
    "    cfg = load_config(\"train\", [\"data=kinect\"])\n",
    "    K = load_intrinsics(data_dir=cfg.data.intrinsics_dir, return_tensor=\"pt\")\n",
    "    camera = Camera(\n",
    "        K=K,\n",
    "        width=cfg.data.width,\n",
    "        height=cfg.data.height,\n",
    "        near=cfg.data.near,\n",
    "        far=cfg.data.far,\n",
    "        scale=cfg.data.scale,\n",
    "    )\n",
    "    rasterizer = Rasterizer(width=camera.width, height=camera.height)\n",
    "    renderer = Renderer(rasterizer=rasterizer, camera=camera)\n",
    "    flame = hydra.utils.instantiate(cfg.model)\n",
    "    return flame, renderer\n",
    "\n",
    "\n",
    "def load_neural_optimizer(flame, renderer, path, override=[]):\n",
    "    o = [\"data=kinect\"] + override\n",
    "    cfg = load_config(\"train\", o)\n",
    "    correspondence = hydra.utils.instantiate(cfg.correspondence)\n",
    "    weighting = hydra.utils.instantiate(cfg.weighting)\n",
    "    residuals = hydra.utils.instantiate(cfg.residuals)\n",
    "    regularize = hydra.utils.instantiate(cfg.regularize)\n",
    "    neural_optimizer = NeuralOptimizer.load_from_checkpoint(\n",
    "        path,\n",
    "        renderer=renderer,\n",
    "        flame=flame,\n",
    "        correspondence=correspondence,\n",
    "        regularize=regularize,\n",
    "        residuals=residuals,\n",
    "        weighting=weighting,\n",
    "    )\n",
    "    return neural_optimizer\n",
    "\n",
    "\n",
    "def load_icp_optimizer(flame, renderer, overrides):\n",
    "    o = [\"data=kinect\", \"optimizer.output_dir=none\"] + overrides\n",
    "    cfg = load_config(\"train\", o)\n",
    "    correspondence = hydra.utils.instantiate(cfg.correspondence)\n",
    "    weighting = hydra.utils.instantiate(cfg.weighting)\n",
    "    residuals = hydra.utils.instantiate(cfg.residuals)\n",
    "    optimizer = hydra.utils.instantiate(cfg.optimizer)\n",
    "    regularize = hydra.utils.instantiate(cfg.regularize)\n",
    "    icp_optimizer = hydra.utils.instantiate(\n",
    "        cfg.framework,\n",
    "        flame=flame,\n",
    "        logger=None,\n",
    "        renderer=renderer,\n",
    "        correspondence=correspondence,\n",
    "        regularize=regularize,\n",
    "        residuals=residuals,\n",
    "        optimizer=optimizer,\n",
    "        weighting=weighting,\n",
    "    )\n",
    "    return icp_optimizer.to(\"cuda\")\n",
    "\n",
    "\n",
    "# setup the datamodule\n",
    "def load_datamodule(renderer, start_frame, end_frame, jump_size):\n",
    "    cfg = load_config(\"train\", [\"data=kinect\"])\n",
    "    datamodule = hydra.utils.instantiate(\n",
    "        cfg.data,\n",
    "        renderer=renderer,\n",
    "        val_dataset=dict(\n",
    "            start_frame=start_frame,\n",
    "            end_frame=end_frame,\n",
    "            jump_size=jump_size,\n",
    "        ),\n",
    "    )\n",
    "    datamodule.setup(\"fit\")\n",
    "    return datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "step_size = 0.7\n",
    "# start_frame = 48 \n",
    "# end_frame = 56\n",
    "start_frame = 30\n",
    "end_frame = None\n",
    "\n",
    "# checkpoints\n",
    "# ours = \"/home/borth/GuidedResearch/checkpoints/kinect/ours.ckpt\"\n",
    "# ours_wo_prior = \"/home/borth/GuidedResearch/checkpoints/kinect/wo_prior.ckpt\"\n",
    "# ours_syn = \"/home/borth/GuidedResearch/checkpoints/kinect/synthetic.ckpt\"\n",
    "ours = \"/home/borth/GuidedResearch/logs/2024-10-17/19-21-35_train_kinect_fix/checkpoints/epoch_379.ckpt\"\n",
    "ours_syn = \"/home/borth/GuidedResearch/logs/2024-10-17/19-21-35_train_synthetic/checkpoints/epoch_1079.ckpt\"\n",
    "\n",
    "# loadings\n",
    "times = defaultdict(dict)\n",
    "p_losses = defaultdict(dict)\n",
    "g_losses = defaultdict(dict)\n",
    "v_losses = defaultdict(dict)\n",
    "flame, renderer = load_flame_renderer()\n",
    "\n",
    "for jump_size in [1, 2, 4, 8]:\n",
    "    datamodule = load_datamodule(renderer, start_frame, end_frame, jump_size)\n",
    "\n",
    "    # override = [\"residuals=face2face_wo_landmarks\", \"regularize=dummy\"]\n",
    "    # optimizer = load_neural_optimizer(flame, renderer, ours_wo_prior, override)\n",
    "    # optimizer.optimizer.step_size = step_size\n",
    "    # p_loss, g_loss, v_loss, time = eval_iterations(optimizer, datamodule, N=N)\n",
    "    # key = \"ours_wo_prior\"\n",
    "    # times[key][jump_size] = time\n",
    "    # p_losses[key][jump_size] = p_loss\n",
    "    # v_losses[key][jump_size] = v_loss\n",
    "    # g_losses[key][jump_size] = g_loss    \n",
    "\n",
    "    optimizer = load_neural_optimizer(flame, renderer, ours_syn, [])\n",
    "    optimizer.optimizer.step_size = step_size\n",
    "    p_loss, g_loss, v_loss, time = eval_iterations(optimizer, datamodule, N=N)\n",
    "    key = \"ours_syn\"\n",
    "    times[key][jump_size] = time\n",
    "    p_losses[key][jump_size] = p_loss\n",
    "    v_losses[key][jump_size] = v_loss\n",
    "    g_losses[key][jump_size] = g_loss\n",
    "\n",
    "    optimizer = load_neural_optimizer(flame, renderer, ours, [])\n",
    "    optimizer.optimizer.step_size = step_size\n",
    "    p_loss, g_loss, v_loss, time = eval_iterations(optimizer, datamodule, N=N)\n",
    "    key = \"ours\"\n",
    "    times[key][jump_size] = time\n",
    "    p_losses[key][jump_size] = p_loss\n",
    "    v_losses[key][jump_size] = v_loss\n",
    "    g_losses[key][jump_size] = g_loss\n",
    "\n",
    "    optimizer = load_icp_optimizer(flame, renderer, [])\n",
    "    optimizer.optimizer.step_size = 0.0\n",
    "    p_loss, g_loss, v_loss, time = eval_iterations(optimizer, datamodule, N=1)\n",
    "    key = \"base\"\n",
    "    times[key][jump_size] = time\n",
    "    p_losses[key][jump_size] = p_loss\n",
    "    v_losses[key][jump_size] = v_loss\n",
    "    g_losses[key][jump_size] = g_loss\n",
    "\n",
    "    optimizer = load_icp_optimizer(flame, renderer, [\"residuals=point2plane\", \"weighting=dummy\", \"regularize=dummy\"])\n",
    "    optimizer.optimizer.step_size = step_size\n",
    "    p_loss, g_loss, v_loss, time = eval_iterations(optimizer, datamodule, N=N)\n",
    "    key = \"icp-wo_reg\"\n",
    "    times[key][jump_size] = time\n",
    "    p_losses[key][jump_size] = p_loss\n",
    "    v_losses[key][jump_size] = v_loss\n",
    "    g_losses[key][jump_size] = g_loss\n",
    "\n",
    "    optimizer = load_icp_optimizer(flame, renderer, [\"residuals=face2face_wo_landmarks\", \"weighting=dummy\", \"regularize=dummy\"])\n",
    "    optimizer.optimizer.step_size = step_size\n",
    "    p_loss, g_loss, v_loss, time = eval_iterations(optimizer, datamodule, N=N)\n",
    "    key = \"icp-w_reg\"\n",
    "    times[key][jump_size] = time\n",
    "    p_losses[key][jump_size] = p_loss\n",
    "    v_losses[key][jump_size] = v_loss\n",
    "    g_losses[key][jump_size] = g_loss\n",
    "\n",
    "    optimizer = load_icp_optimizer(flame, renderer, [\"residuals=face2face_wo_landmarks\", \"weighting=dummy\", \"regularize=dummy\"])\n",
    "    optimizer.optimizer.step_size = 0.3\n",
    "    p_loss, g_loss, v_loss, time = eval_iterations(optimizer, datamodule, N=N)\n",
    "    key = \"icp-step\"\n",
    "    times[key][jump_size] = time\n",
    "    p_losses[key][jump_size] = p_loss\n",
    "    v_losses[key][jump_size] = v_loss\n",
    "    g_losses[key][jump_size] = g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_order = [\"base\", \"icp-w_reg\", \"icp-step\", \"icp-wo_reg\", \"ours\", \"ours_wo_prior\", \"ours_syn\"]\n",
    "\n",
    "# Create the DataFrame for p_losses\n",
    "p_losses_df = pd.DataFrame(p_losses).transpose()\n",
    "p_losses_df.columns = [f\"0->{c}\" for c in p_losses_df.columns]\n",
    "p_losses_df = p_losses_df.reindex(desired_order)\n",
    "p_losses_df.columns = pd.MultiIndex.from_tuples([(\"FLAME (norm)\", c) for c in p_losses_df.columns])\n",
    "\n",
    "# Create the DataFrame for g_losses\n",
    "g_losses_df = pd.DataFrame(g_losses).transpose()\n",
    "g_losses_df.columns = [f\"0->{c}\" for c in g_losses_df.columns]\n",
    "g_losses_df = g_losses_df.reindex(desired_order)\n",
    "g_losses_df.columns = pd.MultiIndex.from_tuples([(\"P2P (mm)\", c) for c in g_losses_df.columns])\n",
    "\n",
    "# Create the DataFrame for v_losses\n",
    "v_losses_df = pd.DataFrame(v_losses).transpose()\n",
    "v_losses_df.columns = [f\"0->{c}\" for c in v_losses_df.columns]\n",
    "v_losses_df = v_losses_df.reindex(desired_order)\n",
    "v_losses_df.columns = pd.MultiIndex.from_tuples([(\"Vertices (mm)\", c) for c in v_losses_df.columns])\n",
    "\n",
    "# Time\n",
    "time_df = pd.DataFrame(times).transpose()\n",
    "time_df = pd.DataFrame(time_df.mean(axis=1))\n",
    "time_df = time_df.reindex(desired_order)\n",
    "time_df.columns = pd.MultiIndex.from_tuples([(\"Time (ms)\", \"\")])\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "pd.concat([p_losses_df, g_losses_df, v_losses_df, time_df], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
