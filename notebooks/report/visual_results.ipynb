{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import hydra\n",
    "import torch\n",
    "from lib.utils.config import load_config\n",
    "from lib.optimizer.framework import NeuralOptimizer\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.rasterizer import Rasterizer\n",
    "from lib.renderer.renderer import Renderer\n",
    "from lib.renderer.camera import Camera\n",
    "from lib.utils.visualize import visualize_point2plane_error\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.utils.visualize import visualize_merged\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def path_to_abblation(path):\n",
    "    return \"_\".join(path.split(\"/\")[-3].split(\"_\")[1:])\n",
    "\n",
    "def draw_and_save_color(dataset, idx, path):\n",
    "    _path = f\"/home/borth/GuidedResearch/data/dphm_kinect/{dataset}/color/{idx:05}.png\"\n",
    "    img = Image.open(_path)\n",
    "    plt.figure(figsize=(19.2, 10.8), dpi=100)  # Full HD size\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.savefig(path, bbox_inches=\"tight\", pad_inches=0)  # Save without padding\n",
    "    plt.show()\n",
    "\n",
    "def draw_and_save_weight(flame, renderer, out, path):\n",
    "    renderer.update(1)\n",
    "    mask = flame.render(renderer, out[\"params\"])[\"mask\"][0]\n",
    "    renderer.update(8)\n",
    "\n",
    "    # weight inference\n",
    "    weights = out[\"optim_weights\"][-1]\n",
    "    weights = F.interpolate(weights.unsqueeze(0), scale_factor=8, mode='bilinear', align_corners=False)\n",
    "    weights = weights.detach()[0][0]\n",
    "    weights[~mask] = 0.0\n",
    "\n",
    "    plt.figure(figsize=(19.2, 10.8), dpi=100)  # Full HD size\n",
    "    plt.imshow(weights.cpu().numpy())\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.savefig(path, bbox_inches=\"tight\", pad_inches=0)  # Save without padding\n",
    "    plt.show()\n",
    "\n",
    "def draw_and_save_overlay(optimizer, renderer, params, dataset, idx, path):\n",
    "    _path = f\"/home/borth/GuidedResearch/data/dphm_kinect/{dataset}/color/{idx:05}.png\"\n",
    "    color = torch.tensor(np.asarray(Image.open(_path))).unsqueeze(0).to(\"cuda\")\n",
    "    renderer.update(scale=1)\n",
    "    out = optimizer.flame.render(renderer, params)\n",
    "    renderer.update(scale=8)\n",
    "    img = visualize_merged(\n",
    "        s_color=color,\n",
    "        t_color=out[\"color\"],\n",
    "        t_mask=out[\"mask\"],\n",
    "    )\n",
    "    img = img[0].detach().cpu().numpy()\n",
    "    plt.figure(figsize=(19.2, 10.8), dpi=100)  # Full HD size\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.savefig(path, bbox_inches=\"tight\", pad_inches=0)  # Save without padding\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def draw_and_save_normal(dataset, idx, path):\n",
    "    _path = f\"/home/borth/GuidedResearch/data/dphm_kinect/{dataset}/cache/2_normal/{idx:05}.pt\"\n",
    "    normal = torch.load(_path)\n",
    "    _path = f\"/home/borth/GuidedResearch/data/dphm_kinect/{dataset}/cache/2_mask/{idx:05}.pt\"\n",
    "    mask = torch.load(_path)\n",
    "    normal_image = (((normal + 1) / 2) * 255).to(torch.uint8)\n",
    "    normal_image[~mask] = 255\n",
    "    plt.figure(figsize=(19.2, 10.8), dpi=100)  # Full HD size\n",
    "    plt.imshow(normal_image.detach().cpu().numpy())\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.savefig(path, bbox_inches=\"tight\", pad_inches=0)  # Save without padding\n",
    "    plt.show()\n",
    "\n",
    "def eval_iterations(\n",
    "    optimizer,\n",
    "    renderer,\n",
    "    dataset,\n",
    "    target_frame_idx,\n",
    "    source_frame_idx,\n",
    "    step_size=0.7,\n",
    "    N=2,\n",
    "):\n",
    "    cfg = load_config(\"train\", [\"data=kinect\"])\n",
    "    datamodule = hydra.utils.instantiate(\n",
    "        cfg.data,\n",
    "        renderer=renderer,\n",
    "        val_dataset=dict(\n",
    "            start_frame=target_frame_idx,\n",
    "            end_frame=target_frame_idx + 1,\n",
    "            jump_size=target_frame_idx - source_frame_idx,\n",
    "            datasets=[dataset]\n",
    "        ),\n",
    "    )\n",
    "    datamodule.setup(\"fit\")\n",
    "\n",
    "    optimizer.max_iters = N\n",
    "    optimizer.max_optims = 1\n",
    "    optimizer.step_size = step_size\n",
    "    out = None\n",
    "    batch = None\n",
    "    for i, b in enumerate(datamodule.val_dataloader()):\n",
    "        with torch.no_grad():\n",
    "            batch = optimizer.transfer_batch_to_device(b, \"cuda\", 0)\n",
    "            out = optimizer(batch)\n",
    "    return out, batch\n",
    "\n",
    "\n",
    "def draw_and_save(img, path):\n",
    "    # Display and save the error image\n",
    "    plt.figure(figsize=(19.2, 10.8), dpi=100)  # Full HD size\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.savefig(path, bbox_inches=\"tight\", pad_inches=0)  # Save without padding\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_flame_renderer():\n",
    "    # instanciate similar to training\n",
    "    cfg = load_config(\"train\", [\"data=kinect\"])\n",
    "    K = load_intrinsics(data_dir=cfg.data.intrinsics_dir, return_tensor=\"pt\")\n",
    "    camera = Camera(\n",
    "        K=K,\n",
    "        width=cfg.data.width,\n",
    "        height=cfg.data.height,\n",
    "        near=cfg.data.near,\n",
    "        far=cfg.data.far,\n",
    "        scale=cfg.data.scale,\n",
    "    )\n",
    "    rasterizer = Rasterizer(width=camera.width, height=camera.height)\n",
    "    renderer = Renderer(rasterizer=rasterizer, camera=camera)\n",
    "    flame = hydra.utils.instantiate(cfg.model)\n",
    "    return flame, renderer\n",
    "\n",
    "\n",
    "def render_output(renderer, optimizer, out, batch):\n",
    "    renderer.update(scale=1)\n",
    "    pred_out = optimizer.flame.render(renderer, out[\"params\"])\n",
    "    gt_out = optimizer.flame.render(renderer, batch[\"params\"])\n",
    "    error_map = visualize_point2plane_error(\n",
    "        s_point=gt_out[\"point\"][0],\n",
    "        t_normal=pred_out[\"normal\"][0],\n",
    "        t_point=pred_out[\"point\"][0],\n",
    "        t_mask=pred_out[\"mask\"][0],\n",
    "        max_error=2e-03,  # 2mm\n",
    "    )\n",
    "    renderer.update(scale=8)\n",
    "    color = pred_out[\"color\"][0].detach().cpu()\n",
    "    normal = pred_out[\"normal_image\"][0].detach().cpu()\n",
    "    return color, normal, error_map\n",
    "\n",
    "\n",
    "def render(renderer, optimizer, out, batch):\n",
    "    renderer.update(scale=1)\n",
    "    pred_out = optimizer.flame.render(renderer, out[\"params\"])\n",
    "    gt_out = optimizer.flame.render(renderer, batch[\"params\"])\n",
    "    error_map = visualize_point2plane_error(\n",
    "        s_point=gt_out[\"point\"][0],\n",
    "        t_normal=pred_out[\"normal\"][0],\n",
    "        t_point=pred_out[\"point\"][0],\n",
    "        t_mask=pred_out[\"mask\"][0],\n",
    "        max_error=6e-03,  # 2mm\n",
    "    )\n",
    "    renderer.update(scale=8)\n",
    "    color = pred_out[\"color\"][0].detach().cpu()\n",
    "    return color, error_map\n",
    "\n",
    "\n",
    "def load_neural_optimizer(flame, renderer, path, override=[]):\n",
    "    cfg = load_config(\"train\", [\"data=kinect\"] + override)\n",
    "    correspondence = hydra.utils.instantiate(cfg.correspondence)\n",
    "    weighting = hydra.utils.instantiate(cfg.weighting)\n",
    "    residuals = hydra.utils.instantiate(cfg.residuals)\n",
    "    regularize = hydra.utils.instantiate(cfg.regularize)\n",
    "    neural_optimizer = NeuralOptimizer.load_from_checkpoint(\n",
    "        path,\n",
    "        renderer=renderer,\n",
    "        flame=flame,\n",
    "        correspondence=correspondence,\n",
    "        regularize=regularize,\n",
    "        residuals=residuals,\n",
    "        weighting=weighting,\n",
    "    )\n",
    "    return neural_optimizer\n",
    "\n",
    "\n",
    "def load_icp_optimizer(flame, renderer, overrides):\n",
    "    cfg = load_config(\"train\", [\"data=kinect\", \"optimizer.output_dir=none\"] + overrides)\n",
    "    correspondence = hydra.utils.instantiate(cfg.correspondence)\n",
    "    weighting = hydra.utils.instantiate(cfg.weighting)\n",
    "    residuals = hydra.utils.instantiate(cfg.residuals)\n",
    "    optimizer = hydra.utils.instantiate(cfg.optimizer)\n",
    "    regularize = hydra.utils.instantiate(cfg.regularize)\n",
    "    icp_optimizer = hydra.utils.instantiate(\n",
    "        cfg.framework,\n",
    "        flame=flame,\n",
    "        logger=None,\n",
    "        renderer=renderer,\n",
    "        correspondence=correspondence,\n",
    "        regularize=regularize,\n",
    "        residuals=residuals,\n",
    "        optimizer=optimizer,\n",
    "        weighting=weighting,\n",
    "    )\n",
    "    return icp_optimizer.to(\"cuda\")\n",
    "\n",
    "\n",
    "# setup the datamodule\n",
    "def load_datamodule(renderer, start_frame, end_frame, jump_size=1):\n",
    "    cfg = load_config(\"train\", [\"data=kinect\"])\n",
    "    datamodule = hydra.utils.instantiate(\n",
    "        cfg.data,\n",
    "        renderer=renderer,\n",
    "        val_dataset=dict(\n",
    "            start_frame=start_frame, end_frame=end_frame, jump_size=jump_size\n",
    "        ),\n",
    "    )\n",
    "    datamodule.setup(\"fit\")\n",
    "    return datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadings\n",
    "flame, renderer = load_flame_renderer()\n",
    "ours = \"/home/borth/GuidedResearch/checkpoints/kinect/ours.ckpt\"\n",
    "ours_wo_prior = \"/home/borth/GuidedResearch/checkpoints/kinect/wo_prior.ckpt\"\n",
    "ours_syn = \"/home/borth/GuidedResearch/checkpoints/kinect/synthetic.ckpt\"\n",
    "# ours = \"/home/borth/GuidedResearch/logs/2024-10-14/19-05-04_train_kinect/checkpoints/epoch_159.ckpt\"\n",
    "# ours_wo_prior = \"/home/borth/GuidedResearch/logs/2024-10-14/19-05-04_train_kinect_wo_prior/checkpoints/epoch_159.ckpt\"\n",
    "# ours_syn = \"/home/borth/GuidedResearch/logs/2024-10-15/05-45-43_train_synthetic/checkpoints/epoch_779.ckpt\"\n",
    "setup = \"kinect\"\n",
    "\n",
    "settings = [\n",
    "    (\"christoph_mouthmove\", 48, 52),\n",
    "    (\"christoph_smile\", 77, 84),\n",
    "    (\"innocenzo_fulgintl_rotatemouth\", 78, 82),\n",
    "    (\"innocenzo_fulgintl_rotatemouth\", 47, 49),\n",
    "]\n",
    "\n",
    "\n",
    "for setting in settings:\n",
    "    dataset, source_idx, target_idx = setting \n",
    "\n",
    "    # Ours\n",
    "    path = ours\n",
    "    optimizer = load_neural_optimizer(flame, renderer, path)\n",
    "    out, batch = eval_iterations(optimizer, renderer, dataset, target_idx, source_idx)\n",
    "    color, _, error = render_output(renderer, optimizer, out, batch)\n",
    "    draw_and_save(color, f\"results/{setup}/ours_color_{dataset}_{target_idx}.png\")\n",
    "    draw_and_save(error, f\"results/{setup}/ours_error_{dataset}_{target_idx}.png\")\n",
    "\n",
    "    # path = ours_syn\n",
    "    # optimizer = load_neural_optimizer(flame, renderer, path)\n",
    "    # out, batch = eval_iterations(optimizer, renderer, dataset, target_idx, source_idx)\n",
    "    # color, _, error = render_output(renderer, optimizer, out, batch)\n",
    "    # draw_and_save(color, f\"results/{setup}/ours_syn_color_{dataset}_{target_idx}.png\")\n",
    "    # draw_and_save(error, f\"results/{setup}/ours_syn_error_{dataset}_{target_idx}.png\")\n",
    "\n",
    "    # path = ours_wo_prior\n",
    "    # override = [\"residuals=face2face_wo_landmarks\", \"regularize=dummy\"]\n",
    "    # optimizer = load_neural_optimizer(flame, renderer, path, override)\n",
    "    # out, batch = eval_iterations(optimizer, renderer, dataset, target_idx, source_idx)\n",
    "    # color, _, error = render_output(renderer, optimizer, out, batch)\n",
    "    # draw_and_save(color, f\"results/{setup}/ours_wo_prior_color_{dataset}_{target_idx}.png\")\n",
    "    # draw_and_save(error, f\"results/{setup}/ours_wo_prior_error_{dataset}_{target_idx}.png\")\n",
    "\n",
    "    # ICP\n",
    "    override = [\"residuals=face2face_wo_landmarks\", \"regularize=dummy\", \"weighting=dummy\"]\n",
    "    optimizer = load_icp_optimizer(flame, renderer, override)\n",
    "    out, batch = eval_iterations(optimizer, renderer, dataset, target_idx, source_idx)\n",
    "    color, _, error = render_output(renderer, optimizer, out, batch)\n",
    "    draw_and_save(color, f\"results/{setup}/icp_color_{dataset}_{target_idx}.png\")\n",
    "    draw_and_save(error, f\"results/{setup}/icp_error_{dataset}_{target_idx}.png\")\n",
    "\n",
    "    # override = [\"residuals=face2face_wo_landmarks\", \"regularize=dummy\", \"weighting=dummy\"]\n",
    "    # optimizer = load_icp_optimizer(flame, renderer, override)\n",
    "    # out, batch = eval_iterations(optimizer, renderer, dataset, target_idx, source_idx, step_size=0.3)\n",
    "    # color, _, error = render_output(renderer, optimizer, out, batch)\n",
    "    # draw_and_save(color, f\"results/{setup}/icp_03_color_{dataset}_{target_idx}.png\")\n",
    "    # draw_and_save(error, f\"results/{setup}/icp_03_error_{dataset}_{target_idx}.png\")\n",
    "\n",
    "    # Base \n",
    "    out[\"params\"] = batch[\"init_params\"]\n",
    "    color, _, error = render_output(renderer, optimizer, out, batch)\n",
    "    draw_and_save(color, f\"results/{setup}/init_color_{dataset}_{target_idx}.png\")\n",
    "    out[\"params\"] = batch[\"params\"]\n",
    "    color, _, error = render_output(renderer, optimizer, out, batch)\n",
    "    draw_and_save(color, f\"results/{setup}/gt_color_{dataset}_{target_idx}.png\")\n",
    "\n",
    "    # Scan and GT Information\n",
    "    draw_and_save_overlay(optimizer, renderer, batch[\"init_params\"], dataset, source_idx, f\"results/{setup}/overlay_source_{dataset}_{target_idx}.png\")\n",
    "    draw_and_save_overlay(optimizer, renderer, batch[\"params\"], dataset, target_idx, f\"results/{setup}/overlay_target_{dataset}_{target_idx}.png\")\n",
    "    draw_and_save_color(dataset, target_idx, f\"results/{setup}/color_{dataset}_{target_idx}.png\")\n",
    "    draw_and_save_normal(dataset, target_idx, f\"results/{setup}/scan_{dataset}_{target_idx}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadings\n",
    "flame, renderer = load_flame_renderer()\n",
    "ours = \"/home/borth/GuidedResearch/checkpoints/kinect/ours.ckpt\"\n",
    "ours_wo_prior = \"/home/borth/GuidedResearch/checkpoints/kinect/wo_prior.ckpt\"\n",
    "ours_syn = \"/home/borth/GuidedResearch/checkpoints/kinect/synthetic.ckpt\"\n",
    "\n",
    "# settings\n",
    "setup = \"method\"\n",
    "dataset = \"christoph_mouthmove\"\n",
    "source_idx = 48\n",
    "target_idx = 52\n",
    "\n",
    "# Ours\n",
    "path = ours_syn\n",
    "optimizer = load_neural_optimizer(flame, renderer, path)\n",
    "out, batch = eval_iterations(optimizer, renderer, dataset, target_idx, source_idx)\n",
    "color, normal, error = render_output(renderer, optimizer, out, batch)\n",
    "draw_and_save(color, f\"results/{setup}/ours_syn_color_{dataset}_{target_idx}.png\")\n",
    "draw_and_save(normal, f\"results/{setup}/ours_syn_normal_{dataset}_{target_idx}.png\")\n",
    "draw_and_save(error, f\"results/{setup}/ours_syn_error_{dataset}_{target_idx}.png\")\n",
    "\n",
    "# Base \n",
    "out[\"params\"] = batch[\"init_params\"]\n",
    "color, _, error = render_output(renderer, optimizer, out, batch)\n",
    "draw_and_save(color, f\"results/{setup}/init_color_{dataset}_{target_idx}.png\")\n",
    "out[\"params\"] = batch[\"params\"]\n",
    "color, _, error = render_output(renderer, optimizer, out, batch)\n",
    "draw_and_save(color, f\"results/{setup}/gt_color_{dataset}_{target_idx}.png\")\n",
    "\n",
    "# Scan and GT Information\n",
    "draw_and_save_normal(dataset, target_idx, f\"results/{setup}/scan_{dataset}_{target_idx}.png\")\n",
    "draw_and_save_weight(flame, renderer, out, f\"results/{setup}/weight_{dataset}_{target_idx}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadings\n",
    "flame, renderer = load_flame_renderer()\n",
    "ours = \"/home/borth/GuidedResearch/checkpoints/kinect/ours.ckpt\"\n",
    "ours_wo_prior = \"/home/borth/GuidedResearch/checkpoints/kinect/wo_prior.ckpt\"\n",
    "ours_syn = \"/home/borth/GuidedResearch/checkpoints/kinect/synthetic.ckpt\"\n",
    "setup = \"teaser\"\n",
    "\n",
    "settings = []\n",
    "for i in range(20, 120, 2):\n",
    "    settings.append((\"ali_kocal_mouthmove\", i, i+1))\n",
    "    settings.append((\"innocenzo_fulgintl_rotatemouth\", i, i+1))\n",
    "\n",
    "# settings = [\n",
    "#     (\"ali_kocal_mouthmove\", 53, 54),\n",
    "#     (\"ali_kocal_mouthmove\", 54, 55),\n",
    "#     (\"ali_kocal_mouthmove\", 54, 56),\n",
    "#     (\"ali_kocal_mouthmove\", 56, 57),\n",
    "\n",
    "#     (\"ali_kocal_mouthmove\", 115, 116),\n",
    "#     (\"ali_kocal_mouthmove\", 118, 119),\n",
    "#     (\"ali_kocal_mouthmove\", 121, 122),\n",
    "\n",
    "#     (\"innocenzo_fulgintl_rotatemouth\", 75, 76),\n",
    "#     (\"innocenzo_fulgintl_rotatemouth\", 77, 78),\n",
    "#     (\"innocenzo_fulgintl_rotatemouth\", 81, 82),\n",
    "\n",
    "#     (\"innocenzo_fulgintl_rotatemouth\", 65, 66),\n",
    "#     (\"innocenzo_fulgintl_rotatemouth\", 67, 68),\n",
    "#     (\"innocenzo_fulgintl_rotatemouth\", 71, 72),\n",
    "\n",
    "#     (\"innocenzo_fulgintl_rotatemouth\", 113, 114),\n",
    "#     (\"innocenzo_fulgintl_rotatemouth\", 116, 117),\n",
    "#     (\"innocenzo_fulgintl_rotatemouth\", 119, 120),\n",
    "# ]\n",
    "\n",
    "for setting in settings:\n",
    "    dataset, source_idx, target_idx = setting \n",
    "\n",
    "    # Ours\n",
    "    path = ours_syn\n",
    "    optimizer = load_neural_optimizer(flame, renderer, path)\n",
    "    out, batch = eval_iterations(optimizer, renderer, dataset, target_idx, source_idx)\n",
    "    color, _, error = render_output(renderer, optimizer, out, batch)\n",
    "    draw_and_save(color, f\"results/{setup}/ours_syn_color_{dataset}_{target_idx}.png\")\n",
    "\n",
    "    # Scan and GT Information\n",
    "    draw_and_save_color(dataset, target_idx, f\"results/{setup}/color_{dataset}_{target_idx}.png\")\n",
    "    draw_and_save_normal(dataset, target_idx, f\"results/{setup}/scan_{dataset}_{target_idx}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
