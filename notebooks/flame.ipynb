{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flame\n",
    "\n",
    "In order to download the models we need to look at: https://flame.is.tue.mpg.de/download.php\n",
    "This could be usefull if onw want's to look how to load the FLAME model from the SMPL loader: https://github.com/Rubikplayer/flame-fitting/blob/master/smpl_webuser/serialization.py#L117\n",
    "Useful utils if one needs to transform the chumpy format into nupy or torch: https://github.com/vchoutas/smplx/blob/main/smplx/utils.py\n",
    "\n",
    "NOTE: That if one want't to unpickle old python=2.x numpy code, we need to use the encoding=\"latin1\". For more information please refere to: https://docs.python.org/3/library/pickle.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.utils.loader import load_flame\n",
    "from lib.model.flame import FLAME\n",
    "\n",
    "# https://github.com/soubhiksanyal/FLAME_PyTorch/blob/master/flame_pytorch/flame.py\n",
    "data_dir = \"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\"\n",
    "flame_dir = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023\"\n",
    "flame_dict = load_flame(flame_dir)\n",
    "flame_model = FLAME(flame_dir=flame_dir, data_dir=data_dir)\n",
    "print(\"FLAME keys:\")\n",
    "print(list(flame_dict.keys()))\n",
    "print()\n",
    "\n",
    "# This is the linear blend skinning (LBS) with corrective blendshapes with N=5023 and\n",
    "# K=4 joint (neck, jaw, and eyeballs (left, right))\n",
    "bs_style = flame_dict[\"bs_style\"]\n",
    "print(\"bs_style:\", bs_style)\n",
    "bs_type = flame_dict[\"bs_type\"]\n",
    "print(\"bs_type:\", bs_type)\n",
    "\n",
    "# this is the template mesh, e.g. T bar in the \"zero pose\"\n",
    "v_template = flame_dict[\"v_template\"]\n",
    "print(\"v_template:\", v_template.shape)\n",
    "for i in range(3):\n",
    "    d = v_template[:, i].max() - v_template[:, i].min()\n",
    "    s = [\"x\", \"y\", \"z\"][i]\n",
    "    print(f\"{s}-delta in meter {d:.2}m\")\n",
    "\n",
    "# those are used in the pytorch flame example\n",
    "f = flame_dict[\"f\"]\n",
    "print(\"f:\", f.shape)\n",
    "\n",
    "# shape (beta); note that the dimension is (5023, 3, 400)\n",
    "# where the first 300 are for the shape params and the last 400 for the expression\n",
    "# params, but the matrix is shared\n",
    "shapedirs = flame_dict[\"shapedirs\"]\n",
    "print(\"shapedirs:\", shapedirs.shape)\n",
    "\n",
    "# pose (theta)\n",
    "posedirs = flame_dict[\"posedirs\"]\n",
    "print(\"posedirs:\", posedirs.shape)\n",
    "\n",
    "# is this the expressions? (psi)\n",
    "weights = flame_dict[\"weights\"]  # lbs := linear blend shapes\n",
    "print(\"weights:\", weights.shape)\n",
    "\n",
    "# Linear smoothed by skinning function(T, J, theta, W).\n",
    "# Blendweights W (KxN) are J_regressor\n",
    "J_regressor = flame_dict[\"J_regressor\"]\n",
    "print(\"J_regressor:\", J_regressor.shape)\n",
    "\n",
    "# J are the joints that the vertices of T are rotated\n",
    "J = flame_dict[\"J\"]\n",
    "print(\"J:\", J.shape)\n",
    "\n",
    "kintree_table = flame_dict[\"kintree_table\"]\n",
    "print(\"kintree_table:\", kintree_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Falme Landmarks\n",
    "\n",
    "The landmark file defines the barycentric embedding of 105 points of the Mediapipe mesh in the surface of FLAME.\n",
    "In consists of three arrays: lmk_face_idx, lmk_b_coords, and landmark_indices.\n",
    "\n",
    "- lmk_face_idx contains for every landmark the index of the FLAME triangle which each landmark is embedded into\n",
    "- lmk_b_coords are the barycentric weights for each vertex of the triangles\n",
    "- landmark_indices are the indices of the vertices of the Mediapipe mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.loader import load_static_landmark_embedding\n",
    "\n",
    "flame_landmarks = load_static_landmark_embedding(flame_dir, \"pt\")\n",
    "print(list(flame_landmarks.keys()))\n",
    "print()\n",
    "\n",
    "print(\"lmk_face_idx:\")\n",
    "print(flame_landmarks[\"lm_face_idx\"][:5])\n",
    "print(flame_landmarks[\"lm_face_idx\"].min())\n",
    "print(flame_landmarks[\"lm_face_idx\"].max())\n",
    "print(flame_landmarks[\"lm_face_idx\"].shape)\n",
    "print()\n",
    "\n",
    "print(\"lmk_b_coords:\")\n",
    "print(flame_landmarks[\"lm_bary_coords\"][:5])\n",
    "print(flame_landmarks[\"lm_bary_coords\"].min())\n",
    "print(flame_landmarks[\"lm_bary_coords\"].max())\n",
    "print(flame_landmarks[\"lm_bary_coords\"].shape)\n",
    "print()\n",
    "\n",
    "print(\"landmark_indices:\")\n",
    "print(flame_landmarks[\"lm_mediapipe_idx\"][:5])\n",
    "print(flame_landmarks[\"lm_mediapipe_idx\"].min())\n",
    "print(flame_landmarks[\"lm_mediapipe_idx\"].max())\n",
    "print(flame_landmarks[\"lm_mediapipe_idx\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAME Mask\n",
    "\n",
    "Dictionary with vertex indices for different masks for the publicly available FLAME head model (https://flame.is.tue.mpg.de/).\n",
    "See the gif for a visualization of all masks.\n",
    "\n",
    "Those are the vertices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.loader import load_flame_masks\n",
    "import torch\n",
    "\n",
    "flame_masks = load_flame_masks(flame_dir)\n",
    "print(list(flame_masks.keys()))\n",
    "print()\n",
    "\n",
    "print(\"face:\")\n",
    "print(flame_masks[\"face\"][:5])\n",
    "print(flame_masks[\"face\"].min())\n",
    "print(flame_masks[\"face\"].max())\n",
    "print(flame_masks[\"face\"].shape)\n",
    "\n",
    "faces_mask = torch.tensor(flame_masks[\"face\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flame_masks[\"face\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch3D Rasterizer\n",
    "\n",
    "We want to implement our own rasterizer, hence we can look how pytorch metric is doing it:\n",
    "from pytorch3d.renderer.mesh import rasterize_meshes\n",
    "Or we can implmenet it, for reference here:\n",
    "https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation/rasterization-stage.html\n",
    "\n",
    "Go over the rasterization:\n",
    "https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation/overview-rasterization-algorithm.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.model.flame import FLAME\n",
    "from lib.rasterizer import Rasterizer\n",
    "from lib.renderer.renderer import Renderer\n",
    "from lib.renderer.camera import Camera\n",
    "from lib.utils.loader import load_intrinsics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "flame_dir = \"/home/borth/GuidedResearch/checkpoints/flame2023\"\n",
    "data_dir = \"/home/borth/GuidedResearch/data/dphm_christoph_mouthmove\"\n",
    "flame = FLAME(\n",
    "    flame_dir=flame_dir,\n",
    "    data_dir=data_dir,\n",
    "    optimize_shapes=1,\n",
    "    optimize_frames=100,\n",
    ")\n",
    "flame.init_params_dphm()\n",
    "K = load_intrinsics(data_dir, return_tensor=\"pt\")\n",
    "camera = Camera(K=K, width=1920, height=1080, near=0.01, far=100, scale=1.0)\n",
    "rasterizer = Rasterizer(width=camera.width, height=camera.height)\n",
    "flame.init_renderer(camera=camera, rasterizer=rasterizer)\n",
    "flame = flame.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flame.shape_params.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_idx = torch.tensor([0] * 5).to(\"cuda\")\n",
    "frame_idx = torch.tensor([0] * 5).to(\"cuda\")\n",
    "batch = {\"shape_idx\": shape_idx, \"frame_idx\": frame_idx}\n",
    "out = flame.optimization_step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "shape_idx = torch.tensor([0] * 5).to(\"cuda\")\n",
    "frame_idx = torch.tensor([0] * 5).to(\"cuda\")\n",
    "batch = {\"shape_idx\": shape_idx, \"frame_idx\": frame_idx}\n",
    "model = flame.model_step(batch)\n",
    "n = 100\n",
    "mt = 0\n",
    "rt = 0\n",
    "for _ in range(n):\n",
    "    # s = time.time()\n",
    "    # model = flame.model_step(batch)\n",
    "    # e = time.time() \n",
    "    # mt += e - s\n",
    "\n",
    "    s = time.time()\n",
    "    render = flame.render_step(model)\n",
    "    e = time.time()\n",
    "    rt += e - s\n",
    "\n",
    "\n",
    "print(\"Model Step:\")\n",
    "print(f\"Total time: {mt}s\")\n",
    "print(f\"Mean time: {(mt / n) * 1000} ms\")\n",
    "\n",
    "print(\"Render Step:\")\n",
    "print(f\"Total time: {rt}s\")\n",
    "print(f\"Mean time: {(rt / n) * 1000} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Model Step:\")\n",
    "print(f\"Total time: {mt}s\")\n",
    "print(f\"Mean time: {(mt / n) * 1000} ms\")\n",
    "\n",
    "scale = 0.5\n",
    "height = int(1080 * scale)\n",
    "width = int(1920 * scale)\n",
    "\n",
    "near = 1.0\n",
    "far = 100\n",
    "fx = 914.4150 * scale\n",
    "fy = 914.0300 * scale\n",
    "cx = 959.5980 * scale\n",
    "cy = 547.2020 * scale\n",
    "A = near + far\n",
    "B = near * far\n",
    "\n",
    "l = 0\n",
    "r = width\n",
    "b = height\n",
    "t = 0\n",
    "# l = -width/2\n",
    "# r = width/2\n",
    "# b = -height/2\n",
    "# t = height\n",
    "\n",
    "tx = -(r + l) / (r - l)\n",
    "ty = -(t + b) / (t - b)\n",
    "tz = -(far + near) / (far - near)\n",
    "\n",
    "Persp = torch.tensor(\n",
    "    [\n",
    "        [fx, 0.0, -cx, 0.0],\n",
    "        [0.0, -fy, -cy, 0.0],\n",
    "        [0.0, 0.0, A, B],\n",
    "        [0.0, 0.0, -1.0, 0.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "NDC = torch.tensor(\n",
    "    [\n",
    "        [2 / (r - l), 0.0, 0.0, tx],\n",
    "        [0.0, 2 / (t - b), 0.0, ty],\n",
    "        [0.0, 0.0, -2 / (far - near), tz],\n",
    "        [0.0, 0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "Proj = NDC @ Persp\n",
    "Proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model.flame import FLAME\n",
    "from lib.rasterizer import Rasterizer\n",
    "from lib.renderer.renderer import Renderer\n",
    "from lib.renderer.camera import Camera\n",
    "from lib.utils.loader import load_intrinsics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "flame_dir = \"/home/borth/GuidedResearch/checkpoints/flame2023_no_jaw\"\n",
    "data_dir = \"/home/borth/GuidedResearch/data/dphm_christoph_mouthmove\"\n",
    "\n",
    "flame = FLAME(\n",
    "    flame_dir,\n",
    "    data_dir=data_dir,\n",
    "    vertices_mask=\"full\",\n",
    ").to(\"cuda\")\n",
    "flame.init_params(\n",
    "    global_pose=[0, 0, 0],\n",
    "    transl=[0, 0, -0.5],\n",
    "    neck_pose=[0.2, 0.2, 0.2],\n",
    "    jaw_pose=[0.3, 0.2, 0.7],\n",
    "    eye_pose=[2, 0.5, 0.1, 0.0, 0.0, 0.0],\n",
    ")\n",
    "vertices, landmarks = flame()\n",
    "K = load_intrinsics(data_dir, return_tensor=\"pt\")\n",
    "camera = Camera(fov_y=45, width=1920, height=1080, near=0.01, far=100, scale=1.0)\n",
    "rasterizer = Rasterizer(width=camera.width, height=camera.height)\n",
    "renderer = Renderer(camera=camera, rasterizer=rasterizer)\n",
    "render = renderer.render_full(vertices, flame.faces)\n",
    "plt.imshow(render[\"normal_image\"][0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = flame.renderer(height=100, width=100, fov=fov, near=near, far=far)\n",
    "render = renderer.render_full(vertices, flame.masked_faces(vertices))\n",
    "plt.imshow(render[\"normal_image\"][0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.renderer.camera import FoVCamera\n",
    "\n",
    "width = 600\n",
    "height = 600\n",
    "renderer.rasterizer.glctx.width = width\n",
    "renderer.rasterizer.glctx.height = height\n",
    "renderer.camera = FoVCamera(\n",
    "    fov, aspect=(width / height), near=1.0, far=100.0, device=\"cuda\"\n",
    ")\n",
    "render = renderer.render_full(\n",
    "    vertices.to(\"cuda\"), flame.masked_faces(vertices).to(\"cuda\")\n",
    ")\n",
    "plt.imshow(render[\"normal_image\"][0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.rasterizer import Rasterizer\n",
    "from lib.renderer.camera import FoVCamera\n",
    "\n",
    "width = 800\n",
    "height = 800\n",
    "fov = 45\n",
    "near = 0.3\n",
    "far = 100\n",
    "\n",
    "renderer = flame.renderer(fov=fov, width=width, height=height, near=near, far=far)\n",
    "# homo_vertices = convert_to_homo_coords(vertices)\n",
    "# homo_clip_vertices = renderer.camera.transfrom(vertices)  # (B, V, 4)\n",
    "# (fragments.pix_to_face == -1).sum()\n",
    "# homo_clip_vertices\n",
    "# camera.M @ homo_vertices[0, 0]\n",
    "\n",
    "render = renderer.render_full(vertices, flame.faces[:1])\n",
    "plt.imshow(render[\"normal_image\"][0].detach().cpu().numpy())\n",
    "# (fragments.pix_to_face != -1).sum(), clip_vertices[:, :, :3].max(),clip_vertices[:, :, :3].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_vertices = clip_vertices[:, :4, :]\n",
    "clip_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flame.faces[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.renderer.camera import FoVCamera\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "camera = FoVCamera(fov=20, aspect=1.0, near=1.0, far=100)\n",
    "clip_v = camera.transfrom(vertices)\n",
    "clip_v[:, :, 0] /= clip_v[:, :, 3]\n",
    "clip_v[:, :, 1] /= clip_v[:, :, 3]\n",
    "clip_v[:, :, 2] /= clip_v[:, :, 3]\n",
    "points = clip_v[0, :, :3].detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "color = [255, 0, 0]\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "color = [np.array(color, dtype=np.uint8)] * points.shape[0]\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.stack(color))\n",
    "\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vertices.detach().cpu().numpy().astype(np.float32)[0]\n",
    "color = [255, 0, 0]\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(v)\n",
    "color = [np.array(color, dtype=np.uint8)] * points.shape[0]\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.stack(color))\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.renderer.camera import FoVCamera\n",
    "\n",
    "camera = FoVCamera(fov=45, aspect=1.0, near=1.0, far=100)\n",
    "pcamera = torch.tensor([0.1, 0.2, -1.2, 1.0])\n",
    "pclip = camera.M @ pcamera\n",
    "print(pclip)\n",
    "pndc = pclip.clone()\n",
    "pndc[0] /= pndc[3]\n",
    "pndc[1] /= pndc[3]\n",
    "pndc[2] /= pndc[3]\n",
    "pndc[3] /= pndc[3]\n",
    "print(pndc)\n",
    "pcamera_inv = camera.M.inverse() @ pclip\n",
    "print(pcamera_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer.camera.transfrom(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv = convert_to_homo_coords(vertices)[0, 0]\n",
    "print(hv)\n",
    "renderer.camera.M @ hv.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(\n",
    "    renderer.camera.M.to(\"cpu\"),\n",
    "    convert_to_homo_coords(vertices).to(\"cpu\").transpose(1, 2),\n",
    ").transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer.camera.M.to(\"cpu\").expand(10, -1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.mesh import vertex_normals\n",
    "\n",
    "renderer = flame.renderer()\n",
    "normal, mask = renderer.render_normal(vertices, flame.faces)\n",
    "normal_image = renderer.normal_to_normal_image(normal, mask)\n",
    "plt.imshow(normal_image[0].detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "color = [255, 0, 0]\n",
    "points = vertices.detach().cpu().numpy().astype(np.float64)[0]\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "color = [np.array(color, dtype=np.uint8)] * points.shape[0]\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.stack(color))\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = list(vertices.shape)\n",
    "shape[-1] = shape[-1] + 1\n",
    "homo_vertices = torch.ones(shape, device=vertices.device)\n",
    "homo_vertices[:, :, :3] = vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.renderer.camera import camera2pixel\n",
    "\n",
    "# fix\n",
    "K = flame.K\n",
    "vp = camera2pixel(landmarks[0], K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "# vp[flame.faces][:, :, :2]\n",
    "# vp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.renderer.camera import camera2normal\n",
    "\n",
    "point, mask = renderer.render_point(vertices, flame.faces)\n",
    "n, m = camera2normal(point)\n",
    "normal_image = renderer.normal_to_normal_image(n, m)\n",
    "plt.imshow(normal_image.detach().cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.loader import load_color\n",
    "\n",
    "color_image = load_color(data_dir=data_dir, idx=0, return_tensor=\"pt\")\n",
    "image = renderer.render_color_image(\n",
    "    vertices, flame_model.faces, color_image, faces_mask, True\n",
    ")\n",
    "plt.imshow(image.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = renderer.render_normal_image(vertices, flame_model.faces, faces_mask)\n",
    "plt.imshow(normal.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = renderer.render_shader_image(vertices, flame_model.faces, faces_mask)\n",
    "plt.imshow(depth.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023_no_jaw/FLAME_texture.npz\"\n",
    "albedo = np.load(path)\n",
    "albedo_faces = albedo[\"vt\"][albedo[\"ft\"]]\n",
    "albedo_map = albedo[\"mean\"].astype(np.uint8)\n",
    "plt.imshow(albedo_map[:, :, ::-1])\n",
    "# albedo_faces.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open3D Point Cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from lib.utils.loader import load_points_3d\n",
    "\n",
    "C = 5023\n",
    "red = [np.array([255, 0, 0], dtype=np.uint8)] * C\n",
    "red = o3d.utility.Vector3dVector(np.stack(red))\n",
    "green = [np.array([0, 255, 0], dtype=np.uint8)] * C\n",
    "green = o3d.utility.Vector3dVector(np.stack(green))\n",
    "blue = [np.array([0, 0, 255], dtype=np.uint8)] * C\n",
    "blue = o3d.utility.Vector3dVector(np.stack(blue))\n",
    "\n",
    "vertices = np.load(\"temp/vertices.npy\")\n",
    "pcd_flame = o3d.geometry.PointCloud()\n",
    "pcd_flame.points = o3d.utility.Vector3dVector(vertices)\n",
    "pcd_flame.colors = blue\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "points = load_points_3d(data_dir=data_dir, idx=0)\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "pcd.colors = red\n",
    "\n",
    "o3d.visualization.draw_plotly([pcd, pcd_flame])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albedo Diffuse\n",
    "\n",
    "This describes how to convert from BFM to FLAME:\n",
    "https://github.com/TimoBolkart/BFM_to_FLAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023_no_jaw/albedoModel2020_FLAME_albedoPart.npz\"\n",
    "albedo = np.load(path)\n",
    "print(list(albedo.keys()))\n",
    "print(f\"{albedo['vt'].shape=}\")\n",
    "print(f\"{albedo['vt'].min()=}\")\n",
    "print(f\"{albedo['vt'].max()=}\")\n",
    "print(f\"{albedo['ft'].shape=}\")\n",
    "print(f\"{albedo['ft'].min()=}\")\n",
    "print(f\"{albedo['ft'].max()=}\")\n",
    "print(f\"{albedo['specPC'].shape=}\")\n",
    "print(f\"{albedo['PC'].shape=}\")\n",
    "print(f\"{albedo['specMU'].shape=}\")\n",
    "print(f\"{albedo['MU'].shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albedo[\"ft\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albedo[\"vt\"].shape[0] - 5023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(albedo[\"MU\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Albedo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023_no_jaw/FLAME_texture.npz\"\n",
    "albedo = np.load(path)\n",
    "albedo_faces = albedo[\"vt\"][albedo[\"ft\"]]\n",
    "albedo_map = albedo[\"mean\"]\n",
    "\n",
    "list(albedo.keys())\n",
    "print(f\"{albedo['vt'].shape=}\")\n",
    "print(f\"{albedo['vt'].min()=}\")\n",
    "print(f\"{albedo['vt'].max()=}\")\n",
    "print(f\"{albedo['ft'].shape=}\")\n",
    "print(f\"{albedo['ft'].min()=}\")\n",
    "print(f\"{albedo['ft'].max()=}\")\n",
    "print(f\"{albedo['tex_dir'].shape=}\")\n",
    "print(f\"{albedo['mean'].shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(albedo[\"mean\"].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flame_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023/FLAME_albedo_from_BFM.npz\"\n",
    "albedo = np.load(path)\n",
    "print(list(albedo.keys()))\n",
    "albedo[\"PC\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.io import load_obj\n",
    "\n",
    "verts, faces, aux = load_obj(\n",
    "    \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023/head_template.obj\"\n",
    ")\n",
    "aux.verts_uvs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces.verts_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts_uvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/robinborth/Code/GuidedResearch/logs/optimize/runs/2024-04-25_17-49-55\"\n",
    "i = 650\n",
    "vertices = np.load(f\"{path}/pcd_vertices/000_{i:05}.npz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
