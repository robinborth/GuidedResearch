{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flame\n",
    "\n",
    "In order to download the models we need to look at: https://flame.is.tue.mpg.de/download.php\n",
    "This could be usefull if onw want's to look how to load the FLAME model from the SMPL loader: https://github.com/Rubikplayer/flame-fitting/blob/master/smpl_webuser/serialization.py#L117\n",
    "Useful utils if one needs to transform the chumpy format into nupy or torch: https://github.com/vchoutas/smplx/blob/main/smplx/utils.py\n",
    "\n",
    "NOTE: That if one want't to unpickle old python=2.x numpy code, we need to use the encoding=\"latin1\". For more information please refere to: https://docs.python.org/3/library/pickle.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from lib.flame.utils import load_flame\n",
    "from lib.flame.model import FLAME\n",
    "\n",
    "# https://github.com/soubhiksanyal/FLAME_PyTorch/blob/master/flame_pytorch/flame.py\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "flame_dir = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023\"\n",
    "flame_dict = load_flame(flame_dir)\n",
    "flame_model = FLAME(flame_dir=flame_dir)\n",
    "print(\"FLAME keys:\")\n",
    "print(list(flame_dict.keys()))\n",
    "print()\n",
    "\n",
    "# This is the linear blend skinning (LBS) with corrective blendshapes with N=5023 and\n",
    "# K=4 joint (neck, jaw, and eyeballs (left, right))\n",
    "bs_style = flame_dict[\"bs_style\"]\n",
    "print(\"bs_style:\", bs_style)\n",
    "bs_type = flame_dict[\"bs_type\"]\n",
    "print(\"bs_type:\", bs_type)\n",
    "\n",
    "# this is the template mesh, e.g. T bar in the \"zero pose\"\n",
    "v_template = flame_dict[\"v_template\"]\n",
    "print(\"v_template:\", v_template.shape)\n",
    "for i in range(3):\n",
    "    d = v_template[:, i].max() - v_template[:, i].min()\n",
    "    s = [\"x\", \"y\", \"z\"][i]\n",
    "    print(f\"{s}-delta in meter {d:.2}m\")\n",
    "\n",
    "# those are used in the pytorch flame example\n",
    "f = flame_dict[\"f\"]\n",
    "print(\"f:\", f.shape)\n",
    "\n",
    "# shape (beta); note that the dimension is (5023, 3, 400)\n",
    "# where the first 300 are for the shape params and the last 400 for the expression\n",
    "# params, but the matrix is shared\n",
    "shapedirs = flame_dict[\"shapedirs\"]\n",
    "print(\"shapedirs:\", shapedirs.shape)\n",
    "\n",
    "# pose (theta)\n",
    "posedirs = flame_dict[\"posedirs\"]\n",
    "print(\"posedirs:\", posedirs.shape)\n",
    "\n",
    "# is this the expressions? (psi)\n",
    "weights = flame_dict[\"weights\"]  # lbs := linear blend shapes\n",
    "print(\"weights:\", weights.shape)\n",
    "\n",
    "# Linear smoothed by skinning function(T, J, theta, W).\n",
    "# Blendweights W (KxN) are J_regressor\n",
    "J_regressor = flame_dict[\"J_regressor\"]\n",
    "print(\"J_regressor:\", J_regressor.shape)\n",
    "\n",
    "# J are the joints that the vertices of T are rotated\n",
    "J = flame_dict[\"J\"]\n",
    "print(\"J:\", J.shape)\n",
    "\n",
    "kintree_table = flame_dict[\"kintree_table\"]\n",
    "print(\"kintree_table:\", kintree_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Falme Landmarks\n",
    "\n",
    "The landmark file defines the barycentric embedding of 105 points of the Mediapipe mesh in the surface of FLAME.\n",
    "In consists of three arrays: lmk_face_idx, lmk_b_coords, and landmark_indices.\n",
    "\n",
    "- lmk_face_idx contains for every landmark the index of the FLAME triangle which each landmark is embedded into\n",
    "- lmk_b_coords are the barycentric weights for each vertex of the triangles\n",
    "- landmark_indices are the indices of the vertices of the Mediapipe mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.flame.utils import load_static_landmark_embedding\n",
    "\n",
    "flame_landmarks = load_static_landmark_embedding(flame_dir)\n",
    "print(list(flame_landmarks.keys()))\n",
    "print()\n",
    "\n",
    "print(\"lmk_face_idx:\")\n",
    "print(flame_landmarks[\"lmk_face_idx\"][:5])\n",
    "print(flame_landmarks[\"lmk_face_idx\"].min())\n",
    "print(flame_landmarks[\"lmk_face_idx\"].max())\n",
    "print(flame_landmarks[\"lmk_face_idx\"].shape)\n",
    "print()\n",
    "\n",
    "print(\"lmk_b_coords:\")\n",
    "print(flame_landmarks[\"lmk_b_coords\"][:5])\n",
    "print(flame_landmarks[\"lmk_b_coords\"].min())\n",
    "print(flame_landmarks[\"lmk_b_coords\"].max())\n",
    "print(flame_landmarks[\"lmk_b_coords\"].shape)\n",
    "print()\n",
    "\n",
    "print(\"landmark_indices:\")\n",
    "print(flame_landmarks[\"landmark_indices\"][:5])\n",
    "print(flame_landmarks[\"landmark_indices\"].min())\n",
    "print(flame_landmarks[\"landmark_indices\"].max())\n",
    "print(flame_landmarks[\"landmark_indices\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAME Mask\n",
    "\n",
    "Dictionary with vertex indices for different masks for the publicly available FLAME head model (https://flame.is.tue.mpg.de/).\n",
    "See the gif for a visualization of all masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.flame.utils import load_flame_masks\n",
    "\n",
    "flame_masks = load_flame_masks(flame_dir)\n",
    "print(list(flame_masks.keys()))\n",
    "print()\n",
    "\n",
    "print(\"eye_region:\")\n",
    "print(flame_masks[\"eye_region\"][:5])\n",
    "print(flame_masks[\"eye_region\"].min())\n",
    "print(flame_masks[\"eye_region\"].max())\n",
    "print(flame_masks[\"eye_region\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAME Forward\n",
    "\n",
    "We use the right hand rule; note that for this rule we have a counter-clockwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "shape_params = torch.zeros(300)\n",
    "# shape_params = torch.randn(300)\n",
    "expression_params = torch.zeros(100)\n",
    "# expression_params = torch.randn(100)\n",
    "\n",
    "vertices_out = flame_model(\n",
    "    shape_params=shape_params,\n",
    "    expression_params=expression_params,\n",
    "    global_pose=torch.tensor([0.0, 0.0, 0.0]),\n",
    "    # global_pose=torch.tensor([np.pi, 0.0, 0.0]),\n",
    "    neck_pose=torch.tensor([0.0, 0.0, 0.0]),\n",
    "    jaw_pose=torch.tensor([0.0, 0.0, 0.0]),\n",
    "    eye_pose=torch.tensor([0, 0, 0, 0, 0, 0]),\n",
    "    transl=torch.tensor([0, 0, 0.0]),\n",
    ")\n",
    "vertices = vertices_out.detach().cpu().numpy()\n",
    "\n",
    "np.save(\"temp/vertices\", vertices)\n",
    "print(\"x_min_max\", vertices[:, 0].min(), vertices[:, 0].max())\n",
    "print(\"y_min_max\", vertices[:, 1].min(), vertices[:, 1].max())\n",
    "print(\"z_min_max\", vertices[:, 2].min(), vertices[:, 2].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render with Trimesh & Pyrender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import trimesh\n",
    "from pyrender import OffscreenRenderer, Mesh, Scene, PerspectiveCamera\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# convert to trimesh\n",
    "vertex_colors = np.ones([vertices.shape[0], 4]) * [0.3, 0.3, 0.3, 0.8]\n",
    "mesh = trimesh.Trimesh(vertices, flame_model.faces, vertex_colors=vertex_colors)\n",
    "\n",
    "# add mesh to scene\n",
    "scene = Scene()\n",
    "scene.add(Mesh.from_trimesh(mesh))\n",
    "\n",
    "# create camera\n",
    "cam = PerspectiveCamera(yfov=(np.pi / 3.0))\n",
    "cam_pose = np.array(\n",
    "    [\n",
    "        [1.0, 0.0, 0.0, 0.0],\n",
    "        [0.0, 1.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 1.0, 1.0],\n",
    "        [0.0, 0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "cam_node = scene.add(cam, pose=cam_pose)\n",
    "\n",
    "# render the scene\n",
    "renderer = OffscreenRenderer(viewport_width=1024, viewport_height=1024)\n",
    "color, depth = renderer.render(scene)\n",
    "renderer.delete()\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Renderer with Mesh Rasterization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.camera import camera2pixel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "K = dict(fx=512, fy=512, cx=128, cy=128)\n",
    "lm = camera2pixel(vertices, **K).astype(int)\n",
    "\n",
    "img = np.ones((1920, 1080))\n",
    "for point in lm:\n",
    "    u, v, z = point.astype(int)\n",
    "    v = -v\n",
    "    if 0 <= u < 1920 and 0 <= v < 1080:\n",
    "        img[u, v] = 0\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open3D Point Cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "C = 5023\n",
    "red = [np.array([255, 0, 0], dtype=np.uint8)] * C\n",
    "red = o3d.utility.Vector3dVector(np.stack(red))\n",
    "green = [np.array([0, 255, 0], dtype=np.uint8)] * C\n",
    "green = o3d.utility.Vector3dVector(np.stack(green))\n",
    "blue = [np.array([0, 0, 255], dtype=np.uint8)] * C\n",
    "blue = o3d.utility.Vector3dVector(np.stack(blue))\n",
    "\n",
    "vertices = np.load(\"temp/vertices.npy\")\n",
    "pcd_flame = o3d.geometry.PointCloud()\n",
    "pcd_flame.points = o3d.utility.Vector3dVector(vertices)\n",
    "pcd_flame.colors = blue\n",
    "o3d.visualization.draw_plotly([pcd_flame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "from pathlib import Path\n",
    "from lib.data.utils import load_points_3d\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "points = load_points_3d(data_dir=data_dir, idx=0)\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "pcd.colors = red\n",
    "o3d.visualization.draw_plotly([pcd, pcd_flame])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch3D Rasterizer\n",
    "\n",
    "We want to implement our own rasterizer, hence we can look how pytorch metric is doing it:\n",
    "from pytorch3d.renderer.mesh import rasterize_meshes\n",
    "Or we can implmenet it, for reference here:\n",
    "https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation/rasterization-stage.html\n",
    "\n",
    "# Go over the rasterization:\n",
    "\n",
    "https://www.scratchapixel.com/lessons/3d-basic-rendering/rasterization-practical-implementation/overview-rasterization-algorithm.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.renderer.mesh import rasterize_meshes\n",
    "from pytorch3d.structures import Meshes\n",
    "\n",
    "s = 2.0\n",
    "t = torch.tensor([0.0, 0.0, 3.0])\n",
    "image_size = 256\n",
    "\n",
    "fixed_vetices = (vertices_out.clone() * s) + t\n",
    "fixed_faces = flame_model.faces.clone()\n",
    "meshes_screen = Meshes(verts=[fixed_vetices], faces=[fixed_faces])\n",
    "\n",
    "pix_to_face, zbuf, bary_coords, dist = rasterize_meshes(\n",
    "    meshes_screen,\n",
    "    image_size=image_size,\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    "    bin_size=None,\n",
    "    max_faces_per_bin=None,\n",
    "    perspective_correct=False,\n",
    ")\n",
    "# pix_to_face = pix_to_face.squeeze()\n",
    "# bary_coords = bary_coords.squeeze()\n",
    "# dist = dist.squeeze()\n",
    "\n",
    "# print(dist.min(), dist.max())\n",
    "# plt.imshow(dist.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Albedo Diffuse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023_no_jaw/albedoModel2020_FLAME_albedoPart.npz\"\n",
    "albedo = np.load(path)\n",
    "print(list(albedo.keys()))\n",
    "print(f\"{albedo['vt'].shape=}\")\n",
    "print(f\"{albedo['vt'].min()=}\")\n",
    "print(f\"{albedo['vt'].max()=}\")\n",
    "print(f\"{albedo['ft'].shape=}\")\n",
    "print(f\"{albedo['ft'].min()=}\")\n",
    "print(f\"{albedo['ft'].max()=}\")\n",
    "print(f\"{albedo['specPC'].shape=}\")\n",
    "print(f\"{albedo['PC'].shape=}\")\n",
    "print(f\"{albedo['specMU'].shape=}\")\n",
    "print(f\"{albedo['MU'].shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(albedo[\"MU\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Albedo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023_no_jaw/FLAME_texture.npz\"\n",
    "albedo = np.load(path)\n",
    "\n",
    "list(albedo.keys())\n",
    "print(f\"{albedo['vt'].shape=}\")\n",
    "print(f\"{albedo['vt'].min()=}\")\n",
    "print(f\"{albedo['vt'].max()=}\")\n",
    "print(f\"{albedo['ft'].shape=}\")\n",
    "print(f\"{albedo['ft'].min()=}\")\n",
    "print(f\"{albedo['ft'].max()=}\")\n",
    "print(f\"{albedo['tex_dir'].shape=}\")\n",
    "print(f\"{albedo['mean'].shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(albedo[\"mean\"].astype(int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
