{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import hydra\n",
    "import torch\n",
    "from lib.utils.config import load_config\n",
    "from lib.optimizer.framework import NeuralOptimizer\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.rasterizer import Rasterizer\n",
    "from lib.renderer.renderer import Renderer\n",
    "from lib.renderer.camera import Camera\n",
    "from lib.utils.visualize import visualize_point2plane_error\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.utils.visualize import visualize_merged\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def path_to_abblation(path):\n",
    "    return \"_\".join(path.split(\"/\")[-3].split(\"_\")[1:])\n",
    "\n",
    "\n",
    "def draw_and_save_color(dataset, idx, path):\n",
    "    _path = f\"/home/borth/GuidedResearch/data/dphm_kinect/{dataset}/color/{idx:05}.png\"\n",
    "    img = Image.open(_path)\n",
    "    img.save(path)\n",
    "    # plt.imshow(img)\n",
    "    # plt.axis(\"off\")  # Hide axes\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def draw_and_save_weight(flame, renderer, out, path):\n",
    "    renderer.update(1)\n",
    "    mask = flame.render(renderer, out[\"params\"])[\"mask\"][0]\n",
    "    renderer.update(8)\n",
    "\n",
    "    # weight inference\n",
    "    weights = out[\"optim_weights\"][-1]\n",
    "    weights = F.interpolate(\n",
    "        weights.unsqueeze(0), scale_factor=8, mode=\"bilinear\", align_corners=False\n",
    "    )\n",
    "    weights = weights.detach()[0][0]\n",
    "    weights[~mask] = 0.0\n",
    "\n",
    "    plt.figure(figsize=(19.2, 10.8), dpi=100)  # Full HD size\n",
    "    plt.imshow(weights.cpu().numpy())\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.savefig(path, bbox_inches=\"tight\", pad_inches=0)  # Save without padding\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_and_save_overlay(optimizer, renderer, params, dataset, idx, path):\n",
    "    _path = f\"/home/borth/GuidedResearch/data/dphm_kinect/{dataset}/color/{idx:05}.png\"\n",
    "    color = torch.tensor(np.asarray(Image.open(_path))).unsqueeze(0).to(\"cuda\")\n",
    "    renderer.update(scale=1)\n",
    "    out = optimizer.flame.render(renderer, params)\n",
    "    renderer.update(scale=8)\n",
    "    img = visualize_merged(\n",
    "        s_color=color,\n",
    "        t_color=out[\"color\"],\n",
    "        t_mask=out[\"mask\"],\n",
    "    )\n",
    "    img = img[0].detach().cpu().numpy()\n",
    "    Image.fromarray(img).save(path)\n",
    "\n",
    "\n",
    "def eval_iterations(\n",
    "    optimizer,\n",
    "    renderer,\n",
    "    dataset,\n",
    "    target_frame_idx,\n",
    "    source_frame_idx,\n",
    "    step_size=0.7,\n",
    "    params=None,\n",
    "    N=2,\n",
    "):\n",
    "    cfg = load_config(\"train\", [\"data=kinect\"])\n",
    "    datamodule = hydra.utils.instantiate(\n",
    "        cfg.data,\n",
    "        renderer=renderer,\n",
    "        val_dataset=dict(\n",
    "            start_frame=target_frame_idx,\n",
    "            end_frame=target_frame_idx + 1,\n",
    "            jump_size=target_frame_idx - source_frame_idx,\n",
    "            datasets=[dataset],\n",
    "        ),\n",
    "    )\n",
    "    datamodule.setup(\"fit\")\n",
    "\n",
    "    optimizer.max_iters = N\n",
    "    optimizer.max_optims = 1\n",
    "    optimizer.step_size = step_size\n",
    "    out = None\n",
    "    batch = None\n",
    "    for i, b in enumerate(datamodule.val_dataloader()):\n",
    "        with torch.no_grad():\n",
    "            batch = optimizer.transfer_batch_to_device(b, \"cuda\", 0)\n",
    "            if params is not None:\n",
    "                batch[\"init_params\"] = params\n",
    "            out = optimizer(batch)\n",
    "    return out, batch\n",
    "\n",
    "\n",
    "def draw_and_save(img, path):\n",
    "    # Display and save the error image\n",
    "    Image.fromarray(img.detach().cpu().numpy()).save(path)\n",
    "    # plt.imshow(img)\n",
    "    # plt.axis(\"off\")  # Hide axes\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def load_flame_renderer():\n",
    "    # instanciate similar to training\n",
    "    cfg = load_config(\"train\", [\"data=kinect\"])\n",
    "    K = load_intrinsics(data_dir=cfg.data.intrinsics_dir, return_tensor=\"pt\")\n",
    "    camera = Camera(\n",
    "        K=K,\n",
    "        width=cfg.data.width,\n",
    "        height=cfg.data.height,\n",
    "        near=cfg.data.near,\n",
    "        far=cfg.data.far,\n",
    "        scale=cfg.data.scale,\n",
    "    )\n",
    "    rasterizer = Rasterizer(width=camera.width, height=camera.height)\n",
    "    renderer = Renderer(rasterizer=rasterizer, camera=camera)\n",
    "    flame = hydra.utils.instantiate(cfg.model)\n",
    "    return flame, renderer\n",
    "\n",
    "\n",
    "def render_output(renderer, optimizer, out, batch):\n",
    "    renderer.update(scale=1)\n",
    "    pred_out = optimizer.flame.render(renderer, out[\"params\"])\n",
    "    gt_out = optimizer.flame.render(renderer, batch[\"params\"])\n",
    "    error_map = visualize_point2plane_error(\n",
    "        s_point=gt_out[\"point\"][0],\n",
    "        t_normal=pred_out[\"normal\"][0],\n",
    "        t_point=pred_out[\"point\"][0],\n",
    "        t_mask=pred_out[\"mask\"][0],\n",
    "        max_error=2e-03,  # 2mm\n",
    "    )\n",
    "    renderer.update(scale=8)\n",
    "    color = pred_out[\"color\"][0].detach().cpu()\n",
    "    normal = pred_out[\"normal_image\"][0].detach().cpu()\n",
    "    return color, normal, error_map\n",
    "\n",
    "\n",
    "def render(renderer, optimizer, out, batch):\n",
    "    renderer.update(scale=1)\n",
    "    pred_out = optimizer.flame.render(renderer, out[\"params\"])\n",
    "    gt_out = optimizer.flame.render(renderer, batch[\"params\"])\n",
    "    error_map = visualize_point2plane_error(\n",
    "        s_point=gt_out[\"point\"][0],\n",
    "        t_normal=pred_out[\"normal\"][0],\n",
    "        t_point=pred_out[\"point\"][0],\n",
    "        t_mask=pred_out[\"mask\"][0],\n",
    "        max_error=6e-03,  # 2mm\n",
    "    )\n",
    "    renderer.update(scale=8)\n",
    "    color = pred_out[\"color\"][0].detach().cpu()\n",
    "    return color, error_map\n",
    "\n",
    "\n",
    "def load_neural_optimizer(flame, renderer, path, override=[]):\n",
    "    cfg = load_config(\"train\", [\"data=kinect\"] + override)\n",
    "    correspondence = hydra.utils.instantiate(cfg.correspondence)\n",
    "    weighting = hydra.utils.instantiate(cfg.weighting)\n",
    "    residuals = hydra.utils.instantiate(cfg.residuals)\n",
    "    regularize = hydra.utils.instantiate(cfg.regularize)\n",
    "    neural_optimizer = NeuralOptimizer.load_from_checkpoint(\n",
    "        path,\n",
    "        renderer=renderer,\n",
    "        flame=flame,\n",
    "        correspondence=correspondence,\n",
    "        regularize=regularize,\n",
    "        residuals=residuals,\n",
    "        weighting=weighting,\n",
    "    )\n",
    "    return neural_optimizer\n",
    "\n",
    "\n",
    "def load_icp_optimizer(flame, renderer, overrides):\n",
    "    cfg = load_config(\"train\", [\"data=kinect\", \"optimizer.output_dir=none\"] + overrides)\n",
    "    correspondence = hydra.utils.instantiate(cfg.correspondence)\n",
    "    weighting = hydra.utils.instantiate(cfg.weighting)\n",
    "    residuals = hydra.utils.instantiate(cfg.residuals)\n",
    "    optimizer = hydra.utils.instantiate(cfg.optimizer)\n",
    "    regularize = hydra.utils.instantiate(cfg.regularize)\n",
    "    icp_optimizer = hydra.utils.instantiate(\n",
    "        cfg.framework,\n",
    "        flame=flame,\n",
    "        logger=None,\n",
    "        renderer=renderer,\n",
    "        correspondence=correspondence,\n",
    "        regularize=regularize,\n",
    "        residuals=residuals,\n",
    "        optimizer=optimizer,\n",
    "        weighting=weighting,\n",
    "    )\n",
    "    return icp_optimizer.to(\"cuda\")\n",
    "\n",
    "def predict_mask(image, x_top, x_bottom):\n",
    "    H, W, C = image.shape\n",
    "    p1 = (x_top, 0)\n",
    "    p2 = (x_bottom, H)\n",
    "\n",
    "    if x_top == x_bottom:\n",
    "        mask = mask = torch.zeros((H, W), dtype=torch.bool)\n",
    "        mask[:, :x_top] = 1.0\n",
    "        return mask\n",
    "\n",
    "    m = ((p2[1] - p1[1]) / p2[0]) / (1 - (p1[0] / p2[0]))\n",
    "    b = p1[1] - m * p1[0]\n",
    "    mask = torch.zeros((H, W), dtype=torch.bool)\n",
    "    for _x in range(mask.shape[1]):\n",
    "        for _y in range(mask.shape[0]):\n",
    "            x = (_y - b) / m\n",
    "            mask[_y, _x] = x > _x\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "# setup the datamodule\n",
    "def load_datamodule(renderer, start_frame, end_frame, jump_size=1):\n",
    "    cfg = load_config(\"train\", [\"data=kinect\"])\n",
    "    datamodule = hydra.utils.instantiate(\n",
    "        cfg.data,\n",
    "        renderer=renderer,\n",
    "        val_dataset=dict(\n",
    "            start_frame=start_frame, end_frame=end_frame, jump_size=jump_size\n",
    "        ),\n",
    "    )\n",
    "    datamodule.setup(\"fit\")\n",
    "    return datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from lib.utils.video import create_video\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "\n",
    "for dataset in [\"elias_wohlgemuth_eyeblink\", \"arnefucks_rotatemouth\", \"mykola_mouthmove\", \"medhansh_mouthmove\"]:\n",
    "    scale = 2  # 2\n",
    "    # dataset = \"elias_wohlgemuth_eyeblink\"\n",
    "    offset = 400\n",
    "    margin_h = 20\n",
    "    margin_w = 120\n",
    "\n",
    "    path = Path(f\"/home/borth/GuidedResearch/data/dphm_kinect/{dataset}/cache\")\n",
    "    sequence_length = len(list((path / \"2_color\").iterdir()))\n",
    "    mask = torch.load(path / f\"{scale}_mask/{0:05}.pt\")\n",
    "    H, W = mask.shape\n",
    "\n",
    "    flame, renderer = load_flame_renderer()\n",
    "    override = [\"residuals=face2face_wo_landmarks\", \"regularize=dummy\", \"weighting=dummy\"]\n",
    "    optimizer = load_icp_optimizer(flame, renderer, override)\n",
    "\n",
    "\n",
    "    def predict_mask(image, x_top, x_bottom):\n",
    "        H, W, C = image.shape\n",
    "        p1 = (x_top, 0)\n",
    "        p2 = (x_bottom, H)\n",
    "\n",
    "        if x_top == x_bottom:\n",
    "            mask = mask = torch.zeros((H, W), dtype=torch.bool)\n",
    "            mask[:, :x_top] = 1.0\n",
    "            return mask\n",
    "\n",
    "        m = ((p2[1] - p1[1]) / p2[0]) / (1 - (p1[0] / p2[0]))\n",
    "        b = p1[1] - m * p1[0]\n",
    "        mask = torch.zeros((H, W), dtype=torch.bool)\n",
    "        for _x in range(mask.shape[1]):\n",
    "            for _y in range(mask.shape[0]):\n",
    "                x = (_y - b) / m\n",
    "                mask[_y, _x] = x > _x\n",
    "        return mask\n",
    "\n",
    "\n",
    "    step_size = math.ceil(2 * W / sequence_length) * 2\n",
    "    xs = [*list(range(-W, W, step_size)), *list(range(W, -W, -step_size))]\n",
    "\n",
    "    for idx, x in tqdm(enumerate(xs), total=len(xs)):\n",
    "        x_bottom = x + 1\n",
    "        x_top = x_bottom + offset\n",
    "\n",
    "        cpath = f\"/home/borth/GuidedResearch/data/dphm_kinect/{dataset}/color/{idx:05}.png\"\n",
    "        color = pil_to_tensor(Image.open(cpath)).permute(1, 2, 0)\n",
    "        size = (int(1080 / scale), int(1920 / scale))\n",
    "        color = v2.functional.resize(\n",
    "            inpt=color.permute(2, 0, 1),\n",
    "            size=size,\n",
    "        ).permute(1, 2, 0)\n",
    "\n",
    "        normal_mask = torch.load(path / f\"{scale}_mask/{idx:05}.pt\")\n",
    "        normal = torch.load(path / f\"{scale}_normal/{idx:05}.pt\")\n",
    "        normal = (((normal + 1) / 2) * 255).to(torch.uint8)\n",
    "\n",
    "        img_left = color.clone()\n",
    "        img_right = normal\n",
    "\n",
    "        image = img_left.clone()\n",
    "        mask = predict_mask(image, x_top, x_bottom)\n",
    "        image[mask & normal_mask] = img_right[mask & normal_mask]\n",
    "        plt.imshow(image)\n",
    "\n",
    "        video_dir = f\"/home/borth/GuidedResearch/results/overlay/{dataset}_input/\"\n",
    "        frame_path = Path(video_dir) / f\"{idx:05}.png\"\n",
    "        frame_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        image = image[margin_h:-margin_h, margin_w:-margin_w, :] \n",
    "        Image.fromarray(image.detach().cpu().numpy()).save(frame_path)\n",
    "\n",
    "        params = torch.load(\n",
    "            f\"/home/borth/GuidedResearch/data/dphm_kinect/{dataset}/params/{idx:05}.pt\"\n",
    "        )\n",
    "        params = optimizer.transfer_batch_to_device(params, \"cuda\", 0)\n",
    "        video_dir = f\"/home/borth/GuidedResearch/results/overlay/{dataset}_output/\"\n",
    "        frame_path = Path(video_dir) / f\"{idx:05}.png\"\n",
    "        frame_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        renderer.update(scale=2)\n",
    "        out = optimizer.flame.render(renderer, params)\n",
    "        renderer.update(scale=scale)\n",
    "        \n",
    "        img_left = color\n",
    "        img_right = out[\"color\"][0].detach().cpu()\n",
    "        img_mask = out[\"mask\"][0].detach().cpu()\n",
    "\n",
    "        image = img_left.clone()\n",
    "        image[img_mask] = img_right[img_mask]\n",
    "        image = image[margin_h:-margin_h, margin_w:-margin_w, :]\n",
    "        Image.fromarray(image.detach().cpu().numpy()).save(frame_path)\n",
    "\n",
    "\n",
    "    video_dir = f\"/home/borth/GuidedResearch/results/overlay/{dataset}_input/\"\n",
    "    video_path = f\"/home/borth/GuidedResearch/results/overlay/{dataset}_input.mp4\"\n",
    "    create_video(video_dir=video_dir, video_path=video_path, framerate=16)\n",
    "\n",
    "    video_dir = f\"/home/borth/GuidedResearch/results/overlay/{dataset}_output/\"\n",
    "    video_path = f\"/home/borth/GuidedResearch/results/overlay/{dataset}_output.mp4\"\n",
    "    create_video(video_dir=video_dir, video_path=video_path, framerate=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from lib.utils.video import create_video\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "\n",
    "scale = 2  # 2\n",
    "offset = 400  # 400\n",
    "margin_w = 150\n",
    "margin_h = 0\n",
    "dataset = \"christoph_mouthmove\"\n",
    "setup = \"dataset\"\n",
    "\n",
    "path = Path(f\"/home/borth/GuidedResearch/data/dphm_kinect/{dataset}/cache\")\n",
    "sequence_length = len(list((path / \"2_color\").iterdir()))\n",
    "mask = torch.load(path / f\"{scale}_mask/{0:05}.pt\")\n",
    "H, W = mask.shape\n",
    "\n",
    "flame, renderer = load_flame_renderer()\n",
    "override = [\"residuals=face2face_wo_landmarks\", \"regularize=dummy\", \"weighting=dummy\"]\n",
    "optimizer = load_icp_optimizer(flame, renderer, override)\n",
    "\n",
    "\n",
    "step_size = math.ceil(2 * W / sequence_length)\n",
    "xs = [*list(range(0, W, step_size)), *list(range(W, 0, -step_size))]\n",
    "\n",
    "for idx, x in tqdm(enumerate(xs), total=len(xs)):\n",
    "    x_bottom = x + 1\n",
    "    x_top = x_bottom + offset\n",
    "\n",
    "    cpath = f\"/home/borth/GuidedResearch/data/dphm_kinect/{dataset}/color/{idx:05}.png\"\n",
    "    color = pil_to_tensor(Image.open(cpath)).permute(1, 2, 0)\n",
    "    size = (int(1080 / scale), int(1920 / scale))\n",
    "    color = v2.functional.resize(\n",
    "        inpt=color.permute(2, 0, 1),\n",
    "        size=size,\n",
    "    ).permute(1, 2, 0)\n",
    "\n",
    "    params = torch.load(\n",
    "        f\"/home/borth/GuidedResearch/data/dphm_kinect/{dataset}/params/{idx:05}.pt\"\n",
    "    )\n",
    "    params = optimizer.transfer_batch_to_device(params, \"cuda\", 0)\n",
    "    video_dir = f\"/home/borth/GuidedResearch/results/{setup}/{dataset}/\"\n",
    "    frame_path = Path(video_dir) / f\"{idx:05}.png\"\n",
    "    frame_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    renderer.update(scale=2)\n",
    "    out = optimizer.flame.render(renderer, params)\n",
    "    renderer.update(scale=scale)\n",
    "    \n",
    "    # img_left = color\n",
    "    # img_right = out[\"color\"][0].detach().cpu()\n",
    "    # img_mask = out[\"mask\"][0].detach().cpu()\n",
    "\n",
    "    # image = img_left.clone()\n",
    "    # mask = predict_mask(image, x_top, x_bottom)\n",
    "    # image[mask & img_mask] = img_right[mask & img_mask] \n",
    "    # image = image[margin_h:-margin_h, margin_w:-margin_w, :]\n",
    "    # Image.fromarray(image.detach().cpu().numpy()).save(frame_path)\n",
    "\n",
    "    image = out[\"color\"][0].detach().cpu()\n",
    "    video_dir = f\"/home/borth/GuidedResearch/results/{setup}/{dataset}_params/\"\n",
    "    frame_path = Path(video_dir) / f\"{idx:05}.png\"\n",
    "    frame_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    Image.fromarray(image.detach().cpu().numpy()).save(frame_path)\n",
    "    \n",
    "video_dir = f\"/home/borth/GuidedResearch/results/{setup}/{dataset}/\"\n",
    "video_path = f\"/home/borth/GuidedResearch/results/{setup}/{dataset}.mp4\"\n",
    "create_video(video_dir=video_dir, video_path=video_path, framerate=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
