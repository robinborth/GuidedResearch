{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display The Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.dataset import DPHMDataset\n",
    "import torch\n",
    "from lib.data.loader import load_intrinsics\n",
    "from lib.renderer import Camera\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.data.preprocessing import point2normal, biliteral_filter\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import decode_image\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "import cv2\n",
    "\n",
    "\n",
    "def normal_to_normal_image(normal, mask):\n",
    "    normal_image = (((normal + 1) / 2) * 255).to(torch.uint8)\n",
    "    normal_image[~mask] = 255\n",
    "    return normal_image\n",
    "\n",
    "\n",
    "def depth_to_depth_image(depth):\n",
    "    return (depth.clip(0, 1) * 255).to(torch.uint8)\n",
    "\n",
    "\n",
    "inf_depth: float = 10.0\n",
    "depth_factor: float = 1000\n",
    "mask_threshold: float = 0.7\n",
    "idx: int = 48\n",
    "data_dir = \"/home/borth/GuidedResearch/data/dphm_christoph_mouthmove\"\n",
    "flame_dir = \"/home/borth/GuidedResearch/checkpoints/flame2023_no_jaw\"\n",
    "\n",
    "# load the camera stats\n",
    "K = load_intrinsics(data_dir=data_dir, return_tensor=\"pt\")\n",
    "camera = Camera(K=K, width=1920, height=1080, scale=1, device=\"cpu\")\n",
    "\n",
    "# load the color image\n",
    "path = Path(data_dir) / \"color\" / f\"{idx:05}.png\"\n",
    "color = pil_to_tensor(Image.open(path)).permute(1, 2, 0)\n",
    "\n",
    "# load the depth image and transform to m\n",
    "path = Path(data_dir) / \"depth\" / f\"{idx:05}.png\"\n",
    "img = Image.open(path)\n",
    "raw_depth = pil_to_tensor(img).to(torch.float32)[0]\n",
    "depth = raw_depth / depth_factor  # (H,W)\n",
    "\n",
    "# select the foreground based on a depth threshold\n",
    "f_mask = (depth < mask_threshold) & (depth != 0)\n",
    "depth[~f_mask] = inf_depth\n",
    "\n",
    "# convert pointmap to normalmap\n",
    "point, _ = camera.depth_map_transform(depth)\n",
    "normal, n_mask = point2normal(point)\n",
    "\n",
    "# create the final mask based on normal and depth\n",
    "mask = f_mask & n_mask\n",
    "\n",
    "# mask the default values\n",
    "color[~mask] = 255\n",
    "normal[~mask] = 0\n",
    "depth[~mask] = 0\n",
    "\n",
    "# smooth the normal maps\n",
    "normal = biliteral_filter(\n",
    "    image=normal,\n",
    "    dilation=30,\n",
    "    sigma_color=250,\n",
    "    sigma_space=250,\n",
    ")\n",
    "\n",
    "# create the point maps\n",
    "depth = biliteral_filter(\n",
    "    image=depth,\n",
    "    dilation=30,\n",
    "    sigma_color=150,\n",
    "    sigma_space=150,\n",
    ")\n",
    "point, _ = camera.depth_map_transform(depth)\n",
    "point[~mask] = 0\n",
    "\n",
    "# depth = load_depth(data_dir=data_dir, idx=idx, smooth=True)\n",
    "# normal, _ = depth2normal(torch.tensor(depth), camera)\n",
    "normal_image = normal_to_normal_image(normal, mask)\n",
    "depth_image = depth_to_depth_image(depth)\n",
    "\n",
    "# plt.imshow(depth)\n",
    "# plt.imshow(normal.detach().cpu().numpy())\n",
    "# mask = dataset._load_normal(0)\n",
    "# mask = (((mask + 1) / 2) * 255).to(torch.uint8)\n",
    "# plt.imshow(mask.detach().cpu().numpy())\n",
    "\n",
    "print(point.shape, point.dtype)\n",
    "print(normal.shape, normal.dtype)\n",
    "print(color.shape, color.dtype)\n",
    "print(mask.shape, mask.dtype, mask.sum(), (~mask).sum())\n",
    "plt.imshow(color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medipipe Landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.loader import (\n",
    "    load_mediapipe_image,\n",
    "    load_mediapipe_landmark_2d,\n",
    "    load_mediapipe_landmark_3d,\n",
    ")\n",
    "from lib.model.flame.utils import load_static_landmark_embedding\n",
    "from lib.renderer import Camera\n",
    "\n",
    "landmark = load_mediapipe_landmark_2d(data_dir, idx=idx,  return_tensor=\"pt\")\n",
    "print(f\"{landmark.shape=}\")\n",
    "print(landmark[:5, :])\n",
    "\n",
    "mediapipe_landmarks_3d = load_mediapipe_landmark_3d(data_dir, idx=idx, return_tensor=\"pt\")\n",
    "print(f\"{mediapipe_landmarks_3d.shape=}\")\n",
    "print(mediapipe_landmarks_3d[:5, :])\n",
    "\n",
    "flame_landmarks = load_static_landmark_embedding(flame_dir)\n",
    "media_idx = flame_landmarks[\"lm_mediapipe_idx\"]\n",
    "\n",
    "# camera.unproject_points(mediapipe_landmarks_2d)\n",
    "landmark[:, 0] *= camera.width\n",
    "landmark[:, 1] *= camera.height\n",
    "landmark = landmark[media_idx].long()\n",
    "u = landmark[:, 0]\n",
    "v = landmark[:, 1]\n",
    "landmarks = point[v, u]\n",
    "\n",
    "# plt.scatter(u, v, c=\"red\", s=1.0)\n",
    "# plt.imshow(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.visualize import load_pcd\n",
    "import open3d as o3d\n",
    "\n",
    "points = point[v, u]\n",
    "pcd = load_pcd(points)\n",
    "o3d.visualization.draw_plotly([pcd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flame_landmarks = load_static_landmark_embedding(flame_dir)\n",
    "flame_landmarks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.loader import (\n",
    "    load_mediapipe_landmark_2d,\n",
    "    load_mediapipe_landmark_3d,\n",
    "    load_intrinsics\n",
    ")\n",
    "\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "data_dir = \"/home/borth/GuidedResearch/data/dphm_christoph_mouthmove\"\n",
    "\n",
    "face_idx = 0\n",
    "# face_idx = 110\n",
    "# normal = load_normal(data_dir, image_idx, return_tensor=\"np\")\n",
    "# normal = (((normal + 1) / 2) * 255).astype(np.uint8)\n",
    "K = load_intrinsics(data_dir=data_dir, return_tensor=\"pt\")\n",
    "# face = vertices[:, flame.faces][0][face_idx]\n",
    "# pixel = camera2pixel(face, K[\"fx\"], K[\"fy\"], K[\"cx\"], K[\"cy\"])\n",
    "# pixel = camera2pixel(landmarks[0], K[\"fx\"], K[\"fy\"], K[\"cx\"], K[\"cy\"])\n",
    "\n",
    "# points = media\n",
    "# xc = points[:, 0]\n",
    "# yc = points[:, 1]\n",
    "# zc = points[:, 2]\n",
    "\n",
    "# us = cx + fx * (xc / zc)\n",
    "# vs = cy + fy * (yc / zc)\n",
    "\n",
    "# pixels = (K.to(\"cuda\") @ points.T).T\n",
    "path = \"/home/borth/GuidedResearch/data/christoph_mouthmove/color/00000.png\"\n",
    "color = np.asarray(Image.open(path))\n",
    "\n",
    "\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(color)\n",
    "\n",
    "\n",
    "# draw all of the lm on the screen\n",
    "# x, y, _ = pixel[0]\n",
    "\n",
    "W = 1920\n",
    "H = 1080\n",
    "u = (mediapipe_landmarks_2d[:, 0] * W).astype(np.int64)\n",
    "v = (mediapipe_landmarks_2d[:, 1] * H).astype(np.int64)\n",
    "plt.scatter(u, v, c=\"red\", s=0.5)\n",
    "\n",
    "\n",
    "# for lm in mediapipe_landmarks_2d:\n",
    "#     plt.scatter(\n",
    "#         int(W * lm[0]), int(H * lm[1]), c=\"red\", s=0.5\n",
    "#     )  # Drawing a red point for each landmark\n",
    "# plt.scatter(int(pixel[0,0]), int(pixel[0,1]), c=\"red\", s=2)  # Drawing a red point for each landmark\n",
    "# plt.scatter(int(pixel[1,0]), int(pixel[1,1]), c=\"blue\", s=2)  # Drawing a red point for each landmark\n",
    "# plt.scatter(int(pixel[2,0]), int(pixel[2,1]), c=\"green\", s=2)  # Drawing a red point for each landmark\n",
    "# img = load_plt()\n",
    "# img\n",
    "# plt.show()\n",
    "# plt.imshow(img.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth\n",
    "\n",
    "From https://cvg.cit.tum.de/data/datasets/rgbd-dataset/file_formats\n",
    "\n",
    "The color and depth images are already pre-registered using the OpenNI driver from PrimeSense, i.e., the pixels in the color and depth images correspond already 1:1.\n",
    "\n",
    "The depth images are scaled by a factor of 1000, i.e., a pixel value of 1000 in the depth image corresponds to a distance of 1 meter from the camera. A pixel value of 0 means missing value/no data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model.flame import FLAME\n",
    "from lib.renderer.renderer import Renderer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.utils.mesh import vertex_normals\n",
    "\n",
    "flame_dir = \"/home/borth/GuidedResearch/checkpoints/flame2023\"\n",
    "data_dir = \"/home/borth/GuidedResearch/data/dphm_christoph_mouthmove\"\n",
    "scale = 1.0\n",
    "flame = FLAME(\n",
    "    flame_dir=flame_dir,\n",
    "    data_dir=data_dir,\n",
    "    vertices_mask=\"full\",\n",
    ").to(\"cuda\")\n",
    "flame.init_params(\n",
    "    global_pose=[0.0, 0, 0],\n",
    "    transl=[0.0, 0.0, -0.5],\n",
    ")\n",
    "vertices, landmarks = flame()\n",
    "renderer = flame.renderer()\n",
    "\n",
    "# mesh = trimesh.Trimesh(vertices[0].detach().cpu().numpy(), faces=flame.faces.detach().cpu().numpy())\n",
    "# vn = torch.tensor(mesh.vertex_normals).unsqueeze(0).to(vertices.device)\n",
    "faces = flame.faces[:, [0, 2, 1]]\n",
    "vn = vertex_normals(vertices=vertices, faces=faces)\n",
    "normal, mask = renderer.render(vertices, faces, vn)\n",
    "# normal, mask = renderer.render(vertices,flame.masked_faces(vertices), vn)\n",
    "normal_image = renderer.normal_to_normal_image(normal, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.visualize import load_pcd\n",
    "import open3d as o3d\n",
    "\n",
    "pcd = load_pcd(vertices.detach().cpu().numpy()[0])\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mediapipe_landmarks_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = landmarks[0]\n",
    "landmarks[:, 2] = -landmarks[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal[0][int(pixel[0, 1]), int(pixel[0, 0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model.utils import load_static_landmark_embedding\n",
    "\n",
    "flame_dir = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023\"\n",
    "flame_landmarks = load_static_landmark_embedding(flame_dir)\n",
    "print(flame_landmarks[\"landmark_indices\"][1])\n",
    "print(flame_landmarks[\"lmk_face_idx\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_idx = 0\n",
    "\n",
    "lm3d = load_pipnet_landmark_3d(data_dir, idx=image_idx)[lm_idx]\n",
    "print(lm3d)\n",
    "\n",
    "lm2d = load_pipnet_landmark_2d(data_dir, idx=image_idx)[lm_idx]\n",
    "x, y = lm2d.astype(int)\n",
    "print(lm2d)\n",
    "\n",
    "depth = load_depth_masked(data_dir, image_idx, return_tensor=\"np\", depth_factor=1000)\n",
    "print(depth[y, x])\n",
    "plt.imshow(depth)\n",
    "\n",
    "x, y = lm2d.astype(int)\n",
    "plt.scatter(x, y, c=\"red\", s=10)  # Drawing a red point for each landmark\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.renderer.camera import load_intrinsics, pixel2camera\n",
    "from lib.utils.loader import load_depth_masked\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "scale = 0.5\n",
    "\n",
    "# load the intrinsic\n",
    "_K = load_intrinsics(data_dir=data_dir, return_tensor=\"dict\")\n",
    "K = torch.tensor(\n",
    "    [\n",
    "        [_K[\"fx\"] * scale, 0.0, _K[\"cx\"] * scale],\n",
    "        [0.0, _K[\"fy\"] * scale, _K[\"cy\"] * scale],\n",
    "        [0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load the depth image\n",
    "_depth_masked = load_depth_masked(data_dir, 0, return_tensor=\"pt\")\n",
    "_H, _W = _depth_masked.shape\n",
    "H, W = int(_H * scale), int(_W * scale)\n",
    "\n",
    "# get the mask\n",
    "_mask = _depth_masked == 0.0\n",
    "mask = v2.functional.resize(_mask.unsqueeze(0), size=(H, W)).squeeze(0)\n",
    "mask = ~mask\n",
    "\n",
    "# get the new size of the depth image\n",
    "depth_masked = v2.functional.resize(_depth_masked.unsqueeze(0), size=(H, W)).squeeze(0)\n",
    "\n",
    "# span the pixel indexes\n",
    "x = torch.arange(W)\n",
    "y = torch.arange(H)\n",
    "idx = torch.stack(torch.meshgrid(y, x), dim=-1).flip(-1)\n",
    "\n",
    "# get the points in camera coordinates, but with the new resolution\n",
    "points = torch.concat([idx, depth_masked.unsqueeze(-1)], dim=-1)\n",
    "points[:, :, 0] *= points[:, :, 2]\n",
    "points[:, :, 1] *= points[:, :, 2]\n",
    "out = K.inverse() @ points.permute(2, 0, 1).reshape(3, -1)\n",
    "out = out.reshape(3, points.shape[0], points.shape[1]).permute(1, 2, 0)\n",
    "\n",
    "# just save the point of the face\n",
    "cpoints = out[mask]\n",
    "np.save(\"temp/out\", cpoints.detach().cpu().numpy())\n",
    "# [depth_masked] * 3\n",
    "# depth_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.renderer.camera import depth2camera\n",
    "from lib.utils.loader import load_depth_masked, load_intrinsics\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "depth = load_depth_masked(data_dir=data_dir, idx=0, return_tensor=\"pt\")\n",
    "scale = 0.1\n",
    "K = load_intrinsics(data_dir=data_dir, return_tensor=\"pt\", scale=scale)\n",
    "points = depth2camera(depth, K, scale=scale)\n",
    "plt.imshow(points[:, :, 2])\n",
    "points[:, :, 2].min(), points[:, :, 2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.renderer.camera import camera2normal\n",
    "import torch\n",
    "\n",
    "normals = camera2normal(points.unsqueeze(0))\n",
    "\n",
    "# now show the image\n",
    "b_mask = normals.sum(-1) == 0\n",
    "normals_image = (((normals + 1) / 2) * 255).to(torch.uint8)\n",
    "normals_image[b_mask, :] = 0\n",
    "plt.imshow(normals_image[0])\n",
    "# points.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# https://stackoverflow.com/questions/34644101/calculate-surface-normals-from-depth-image-using-neighboring-pixels-cross-produc/34644939#34644939\n",
    "# they have (-dz/dx,-dz/dy,1, however we are in camera space hence we need to calculate the gradient in pixel space, e.g. also the delta x and delta y are in camera space.\n",
    "\n",
    "# make sure that on the boundary is nothing wrong calculated\n",
    "points[points.sum(-1) == 0] = torch.nan\n",
    "\n",
    "H, W, C = points.shape\n",
    "normals = torch.ones_like(points)\n",
    "normals *= -1\n",
    "\n",
    "# we calculate the normal in camera space, hence we also need to normalize with the depth information,\n",
    "# note that the normal is basically on which direction we have the stepest decent.\n",
    "x_right = torch.arange(2, W)\n",
    "x_left = torch.arange(0, W - 2)\n",
    "normals[:, 1:-1, 0] = (points[:, x_right, 2] - points[:, x_left, 2]) / (\n",
    "    points[:, x_right, 0] - points[:, x_left, 0]\n",
    ")\n",
    "\n",
    "y_right = torch.arange(2, H)\n",
    "y_left = torch.arange(0, H - 2)\n",
    "normals[1:-1, :, 1] = (points[y_right, :, 2] - points[y_left, :, 2]) / (\n",
    "    points[y_right, :, 1] - points[y_left, :, 1]\n",
    ")\n",
    "\n",
    "# normalized between [-1, 1]\n",
    "normals = normals / torch.norm(normals, dim=-1).unsqueeze(-1)\n",
    "normals = torch.nan_to_num(normals, 0)\n",
    "normals[:1, :, :] = 0\n",
    "normals[-1:, :, :] = 0\n",
    "normals[:, :1, :] = 0\n",
    "normals[:, -1:, :] = 0\n",
    "b_mask = normals.sum(-1) == 0\n",
    "\n",
    "\n",
    "# now show the image\n",
    "normals_image = (((normals + 1) / 2) * 255).to(torch.uint8)\n",
    "normals_image[b_mask, :] = 0\n",
    "plt.imshow(normals_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(points[:, x_right, 2] - points[:, x_left, 2]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "camera = np.load(\"temp/out.npy\").reshape(-1, 3)\n",
    "points = camera[camera[:, 2] != 0]\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a point in 3D which is:\n",
    "\n",
    "[-0.051, -0.042, 0.575] (x, y, z)\n",
    "\n",
    "The coresponding pixel value is:\n",
    "\n",
    "[878, 480] (x, y)\n",
    "\n",
    "How do we get from 3D to 2D screen coordinates?\n",
    "\n",
    "Input:\n",
    "fx = 914.415\n",
    "fy = 914.03\n",
    "cx = 959.598\n",
    "cy = 547.202\n",
    "xyz_camera = [-0.051, -0.042, 0.575] (x, y, z_c)\n",
    "\n",
    "Output:\n",
    "uvz_pixel = [878.0, 480.0, 0.575] (u, v, z_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.loader import load_pipnet_landmark_3d\n",
    "from lib.renderer.camera import load_intrinsics, camera2pixel\n",
    "\n",
    "flame_landmarks = load_static_landmark_embedding(flame_dir)\n",
    "lm_idx = flame_landmarks[\"landmark_indices\"]\n",
    "\n",
    "plt.imshow(color)\n",
    "\n",
    "lm3d = load_mediapipe_landmark_3d(data_dir, idx=image_idx)\n",
    "K = load_intrinsics(data_dir=data_dir)\n",
    "lm = camera2pixel(lm3d, **K)\n",
    "for point in lm[lm_idx]:\n",
    "    x, y, z = point.astype(int)\n",
    "    plt.scatter(x, y, c=\"red\", s=1)  # Drawing a red point for each landmark\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normals and Points in 3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.loader import load_normals_3d, load_points_3d\n",
    "import open3d as o3d\n",
    "\n",
    "normals = load_normals_3d(data_dir=data_dir, idx=0)\n",
    "print(f\"{normals.shape=}\")\n",
    "print(normals[:5, :])\n",
    "\n",
    "points = load_points_3d(data_dir=data_dir, idx=0)\n",
    "print(f\"{points.shape=}\")\n",
    "print(points[:5, :])\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "from lib.utils.loader import load_points_3d\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "points = load_points_3d(data_dir=data_dir, idx=0)\n",
    "print(f\"{points.shape=}\")\n",
    "print(points[:5, :])\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = \"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove/camera/c00_color_extrinsic.txt\"\n",
    "E = np.zeros((4, 4))\n",
    "E[3, 3] = 1.0\n",
    "E[:3, :] = np.loadtxt(path).reshape(3, 4)  # extrinsic hence world to camera\n",
    "\n",
    "# note that the pose is the camera to world, e.g. if flame calls them pose they mean\n",
    "# that they project from camera to world coordinates, hence the final mesh vertices lives\n",
    "# in the world coordinate system! This is so important!\n",
    "# note that this is 4x4\n",
    "# we need to project the point from camera to world! because the point cloud is in camera\n",
    "# we can see that because the coordinate system is right-hand where z-axes goes inside and\n",
    "# y-axes goes down, usually z goes to the camera and y up (see cv2 reference)\n",
    "pose = np.linalg.inv(E)  # camera to world, hence this is the \"pose\" they call it that.\n",
    "\n",
    "points_c_homo = np.zeros((points.shape[0], 4))\n",
    "points_c_homo[:, 3] = 1.0\n",
    "points_c_homo[:, :3] = points\n",
    "\n",
    "\n",
    "points_w_homo = (E @ points_c_homo.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_w_homo = (pose[:3, :3] @ points.T).T\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points_w_homo)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points_w_homo[:, :3])\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
