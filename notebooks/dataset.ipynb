{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display The Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from lib.data.utils import (\n",
    "    load_color,\n",
    "    load_depth,\n",
    "    load_mask,\n",
    "    load_color_masked,\n",
    "    load_depth_masked,\n",
    ")\n",
    "\n",
    "image_idx = 0\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "\n",
    "color = load_color(data_dir, image_idx, return_tensor=\"img\")\n",
    "display(color)\n",
    "\n",
    "depth = load_depth(data_dir, image_idx, return_tensor=\"np\")\n",
    "plt.imshow(depth)\n",
    "plt.show()\n",
    "\n",
    "mask = load_mask(data_dir, image_idx, return_tensor=\"np\")\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "\n",
    "color_masked = load_color_masked(data_dir, image_idx, return_tensor=\"img\")\n",
    "display(color_masked)\n",
    "\n",
    "depth_masked = load_depth_masked(data_dir, image_idx, return_tensor=\"np\")\n",
    "plt.imshow(depth_masked)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipnet Landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.utils import (\n",
    "    load_pipnet_image,\n",
    "    load_pipnet_landmark_2d,\n",
    "    load_pipnet_landmark_3d,\n",
    ")\n",
    "\n",
    "pipnet_landmarks_2d = load_pipnet_landmark_2d(data_dir, idx=image_idx)\n",
    "\n",
    "print(f\"{pipnet_landmarks_2d.shape=}\")\n",
    "print(pipnet_landmarks_2d[:5, :])\n",
    "\n",
    "pipnet_landmarks_3d = load_pipnet_landmark_3d(data_dir, idx=image_idx)\n",
    "print(f\"{pipnet_landmarks_3d.shape=}\")\n",
    "print(pipnet_landmarks_3d[:5, :])\n",
    "print(\"depth\", pipnet_landmarks_3d[:, 2])\n",
    "plt.hist(pipnet_landmarks_3d[:, 2])\n",
    "plt.show()\n",
    "\n",
    "pipnet_image = load_pipnet_image(data_dir, idx=image_idx)\n",
    "display(pipnet_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medipipe Landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.utils import (\n",
    "    load_mediapipe_image,\n",
    "    load_mediapipe_landmark_2d,\n",
    "    load_mediapipe_landmark_3d,\n",
    ")\n",
    "\n",
    "mediapipe_landmarks_2d = load_mediapipe_landmark_2d(data_dir, idx=image_idx)\n",
    "\n",
    "print(f\"{mediapipe_landmarks_2d.shape=}\")\n",
    "print(mediapipe_landmarks_2d[:5, :])\n",
    "\n",
    "mediapipe_landmarks_3d = load_mediapipe_landmark_3d(data_dir, idx=image_idx)\n",
    "print(f\"{mediapipe_landmarks_3d.shape=}\")\n",
    "print(mediapipe_landmarks_3d[:5, :])\n",
    "print(\"depth\", mediapipe_landmarks_3d[:50, 2])\n",
    "plt.hist(mediapipe_landmarks_3d[:, 2])\n",
    "plt.show()\n",
    "\n",
    "mediapip_image = load_mediapipe_image(data_dir, idx=image_idx)\n",
    "display(mediapip_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model.utils import load_static_landmark_embedding\n",
    "\n",
    "flame_dir = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023\"\n",
    "flame_landmarks = load_static_landmark_embedding(flame_dir)\n",
    "flame_landmarks[\"landmark_indices\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth\n",
    "\n",
    "From https://cvg.cit.tum.de/data/datasets/rgbd-dataset/file_formats\n",
    "\n",
    "The color and depth images are already pre-registered using the OpenNI driver from PrimeSense, i.e., the pixels in the color and depth images correspond already 1:1.\n",
    "\n",
    "The depth images are scaled by a factor of 1000, i.e., a pixel value of 1000 in the depth image corresponds to a distance of 1 meter from the camera. A pixel value of 0 means missing value/no data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.utils import (\n",
    "    load_pipnet_landmark_2d,\n",
    "    load_pipnet_landmark_3d,\n",
    "    load_depth_masked,\n",
    ")\n",
    "\n",
    "depth = load_depth_masked(data_dir, image_idx, return_tensor=\"np\", depth_factor=1000)\n",
    "plt.imshow(depth)\n",
    "\n",
    "# draw all of the lm on the screen\n",
    "lm = load_mediapipe_landmark_2d(data_dir, idx=image_idx)\n",
    "x, y = lm[282]\n",
    "plt.scatter(\n",
    "    int(x * 1980), int(y * 1080), c=\"red\", s=10\n",
    ")  # Drawing a red point for each landmark\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model.utils import load_static_landmark_embedding\n",
    "\n",
    "flame_dir = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023\"\n",
    "flame_landmarks = load_static_landmark_embedding(flame_dir)\n",
    "print(flame_landmarks[\"landmark_indices\"][1])\n",
    "print(flame_landmarks[\"lmk_face_idx\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_idx = 0\n",
    "\n",
    "lm3d = load_pipnet_landmark_3d(data_dir, idx=image_idx)[lm_idx]\n",
    "print(lm3d)\n",
    "\n",
    "lm2d = load_pipnet_landmark_2d(data_dir, idx=image_idx)[lm_idx]\n",
    "x, y = lm2d.astype(int)\n",
    "print(lm2d)\n",
    "\n",
    "depth = load_depth_masked(data_dir, image_idx, return_tensor=\"np\", depth_factor=1000)\n",
    "print(depth[y, x])\n",
    "plt.imshow(depth)\n",
    "\n",
    "x, y = lm2d.astype(int)\n",
    "plt.scatter(x, y, c=\"red\", s=10)  # Drawing a red point for each landmark\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.renderer.camera import load_intrinsics, pixel2camera\n",
    "from lib.data.utils import load_depth_masked\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "scale = 0.5\n",
    "\n",
    "# load the intrinsic\n",
    "_K = load_intrinsics(data_dir=data_dir, return_tensor=\"dict\")\n",
    "\n",
    "\n",
    "K = torch.tensor(\n",
    "    [\n",
    "        [_K[\"fx\"] * scale, 0.0, _K[\"cx\"] * scale],\n",
    "        [0.0, _K[\"fy\"] * scale, _K[\"cy\"] * scale],\n",
    "        [0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load the depth image\n",
    "_depth_masked = load_depth_masked(data_dir, 0, return_tensor=\"pt\")\n",
    "_H, _W = _depth_masked.shape\n",
    "H, W = int(_H * scale), int(_W * scale)\n",
    "\n",
    "# get the mask\n",
    "_mask = _depth_masked == 0.0\n",
    "mask = v2.functional.resize(_mask.unsqueeze(0), size=(H, W)).squeeze(0)\n",
    "mask = ~mask\n",
    "\n",
    "# get the new size of the depth image\n",
    "depth_masked = v2.functional.resize(_depth_masked.unsqueeze(0), size=(H, W)).squeeze(0)\n",
    "\n",
    "# span the pixel indexes\n",
    "x = torch.arange(W)\n",
    "y = torch.arange(H)\n",
    "idx = torch.stack(torch.meshgrid(y, x), dim=-1).flip(-1)\n",
    "\n",
    "# get the points in camera coordinates, but with the new resolution\n",
    "points = torch.concat([idx, depth_masked.unsqueeze(-1)], dim=-1)\n",
    "points[:, :, 0] *= points[:, :, 2]\n",
    "points[:, :, 1] *= points[:, :, 2]\n",
    "out = K.inverse() @ points.permute(2, 0, 1).reshape(3, -1)\n",
    "out = out.reshape(3, points.shape[0], points.shape[1]).permute(1, 2, 0)\n",
    "\n",
    "# just save the point of the face\n",
    "cpoints = out[mask]\n",
    "np.save(\"temp/out\", cpoints.detach().cpu().numpy())\n",
    "# [depth_masked] * 3\n",
    "# depth_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.renderer.camera import depth2camera\n",
    "from lib.data.utils import load_depth_masked\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "depth = load_depth_masked(data_dir=data_dir, return_tensor=\"pt\")\n",
    "K = load_intrinsics(data_dir=data_dir, return_tensor=\"pt\")\n",
    "camera =  depth2camera(depth, K, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.renderer.camera import load_intrinsics, pixel2camera\n",
    "from lib.data.utils import load_depth_masked\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "depth_masked = load_depth_masked(data_dir, 0, return_tensor=\"np\")\n",
    "\n",
    "H, W = 1080, 1920\n",
    "_K = load_intrinsics(data_dir=data_dir)\n",
    "K = torch.tensor(\n",
    "    [\n",
    "        [_K[\"fx\"], 0.0, _K[\"cx\"]],\n",
    "        [0.0, _K[\"fy\"], _K[\"cy\"]],\n",
    "        [0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "point = torch.tensor([[0, 0, 0.5]])\n",
    "point[:, 0] = point[:, 0] * point[:, 2]\n",
    "point[:, 1] = point[:, 1] * point[:, 2]\n",
    "K.inverse() @ point[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "points = np.load(\"temp/out.npy\")\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a point in 3D which is:\n",
    "\n",
    "[-0.051, -0.042, 0.575] (x, y, z)\n",
    "\n",
    "The coresponding pixel value is:\n",
    "\n",
    "[878, 480] (x, y)\n",
    "\n",
    "How do we get from 3D to 2D screen coordinates?\n",
    "\n",
    "Input:\n",
    "fx = 914.415\n",
    "fy = 914.03\n",
    "cx = 959.598\n",
    "cy = 547.202\n",
    "xyz_camera = [-0.051, -0.042, 0.575] (x, y, z_c)\n",
    "\n",
    "Output:\n",
    "uvz_pixel = [878.0, 480.0, 0.575] (u, v, z_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.utils import load_pipnet_landmark_3d\n",
    "from lib.renderer.camera import load_intrinsics, camera2pixel\n",
    "\n",
    "flame_landmarks = load_static_landmark_embedding(flame_dir)\n",
    "lm_idx = flame_landmarks[\"landmark_indices\"]\n",
    "\n",
    "plt.imshow(color)\n",
    "\n",
    "lm3d = load_mediapipe_landmark_3d(data_dir, idx=image_idx)\n",
    "K = load_intrinsics(data_dir=data_dir)\n",
    "lm = camera2pixel(lm3d, **K)\n",
    "for point in lm[lm_idx]:\n",
    "    x, y, z = point.astype(int)\n",
    "    plt.scatter(x, y, c=\"red\", s=1)  # Drawing a red point for each landmark\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normals and Points in 3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.utils import load_normals_3d, load_points_3d\n",
    "import open3d as o3d\n",
    "\n",
    "normals = load_normals_3d(data_dir=data_dir, idx=0)\n",
    "print(f\"{normals.shape=}\")\n",
    "print(normals[:5, :])\n",
    "\n",
    "points = load_points_3d(data_dir=data_dir, idx=0)\n",
    "print(f\"{points.shape=}\")\n",
    "print(points[:5, :])\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "from lib.data.utils import load_points_3d\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "points = load_points_3d(data_dir=data_dir, idx=0)\n",
    "print(f\"{points.shape=}\")\n",
    "print(points[:5, :])\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = \"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove/camera/c00_color_extrinsic.txt\"\n",
    "E = np.zeros((4, 4))\n",
    "E[3, 3] = 1.0\n",
    "E[:3, :] = np.loadtxt(path).reshape(3, 4)  # extrinsic hence world to camera\n",
    "\n",
    "# note that the pose is the camera to world, e.g. if flame calls them pose they mean\n",
    "# that they project from camera to world coordinates, hence the final mesh vertices lives\n",
    "# in the world coordinate system! This is so important!\n",
    "# note that this is 4x4\n",
    "# we need to project the point from camera to world! because the point cloud is in camera\n",
    "# we can see that because the coordinate system is right-hand where z-axes goes inside and\n",
    "# y-axes goes down, usually z goes to the camera and y up (see cv2 reference)\n",
    "pose = np.linalg.inv(E)  # camera to world, hence this is the \"pose\" they call it that.\n",
    "\n",
    "points_c_homo = np.zeros((points.shape[0], 4))\n",
    "points_c_homo[:, 3] = 1.0\n",
    "points_c_homo[:, :3] = points\n",
    "\n",
    "\n",
    "points_w_homo = (E @ points_c_homo.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_w_homo = (pose[:3, :3] @ points.T).T\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points_w_homo)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points_w_homo[:, :3])\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
