{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display The Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from lib.data.utils import (\n",
    "    load_color,\n",
    "    load_depth,\n",
    "    load_mask,\n",
    "    load_color_masked,\n",
    "    load_depth_masked,\n",
    ")\n",
    "\n",
    "image_idx = 0\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "\n",
    "color = load_color(data_dir, image_idx, return_tensor=\"img\")\n",
    "display(color)\n",
    "\n",
    "depth = load_depth(data_dir, image_idx, return_tensor=\"np\")\n",
    "plt.imshow(depth)\n",
    "plt.show()\n",
    "\n",
    "mask = load_mask(data_dir, image_idx, return_tensor=\"np\")\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "\n",
    "color_masked = load_color_masked(data_dir, image_idx, return_tensor=\"img\")\n",
    "display(color_masked)\n",
    "\n",
    "depth_masked = load_depth_masked(data_dir, image_idx, return_tensor=\"np\")\n",
    "plt.imshow(depth_masked)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipnet Landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.utils import (\n",
    "    load_pipnet_image,\n",
    "    load_pipnet_landmark_2d,\n",
    "    load_pipnet_landmark_3d,\n",
    ")\n",
    "\n",
    "pipnet_landmarks_2d = load_pipnet_landmark_2d(data_dir, idx=image_idx)\n",
    "\n",
    "print(f\"{pipnet_landmarks_2d.shape=}\")\n",
    "print(pipnet_landmarks_2d[:5, :])\n",
    "\n",
    "pipnet_landmarks_3d = load_pipnet_landmark_3d(data_dir, idx=image_idx)\n",
    "print(f\"{pipnet_landmarks_3d.shape=}\")\n",
    "print(pipnet_landmarks_3d[:5, :])\n",
    "print(\"depth\", pipnet_landmarks_3d[:, 2])\n",
    "plt.hist(pipnet_landmarks_3d[:, 2])\n",
    "plt.show()\n",
    "\n",
    "pipnet_image = load_pipnet_image(data_dir, idx=image_idx)\n",
    "display(pipnet_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medipipe Landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.utils import (\n",
    "    load_mediapipe_image,\n",
    "    load_mediapipe_landmark_2d,\n",
    "    load_mediapipe_landmark_3d,\n",
    ")\n",
    "\n",
    "mediapipe_landmarks_2d = load_mediapipe_landmark_2d(data_dir, idx=image_idx)\n",
    "\n",
    "print(f\"{mediapipe_landmarks_2d.shape=}\")\n",
    "print(mediapipe_landmarks_2d[:5, :])\n",
    "\n",
    "mediapipe_landmarks_3d = load_mediapipe_landmark_3d(data_dir, idx=image_idx)\n",
    "print(f\"{mediapipe_landmarks_3d.shape=}\")\n",
    "print(mediapipe_landmarks_3d[:5, :])\n",
    "print(\"depth\", mediapipe_landmarks_3d[:50, 2])\n",
    "plt.hist(mediapipe_landmarks_3d[:, 2])\n",
    "plt.show()\n",
    "\n",
    "mediapip_image = load_mediapipe_image(data_dir, idx=image_idx)\n",
    "display(mediapip_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth\n",
    "\n",
    "From https://cvg.cit.tum.de/data/datasets/rgbd-dataset/file_formats\n",
    "\n",
    "The color and depth images are already pre-registered using the OpenNI driver from PrimeSense, i.e., the pixels in the color and depth images correspond already 1:1.\n",
    "\n",
    "The depth images are scaled by a factor of 1000, i.e., a pixel value of 1000 in the depth image corresponds to a distance of 1 meter from the camera. A pixel value of 0 means missing value/no data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.utils import (\n",
    "    load_pipnet_landmark_2d,\n",
    "    load_pipnet_landmark_3d,\n",
    "    load_depth_masked,\n",
    ")\n",
    "\n",
    "depth = load_depth_masked(data_dir, image_idx, return_tensor=\"np\", depth_factor=1000)\n",
    "plt.imshow(depth)\n",
    "\n",
    "# draw all of the lm on the screen\n",
    "lm = load_pipnet_landmark_2d(data_dir, idx=image_idx)\n",
    "for point in lm:\n",
    "    x, y = point.astype(int)\n",
    "    plt.scatter(x, y, c=\"red\", s=10)  # Drawing a red point for each landmark\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_idx = 0\n",
    "\n",
    "lm3d = load_pipnet_landmark_3d(data_dir, idx=image_idx)[lm_idx]\n",
    "print(lm3d)\n",
    "\n",
    "lm2d = load_pipnet_landmark_2d(data_dir, idx=image_idx)[lm_idx]\n",
    "x, y = lm2d.astype(int)\n",
    "print(lm2d)\n",
    "\n",
    "depth = load_depth_masked(data_dir, image_idx, return_tensor=\"np\", depth_factor=1000)\n",
    "print(depth[y, x])\n",
    "plt.imshow(depth)\n",
    "\n",
    "x, y = lm2d.astype(int)\n",
    "plt.scatter(x, y, c=\"red\", s=10)  # Drawing a red point for each landmark\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a point in 3D which is:\n",
    "\n",
    "[-0.051, -0.042, 0.575] (x, y, z)\n",
    "\n",
    "The coresponding pixel value is:\n",
    "\n",
    "[878, 480] (x, y)\n",
    "\n",
    "How do we get from 3D to 2D screen coordinates?\n",
    "\n",
    "Input:\n",
    "fx = 914.415\n",
    "fy = 914.03\n",
    "cx = 959.598\n",
    "cy = 547.202\n",
    "xyz_camera = [-0.051, -0.042, 0.575] (x, y, z_c)\n",
    "\n",
    "Output:\n",
    "uvz_pixel = [878.0, 480.0, 0.575] (u, v, z_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.utils import load_pipnet_landmark_3d\n",
    "from lib.camera import load_intrinsics, camera2pixel\n",
    "\n",
    "lm3d = load_pipnet_landmark_3d(data_dir, idx=image_idx)\n",
    "lm2d = load_pipnet_landmark_3d(data_dir, idx=image_idx)\n",
    "K = load_intrinsics(data_dir=data_dir)\n",
    "camera2pixel(lm3d[:2], **K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normals and Points in 3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.utils import load_normals_3d, load_points_3d\n",
    "import open3d as o3d\n",
    "\n",
    "normals = load_normals_3d(data_dir=data_dir, idx=0)\n",
    "print(f\"{normals.shape=}\")\n",
    "print(normals[:5, :])\n",
    "\n",
    "points = load_points_3d(data_dir=data_dir, idx=0)\n",
    "print(f\"{points.shape=}\")\n",
    "print(points[:5, :])\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "from lib.data.utils import load_points_3d\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "points = load_points_3d(data_dir=data_dir, idx=0)\n",
    "print(f\"{points.shape=}\")\n",
    "print(points[:5, :])\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path = \"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove/camera/c00_color_extrinsic.txt\"\n",
    "E = np.zeros((4, 4))\n",
    "E[3, 3] = 1.0\n",
    "E[:3, :] = np.loadtxt(path).reshape(3, 4)  # extrinsic hence world to camera\n",
    "\n",
    "# note that the pose is the camera to world, e.g. if flame calls them pose they mean\n",
    "# that they project from camera to world coordinates, hence the final mesh vertices lives\n",
    "# in the world coordinate system! This is so important!\n",
    "# note that this is 4x4\n",
    "# we need to project the point from camera to world! because the point cloud is in camera\n",
    "# we can see that because the coordinate system is right-hand where z-axes goes inside and \n",
    "# y-axes goes down, usually z goes to the camera and y up (see cv2 reference)\n",
    "pose = np.linalg.inv(E)  # camera to world, hence this is the \"pose\" they call it that.\n",
    "\n",
    "points_c_homo = np.zeros((points.shape[0], 4))\n",
    "points_c_homo[:, 3] = 1.0\n",
    "points_c_homo[:, :3] = points\n",
    "\n",
    "\n",
    "points_w_homo = (E @ points_c_homo.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_w_homo = (pose[:3, :3] @ points.T).T\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points_w_homo)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points_w_homo[:, :3])\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
