{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display The Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from lib.utils.loader import (\n",
    "    load_normal,\n",
    "    load_color,\n",
    "    load_depth,\n",
    "    load_mask,\n",
    ")\n",
    "\n",
    "image_idx = 0\n",
    "data_dir = Path(\"/home/borth/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "\n",
    "color = load_color(data_dir, image_idx, return_tensor=\"img\")\n",
    "display(color)\n",
    "\n",
    "depth = load_depth(data_dir, image_idx, return_tensor=\"np\", smooth=False)\n",
    "plt.imshow(depth[400:600, 920:1100])\n",
    "plt.show()\n",
    "\n",
    "mask = load_mask(data_dir, image_idx, return_tensor=\"np\")\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "\n",
    "normal = load_normal(data_dir, image_idx, return_tensor=\"np\", smooth=True)\n",
    "normal[:, :, 1] = -normal[:, :, 1]\n",
    "normal[:, :, 2] = -normal[:, :, 2]\n",
    "plt.imshow(((((normal + 1) / 2) * 255).astype(np.uint8)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.renderer.camera import depth2camera\n",
    "from lib.utils.loader import load_intrinsics, load_depth\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from lib.utils.visualize import load_pcd\n",
    "import open3d as o3d\n",
    "\n",
    "image_idx = 0\n",
    "data_dir = \"/home/borth/GuidedResearch/data/dphm_christoph_mouthmove\"\n",
    "\n",
    "scale = 0.25\n",
    "K = load_intrinsics(\n",
    "    data_dir=data_dir,\n",
    "    return_tensor=\"pt\",\n",
    "    scale=scale,\n",
    ")\n",
    "depth = load_depth(data_dir, image_idx, return_tensor=\"pt\", smooth=True)\n",
    "point, mask = depth2camera(depth, K, scale)\n",
    "pcd = load_pcd(point.reshape(-1, 3).detach().cpu().numpy())\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "height = 1080\n",
    "width = 1920\n",
    "\n",
    "near = 1.0\n",
    "far = 100\n",
    "fx = 914.4150\n",
    "fy = 914.0300\n",
    "cx = 959.5980\n",
    "cy = 547.2020\n",
    "A = near + far\n",
    "B = near * far\n",
    "\n",
    "# l = 0\n",
    "# r = width\n",
    "# b = height\n",
    "# t = 0\n",
    "l = -width/2\n",
    "r = width/2\n",
    "b = -height/2\n",
    "t = height\n",
    "\n",
    "tx = -(r + l) / (r - l)\n",
    "ty = -(t + b) / (t - b)\n",
    "tz = -(far + near) / (far - near)\n",
    "\n",
    "Persp = torch.tensor(\n",
    "    [\n",
    "        [fx, 0.0, -cx, 0.0],\n",
    "        [0.0, fy, -cy, 0.0],\n",
    "        [0.0, 0.0, A, B],\n",
    "        [0.0, 0.0, -1.0, 0.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "NDC = torch.tensor(\n",
    "    [\n",
    "        [2 / (r - l), 0.0, 0.0, tx],\n",
    "        [0.0, 2 / (t - b), 0.0, ty],\n",
    "        [0.0, 0.0, -2 / (far - near), tz],\n",
    "        [0.0, 0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "Proj = NDC @ Persp\n",
    "print(Proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipnet Landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.loader import (\n",
    "    load_pipnet_image,\n",
    "    load_pipnet_landmark_2d,\n",
    "    load_pipnet_landmark_3d,\n",
    ")\n",
    "\n",
    "pipnet_landmarks_2d = load_pipnet_landmark_2d(data_dir, idx=image_idx)\n",
    "\n",
    "print(f\"{pipnet_landmarks_2d.shape=}\")\n",
    "print(pipnet_landmarks_2d[:5, :])\n",
    "\n",
    "pipnet_landmarks_3d = load_pipnet_landmark_3d(data_dir, idx=image_idx)\n",
    "print(f\"{pipnet_landmarks_3d.shape=}\")\n",
    "print(pipnet_landmarks_3d[:5, :])\n",
    "print(\"depth\", pipnet_landmarks_3d[:, 2])\n",
    "plt.hist(pipnet_landmarks_3d[:, 2])\n",
    "plt.show()\n",
    "\n",
    "pipnet_image = load_pipnet_image(data_dir, idx=image_idx)\n",
    "display(pipnet_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medipipe Landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.loader import (\n",
    "    load_mediapipe_image,\n",
    "    load_mediapipe_landmark_2d,\n",
    "    load_mediapipe_landmark_3d,\n",
    ")\n",
    "\n",
    "mediapipe_landmarks_2d = load_mediapipe_landmark_2d(data_dir, idx=image_idx)\n",
    "\n",
    "print(f\"{mediapipe_landmarks_2d.shape=}\")\n",
    "print(mediapipe_landmarks_2d[:5, :])\n",
    "\n",
    "mediapipe_landmarks_3d = load_mediapipe_landmark_3d(data_dir, idx=image_idx)\n",
    "print(f\"{mediapipe_landmarks_3d.shape=}\")\n",
    "print(mediapipe_landmarks_3d[:5, :])\n",
    "print(\"depth\", mediapipe_landmarks_3d[:50, 2])\n",
    "plt.hist(mediapipe_landmarks_3d[:, 2])\n",
    "plt.show()\n",
    "\n",
    "mediapip_image = load_mediapipe_image(data_dir, idx=image_idx)\n",
    "display(mediapip_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model.utils import load_static_landmark_embedding\n",
    "\n",
    "flame_dir = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023\"\n",
    "flame_landmarks = load_static_landmark_embedding(flame_dir)\n",
    "flame_landmarks[\"landmark_indices\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth\n",
    "\n",
    "From https://cvg.cit.tum.de/data/datasets/rgbd-dataset/file_formats\n",
    "\n",
    "The color and depth images are already pre-registered using the OpenNI driver from PrimeSense, i.e., the pixels in the color and depth images correspond already 1:1.\n",
    "\n",
    "The depth images are scaled by a factor of 1000, i.e., a pixel value of 1000 in the depth image corresponds to a distance of 1 meter from the camera. A pixel value of 0 means missing value/no data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model.flame import FLAME\n",
    "from lib.renderer.renderer import Renderer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from lib.utils.mesh import vertex_normals\n",
    "\n",
    "flame_dir = \"/home/borth/GuidedResearch/checkpoints/flame2023\"\n",
    "data_dir = \"/home/borth/GuidedResearch/data/dphm_christoph_mouthmove\"\n",
    "scale = 1.0\n",
    "flame = FLAME(\n",
    "    flame_dir=flame_dir,\n",
    "    data_dir=data_dir,\n",
    "    vertices_mask=\"full\",\n",
    ").to(\"cuda\")\n",
    "flame.init_params(\n",
    "    global_pose=[0.0, 0, 0],\n",
    "    transl=[0.0, 0.0, -0.5],\n",
    ")\n",
    "vertices, landmarks = flame()\n",
    "renderer = flame.renderer()\n",
    "\n",
    "# mesh = trimesh.Trimesh(vertices[0].detach().cpu().numpy(), faces=flame.faces.detach().cpu().numpy())\n",
    "# vn = torch.tensor(mesh.vertex_normals).unsqueeze(0).to(vertices.device)\n",
    "faces = flame.faces[:, [0, 2, 1]]\n",
    "vn = vertex_normals(vertices=vertices, faces=faces)\n",
    "normal, mask = renderer.render(vertices, faces, vn)\n",
    "# normal, mask = renderer.render(vertices,flame.masked_faces(vertices), vn)\n",
    "normal_image = renderer.normal_to_normal_image(normal, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.visualize import load_pcd\n",
    "import open3d as o3d\n",
    "\n",
    "pcd = load_pcd(vertices.detach().cpu().numpy()[0])\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.loader import (\n",
    "    load_mediapipe_landmark_2d,\n",
    "    load_mediapipe_landmark_3d,\n",
    "    load_normal,\n",
    ")\n",
    "from lib.renderer.camera import camera2pixel\n",
    "\n",
    "face_idx = 0\n",
    "# face_idx = 110\n",
    "# normal = load_normal(data_dir, image_idx, return_tensor=\"np\")\n",
    "# normal = (((normal + 1) / 2) * 255).astype(np.uint8)\n",
    "K = load_intrinsics(data_dir=data_dir, return_tensor=\"pt\", scale=1.0)\n",
    "# face = vertices[:, flame.faces][0][face_idx]\n",
    "# pixel = camera2pixel(face, K[\"fx\"], K[\"fy\"], K[\"cx\"], K[\"cy\"])\n",
    "# pixel = camera2pixel(landmarks[0], K[\"fx\"], K[\"fy\"], K[\"cx\"], K[\"cy\"])\n",
    "\n",
    "points = landmarks[0] \n",
    "# xc = points[:, 0]\n",
    "# yc = points[:, 1]\n",
    "# zc = points[:, 2]\n",
    "\n",
    "# us = cx + fx * (xc / zc)\n",
    "# vs = cy + fy * (yc / zc)\n",
    "\n",
    "pixels = (K.to(\"cuda\") @ points.T).T\n",
    "\n",
    "plt.imshow(color)\n",
    "\n",
    "# draw all of the lm on the screen\n",
    "# x, y, _ = pixel[0]\n",
    "\n",
    "for p in pixels:\n",
    "    plt.scatter(int(p[0]), int(p[1]), c=\"red\", s=2)  # Drawing a red point for each landmark\n",
    "# plt.scatter(int(pixel[0,0]), int(pixel[0,1]), c=\"red\", s=2)  # Drawing a red point for each landmark\n",
    "# plt.scatter(int(pixel[1,0]), int(pixel[1,1]), c=\"blue\", s=2)  # Drawing a red point for each landmark\n",
    "# plt.scatter(int(pixel[2,0]), int(pixel[2,1]), c=\"green\", s=2)  # Drawing a red point for each landmark\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = landmarks[0]\n",
    "landmarks[:, 2] = -landmarks[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal[0][int(pixel[0,1]), int(pixel[0,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.model.utils import load_static_landmark_embedding\n",
    "\n",
    "flame_dir = \"/Users/robinborth/Code/GuidedResearch/checkpoints/flame2023\"\n",
    "flame_landmarks = load_static_landmark_embedding(flame_dir)\n",
    "print(flame_landmarks[\"landmark_indices\"][1])\n",
    "print(flame_landmarks[\"lmk_face_idx\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_idx = 0\n",
    "\n",
    "lm3d = load_pipnet_landmark_3d(data_dir, idx=image_idx)[lm_idx]\n",
    "print(lm3d)\n",
    "\n",
    "lm2d = load_pipnet_landmark_2d(data_dir, idx=image_idx)[lm_idx]\n",
    "x, y = lm2d.astype(int)\n",
    "print(lm2d)\n",
    "\n",
    "depth = load_depth_masked(data_dir, image_idx, return_tensor=\"np\", depth_factor=1000)\n",
    "print(depth[y, x])\n",
    "plt.imshow(depth)\n",
    "\n",
    "x, y = lm2d.astype(int)\n",
    "plt.scatter(x, y, c=\"red\", s=10)  # Drawing a red point for each landmark\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.renderer.camera import load_intrinsics, pixel2camera\n",
    "from lib.utils.loader import load_depth_masked\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "scale = 0.5\n",
    "\n",
    "# load the intrinsic\n",
    "_K = load_intrinsics(data_dir=data_dir, return_tensor=\"dict\")\n",
    "K = torch.tensor(\n",
    "    [\n",
    "        [_K[\"fx\"] * scale, 0.0, _K[\"cx\"] * scale],\n",
    "        [0.0, _K[\"fy\"] * scale, _K[\"cy\"] * scale],\n",
    "        [0.0, 0.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load the depth image\n",
    "_depth_masked = load_depth_masked(data_dir, 0, return_tensor=\"pt\")\n",
    "_H, _W = _depth_masked.shape\n",
    "H, W = int(_H * scale), int(_W * scale)\n",
    "\n",
    "# get the mask\n",
    "_mask = _depth_masked == 0.0\n",
    "mask = v2.functional.resize(_mask.unsqueeze(0), size=(H, W)).squeeze(0)\n",
    "mask = ~mask\n",
    "\n",
    "# get the new size of the depth image\n",
    "depth_masked = v2.functional.resize(_depth_masked.unsqueeze(0), size=(H, W)).squeeze(0)\n",
    "\n",
    "# span the pixel indexes\n",
    "x = torch.arange(W)\n",
    "y = torch.arange(H)\n",
    "idx = torch.stack(torch.meshgrid(y, x), dim=-1).flip(-1)\n",
    "\n",
    "# get the points in camera coordinates, but with the new resolution\n",
    "points = torch.concat([idx, depth_masked.unsqueeze(-1)], dim=-1)\n",
    "points[:, :, 0] *= points[:, :, 2]\n",
    "points[:, :, 1] *= points[:, :, 2]\n",
    "out = K.inverse() @ points.permute(2, 0, 1).reshape(3, -1)\n",
    "out = out.reshape(3, points.shape[0], points.shape[1]).permute(1, 2, 0)\n",
    "\n",
    "# just save the point of the face\n",
    "cpoints = out[mask]\n",
    "np.save(\"temp/out\", cpoints.detach().cpu().numpy())\n",
    "# [depth_masked] * 3\n",
    "# depth_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.renderer.camera import depth2camera\n",
    "from lib.utils.loader import load_depth_masked, load_intrinsics\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "depth = load_depth_masked(data_dir=data_dir, idx=0, return_tensor=\"pt\")\n",
    "scale = 0.1\n",
    "K = load_intrinsics(data_dir=data_dir, return_tensor=\"pt\", scale=scale)\n",
    "points = depth2camera(depth, K, scale=scale)\n",
    "plt.imshow(points[:, :, 2])\n",
    "points[:, :, 2].min(), points[:, :, 2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.renderer.camera import camera2normal\n",
    "import torch\n",
    "\n",
    "normals = camera2normal(points.unsqueeze(0))\n",
    "\n",
    "# now show the image\n",
    "b_mask = normals.sum(-1) == 0\n",
    "normals_image = (((normals + 1) / 2) * 255).to(torch.uint8)\n",
    "normals_image[b_mask, :] = 0\n",
    "plt.imshow(normals_image[0])\n",
    "# points.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# https://stackoverflow.com/questions/34644101/calculate-surface-normals-from-depth-image-using-neighboring-pixels-cross-produc/34644939#34644939\n",
    "# they have (-dz/dx,-dz/dy,1, however we are in camera space hence we need to calculate the gradient in pixel space, e.g. also the delta x and delta y are in camera space.\n",
    "\n",
    "# make sure that on the boundary is nothing wrong calculated\n",
    "points[points.sum(-1) == 0] = torch.nan\n",
    "\n",
    "H, W, C = points.shape\n",
    "normals = torch.ones_like(points)\n",
    "normals *= -1\n",
    "\n",
    "# we calculate the normal in camera space, hence we also need to normalize with the depth information,\n",
    "# note that the normal is basically on which direction we have the stepest decent.\n",
    "x_right = torch.arange(2, W)\n",
    "x_left = torch.arange(0, W - 2)\n",
    "normals[:, 1:-1, 0] = (points[:, x_right, 2] - points[:, x_left, 2]) / (\n",
    "    points[:, x_right, 0] - points[:, x_left, 0]\n",
    ")\n",
    "\n",
    "y_right = torch.arange(2, H)\n",
    "y_left = torch.arange(0, H - 2)\n",
    "normals[1:-1, :, 1] = (points[y_right, :, 2] - points[y_left, :, 2]) / (\n",
    "    points[y_right, :, 1] - points[y_left, :, 1]\n",
    ")\n",
    "\n",
    "# normalized between [-1, 1]\n",
    "normals = normals / torch.norm(normals, dim=-1).unsqueeze(-1)\n",
    "normals = torch.nan_to_num(normals, 0)\n",
    "normals[:1, :, :] = 0\n",
    "normals[-1:, :, :] = 0\n",
    "normals[:, :1, :] = 0\n",
    "normals[:, -1:, :] = 0\n",
    "b_mask = normals.sum(-1) == 0\n",
    "\n",
    "\n",
    "# now show the image\n",
    "normals_image = (((normals + 1) / 2) * 255).to(torch.uint8)\n",
    "normals_image[b_mask, :] = 0\n",
    "plt.imshow(normals_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(points[:, x_right, 2] - points[:, x_left, 2]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "camera = np.load(\"temp/out.npy\").reshape(-1, 3)\n",
    "points = camera[camera[:, 2] != 0]\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a point in 3D which is:\n",
    "\n",
    "[-0.051, -0.042, 0.575] (x, y, z)\n",
    "\n",
    "The coresponding pixel value is:\n",
    "\n",
    "[878, 480] (x, y)\n",
    "\n",
    "How do we get from 3D to 2D screen coordinates?\n",
    "\n",
    "Input:\n",
    "fx = 914.415\n",
    "fy = 914.03\n",
    "cx = 959.598\n",
    "cy = 547.202\n",
    "xyz_camera = [-0.051, -0.042, 0.575] (x, y, z_c)\n",
    "\n",
    "Output:\n",
    "uvz_pixel = [878.0, 480.0, 0.575] (u, v, z_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.loader import load_pipnet_landmark_3d\n",
    "from lib.renderer.camera import load_intrinsics, camera2pixel\n",
    "\n",
    "flame_landmarks = load_static_landmark_embedding(flame_dir)\n",
    "lm_idx = flame_landmarks[\"landmark_indices\"]\n",
    "\n",
    "plt.imshow(color)\n",
    "\n",
    "lm3d = load_mediapipe_landmark_3d(data_dir, idx=image_idx)\n",
    "K = load_intrinsics(data_dir=data_dir)\n",
    "lm = camera2pixel(lm3d, **K)\n",
    "for point in lm[lm_idx]:\n",
    "    x, y, z = point.astype(int)\n",
    "    plt.scatter(x, y, c=\"red\", s=1)  # Drawing a red point for each landmark\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normals and Points in 3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.utils.loader import load_normals_3d, load_points_3d\n",
    "import open3d as o3d\n",
    "\n",
    "normals = load_normals_3d(data_dir=data_dir, idx=0)\n",
    "print(f\"{normals.shape=}\")\n",
    "print(normals[:5, :])\n",
    "\n",
    "points = load_points_3d(data_dir=data_dir, idx=0)\n",
    "print(f\"{points.shape=}\")\n",
    "print(points[:5, :])\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "from lib.utils.loader import load_points_3d\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove\")\n",
    "points = load_points_3d(data_dir=data_dir, idx=0)\n",
    "print(f\"{points.shape=}\")\n",
    "print(points[:5, :])\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = \"/Users/robinborth/Code/GuidedResearch/data/dphm_christoph_mouthmove/camera/c00_color_extrinsic.txt\"\n",
    "E = np.zeros((4, 4))\n",
    "E[3, 3] = 1.0\n",
    "E[:3, :] = np.loadtxt(path).reshape(3, 4)  # extrinsic hence world to camera\n",
    "\n",
    "# note that the pose is the camera to world, e.g. if flame calls them pose they mean\n",
    "# that they project from camera to world coordinates, hence the final mesh vertices lives\n",
    "# in the world coordinate system! This is so important!\n",
    "# note that this is 4x4\n",
    "# we need to project the point from camera to world! because the point cloud is in camera\n",
    "# we can see that because the coordinate system is right-hand where z-axes goes inside and\n",
    "# y-axes goes down, usually z goes to the camera and y up (see cv2 reference)\n",
    "pose = np.linalg.inv(E)  # camera to world, hence this is the \"pose\" they call it that.\n",
    "\n",
    "points_c_homo = np.zeros((points.shape[0], 4))\n",
    "points_c_homo[:, 3] = 1.0\n",
    "points_c_homo[:, :3] = points\n",
    "\n",
    "\n",
    "points_w_homo = (E @ points_c_homo.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_w_homo = (pose[:3, :3] @ points.T).T\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points_w_homo)\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points_w_homo[:, :3])\n",
    "o3d.visualization.draw_plotly([pcd])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
