{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "class Camera:\n",
    "    \"\"\"\n",
    "    For the camera there are four different coordinate systems (or spaces):\n",
    "    - Camera space: We follow OpenGL coordinate system with the\n",
    "        right-hand rule where we have +X points right, +Y points up and +Z\n",
    "        point to the camera.\n",
    "    - Cilp space: Any coordinate outside within the range is being clipped.\n",
    "        We use this coordinate system as an input for the rasterizer. Note\n",
    "        That we use homogeneous coordinates, hence the w value (x,y,z,w) is\n",
    "        used for clipping -w < x,y,z < w. Note that we convert here from\n",
    "        right-hand rule to left-hand rule, because +Z now points awy from\n",
    "        the camera.\n",
    "    - NDC space: This is the normalzied Clip space where each coordiante\n",
    "        in the frustrum is between (-1,1). We don't performe the\n",
    "        transformation ourselv we let OpenGL do that.\n",
    "    - Screen space: The representation of the frustrum defined in pixel space\n",
    "        instead of a normalzied space, where the top left pixel is (0,0) and\n",
    "        the bottom right corner is (W,H).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        width: int,\n",
    "        height: int,\n",
    "        fov_y: float = 45.0,\n",
    "        near: float = 0.01,\n",
    "        far: float = 100.0,\n",
    "        K: torch.Tensor | None = None,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.near = near\n",
    "        self.far = far\n",
    "        self.fov_y = fov_y\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        if K is None:\n",
    "            self.projection_matrix = self.fov_perspective_projection(\n",
    "                fov_y=self.fov_y,\n",
    "                width=self.width,\n",
    "                height=self.height,\n",
    "                near=self.near,\n",
    "                far=self.far,\n",
    "            )\n",
    "        else:\n",
    "            self.projection_matrix = self.intrinsics_perspective_projection(\n",
    "                K=K,\n",
    "                width=self.width,\n",
    "                height=self.height,\n",
    "                near=self.near,\n",
    "                far=self.far,\n",
    "            )\n",
    "\n",
    "\n",
    "    def fov_perspective_projection(\n",
    "        self,\n",
    "        fov_y: float,\n",
    "        width: int,\n",
    "        height: int,\n",
    "        near: float = 1.0,\n",
    "        far: float = 100.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            P: Perspective projection matrix, (4, 4)\n",
    "                P = [\n",
    "                        [2*n/(r-l), 0.0,        (r+l)/(r-l),    0.0         ],\n",
    "                        [0.0,       2*n/(t-b),  (t+b)/(t-b),    0.0         ],\n",
    "                        [0.0,       0.0,        -(f+n)/(f-n),   -(f*n)/(f-n)],\n",
    "                        [0.0,       0.0,        -1.0,            0.0         ]\n",
    "                    ]\n",
    "        \"\"\"\n",
    "        deg2rad = math.pi / 180\n",
    "        tan_fov_y = math.tan(fov_y * 0.5 * deg2rad)\n",
    "        tan_fov_x = tan_fov_y * (width / height)\n",
    "        top = tan_fov_y * near\n",
    "        bottom = -top\n",
    "        right = tan_fov_x * near\n",
    "        left = -right\n",
    "        z_sign = -1.0\n",
    "\n",
    "        proj = torch.zeros([4, 4], device=self.device)\n",
    "\n",
    "        proj[0, 0] = 2.0 * near / (right - left)\n",
    "        proj[1, 1] = 2.0 * near / (top - bottom)\n",
    "        proj[0, 2] = (right + left) / (right - left)\n",
    "        proj[1, 2] = (top + bottom) / (top - bottom)\n",
    "        proj[3, 2] = z_sign\n",
    "        proj[2, 2] = z_sign * (far + near) / (far - near)\n",
    "        proj[2, 3] = -(2.0 * far * near) / (far - near)\n",
    "        return proj\n",
    "\n",
    "    def intrinsics_perspective_projection(\n",
    "        self,\n",
    "        K: torch.Tensor,\n",
    "        width: int,\n",
    "        height: int,\n",
    "        near: float = 1.0,\n",
    "        far: float = 100.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Transform points from camera space (x: right, y: up, z: out) to clip space (x: right, y: up, z: in)\n",
    "\n",
    "        For information check out the math:\n",
    "        https://www.songho.ca/opengl/gl_projectionmatrix.html\n",
    "\n",
    "        Args:\n",
    "            K: Intrinsic matrix, (3, 3)\n",
    "                K = [\n",
    "                        [fx, 0, cx],\n",
    "                        [0, fy, cy],\n",
    "                        [0,  0,  1],\n",
    "                    ]\n",
    "        Returns:\n",
    "            P: Perspective projection matrix, (4, 4)\n",
    "                P = [\n",
    "                        [2*fx/w, 0.0,     (w - 2*cx)/w,             0.0                     ],\n",
    "                        [0.0,    2*fy/h,  (h - 2*cy)/h,             0.0                     ],\n",
    "                        [0.0,    0.0,     -(far+near) / (far-near), -2*far*near / (far-near)],\n",
    "                        [0.0,    0.0,     -1.0,                     0.0                     ]\n",
    "                    ]\n",
    "        \"\"\"\n",
    "        w = width\n",
    "        h = height\n",
    "        fx = K[0, 0]\n",
    "        fy = K[1, 1]\n",
    "        cx = K[0, 2]\n",
    "        cy = K[1, 2]\n",
    "\n",
    "        proj = torch.zeros([4, 4], device=self.device)\n",
    "        proj[0, 0] = fx * 2 / w\n",
    "        proj[1, 1] = fy * 2 / h\n",
    "        proj[0, 2] = (w - 2 * cx) / w\n",
    "        proj[1, 2] = (h - 2 * cy) / h\n",
    "        proj[2, 2] = -(far + near) / (far - near)\n",
    "        proj[2, 3] = -2 * far * near / (far - near)\n",
    "        proj[3, 2] = -1\n",
    "        return proj\n",
    "\n",
    "    def convert_to_homo_coords(self, p: torch.Tensor):\n",
    "        shape = list(p.shape)\n",
    "        assert shape[-1] == 3\n",
    "        shape[-1] = shape[-1] + 1\n",
    "        p_homo = torch.ones(shape, device=p.device)\n",
    "        p_homo[..., :3] = p\n",
    "        return p_homo \n",
    "\n",
    "    def clip_transform(self, p_camera: torch.Tensor):\n",
    "        return torch.matmul(p_camera, self.projection_matrix.T)\n",
    "\n",
    "    def ndc_transform(self, p_camera: torch.Tensor):\n",
    "        p_clip = self.clip_transform(p_camera)\n",
    "        p_clip[..., 0] /= p_clip[..., 3] \n",
    "        p_clip[..., 1] /= p_clip[..., 3] \n",
    "        p_clip[..., 2] /= p_clip[..., 3] \n",
    "        p_clip[..., 3] /= p_clip[..., 3] \n",
    "        return p_clip\n",
    "    \n",
    "    def screen_transform(self, p_camera: torch.Tensor):\n",
    "        p_ndc = self.ndc_transform(p_camera)\n",
    "        depth = p_camera[..., 2]\n",
    "        u = (p_ndc[..., 0] + 1) * 0.5 * self.width\n",
    "        v = (p_ndc[..., 1] + 1) * 0.5 * self.height\n",
    "        return torch.stack([u, v, depth], dim=-1)\n",
    "    \n",
    "    def unproject_points(self, xy_depth: torch.Tensor):\n",
    "        \"\"\"\n",
    "        The x,y data contains the values in ndc coordinates space, hence they\n",
    "        are between (-1, 1) and the depth is the value from the z-plane in\n",
    "        camera space, hence the sign should be negative.\n",
    "\n",
    "        xy_depth[i] = [x[i], y[i], depth[i]]\n",
    "        \"\"\"\n",
    "        z_camera = xy_depth[..., 2] \n",
    "        p1 = self.projection_matrix[2, 2]\n",
    "        p2 = self.projection_matrix[2, 3]\n",
    "        p3 = self.projection_matrix[3, 2] \n",
    "        w_clip = p3 * z_camera\n",
    "        z_ndc = (p1 * z_camera + p2) / w_clip\n",
    "        p_ndc = torch.stack([xy_depth[..., 0], xy_depth[..., 1], z_ndc])\n",
    "        p_ndc = self.convert_to_homo_coords(p_ndc)\n",
    "        # convert back to clip space\n",
    "        p_clip = p_ndc \n",
    "        p_clip[..., 0] *= w_clip  \n",
    "        p_clip[..., 1] *= w_clip  \n",
    "        p_clip[..., 2] *= w_clip  \n",
    "        p_clip[..., 3] *= w_clip  \n",
    "        # extract only x,y,z component\n",
    "        p_camera = torch.matmul(p_clip, self.projection_matrix.inverse().T)\n",
    "        return p_camera[..., :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from lib.utils.loader import load_depth, load_intrinsics\n",
    "\n",
    "image_idx = 0\n",
    "data_dir = \"/home/borth/GuidedResearch/data/dphm_christoph_mouthmove\"\n",
    "K = load_intrinsics(data_dir, return_tensor=\"pt\")\n",
    "depth = load_depth(data_dir, image_idx, return_tensor=\"pt\", smooth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera = Camera(fov_y=45, width=1920, height=1080, near=0.01, far=100)\n",
    "camera = Camera(K=K, width=1920, height=1080, near=0.01, far=100)\n",
    "p_camera = torch.tensor([[0.05, -0.16, -0.5, 1.0], [-0.1, 0.2, -0.3, 1.0]]).to(\"cuda\")\n",
    "camera.ndc_transform(p_camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ndc = torch.linspace(-1, 1, steps=camera.width)\n",
    "y_ndc = torch.linspace(1, -1, steps=camera.height)\n",
    "y_grid, x_grid = torch.meshgrid(y_ndc, x_ndc, indexing=\"ij\")\n",
    "xy_ndc = torch.stack([x_grid, y_grid], dim=-1)\n",
    "xy_depth = torch.concatenate([xy_ndc, -depth.unsqueeze(-1)], dim=-1).to(\"cuda\")\n",
    "p_camera = camera.unproject_points(xy_depth=xy_depth)\n",
    "p_camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "xy_depth = torch.tensor([0.0948, -0.5283, -0.5]).to(\"cuda\")\n",
    "p_ndc_homo = camera.unproject_points(xy_depth=xy_depth)\n",
    "p_ndc_homo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
