{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def preconditioned_conjugate_gradient(\n",
    "    A: torch.Tensor,  # dim (N,N)\n",
    "    b: torch.Tensor,  # dim (N)\n",
    "    x0: torch.Tensor | None = None, # dim (N)\n",
    "    max_iter: int = 20,\n",
    "    verbose: bool = False,\n",
    "    tol: torch.Tensor = 1e-06,\n",
    "    M: torch.Tensor | None = None, # dim (N,N)\n",
    "):\n",
    "    k = 0\n",
    "    converged = False\n",
    "\n",
    "    M = torch.diag(torch.ones_like(b)) if M is None else M # identity\n",
    "    xk = torch.zeros_like(b) if x0 is None else x0  # (N)\n",
    "    rk = b - A @ xk # column vector (N)\n",
    "    zk = M @ rk  # (N)\n",
    "    pk = zk\n",
    "\n",
    "    if torch.norm(rk) < tol:\n",
    "        converged = True\n",
    "\n",
    "    while k < max_iter and not converged: \n",
    "        # compute step size\n",
    "        ak = (rk[None] @ zk) / (pk[None] @ A @ pk)\n",
    "        # update unknowns \n",
    "        xk_1 = xk + ak * pk\n",
    "        # compute residuals\n",
    "        rk_1 = rk - ak * A @ pk\n",
    "        # compute new pk\n",
    "        zk_1 = M @ rk_1\n",
    "        bk = (rk_1[None] @ zk_1) / (rk[None] @ zk)\n",
    "        pk_1 = zk_1 + bk * pk\n",
    "        # update the next state\n",
    "        xk = xk_1 \n",
    "        pk = pk_1\n",
    "        rk = rk_1\n",
    "        zk = zk_1\n",
    "\n",
    "        k += 1\n",
    "        if torch.norm(rk) < tol:\n",
    "            converged = True\n",
    "    \n",
    "    if verbose and converged:\n",
    "        print(f\"Converged in {k=} steps.\")\n",
    "    if verbose and not converged:\n",
    "        print(f\"Not converged in {max_iter=} steps.\")\n",
    "\n",
    "    return xk\n",
    "\n",
    "# M_inv = torch.diag(torch.ones_like(b)) #  (N, N)\n",
    "\n",
    "N = 20\n",
    "_A = torch.rand((N, N))\n",
    "A = torch.sqrt(_A.T @ _A) # positive and symetric\n",
    "M = torch.diag(1 / torch.diag(A)) #  (N, N)\n",
    "b = torch.rand((N))\n",
    "# x_pcg = preconditioned_conjugate_gradient(A, b, num_linear_iterations=30)\n",
    "\n",
    "# A = torch.tensor([[4.0,1],[1, 3]])\n",
    "# b = torch.tensor([1.0,2])\n",
    "# x0 = torch.tensor([2.0, 1.0])\n",
    "x_pcg = preconditioned_conjugate_gradient(A, b, M=M, verbose=True, max_iter=100)\n",
    "x_pcg\n",
    "\n",
    "# num = 100\n",
    "# pcg_norm = 0\n",
    "# optim_norm = 0\n",
    "# b_norm = 0\n",
    "# for _ in range(num):\n",
    "#     N = 700\n",
    "#     _A = torch.rand((N, N), dtype=torch.float64)\n",
    "#     z = torch.zeros_like(_A)\n",
    "#     for idx in range(70):\n",
    "#         i = idx * 10\n",
    "#         j = (idx+1)*10\n",
    "#         z[i:j, i:j] = 1.0\n",
    "#     _A *= z\n",
    "#     A = torch.sqrt(_A.T @ _A) # positive and symetric\n",
    "#     b = torch.rand((N), dtype=torch.float64)\n",
    "#     x_pcg = preconditioned_conjugate_gradient(A, b, num_linear_iterations=1)\n",
    "#     x_optim = torch.linalg.solve(A, b)\n",
    "#     pcg_norm += ((A @ x_pcg) - b).norm()\n",
    "#     optim_norm  += ((A @ x_optim) - b).norm()\n",
    "#     b_norm += b.norm()\n",
    "# pcg_norm, optim_norm, b_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "def cg_batch(A_bmm, B, M_bmm=None, X0=None, rtol=1e-3, atol=0., maxiter=None, verbose=False):\n",
    "    \"\"\"Solves a batch of PD matrix linear systems using the preconditioned CG algorithm.\n",
    "\n",
    "    This function solves a batch of matrix linear systems of the form\n",
    "\n",
    "        A_i X_i = B_i,  i=1,...,K,\n",
    "\n",
    "    where A_i is a n x n positive definite matrix and B_i is a n x m matrix,\n",
    "    and X_i is the n x m matrix representing the solution for the ith system.\n",
    "\n",
    "    Args:\n",
    "        A_bmm: A callable that performs a batch matrix multiply of A and a K x n x m matrix.\n",
    "        B: A K x n x m matrix representing the right hand sides.\n",
    "        M_bmm: (optional) A callable that performs a batch matrix multiply of the preconditioning\n",
    "            matrices M and a K x n x m matrix. (default=identity matrix)\n",
    "        X0: (optional) Initial guess for X, defaults to M_bmm(B). (default=None)\n",
    "        rtol: (optional) Relative tolerance for norm of residual. (default=1e-3)\n",
    "        atol: (optional) Absolute tolerance for norm of residual. (default=0)\n",
    "        maxiter: (optional) Maximum number of iterations to perform. (default=5*n)\n",
    "        verbose: (optional) Whether or not to print status messages. (default=False)\n",
    "    \"\"\"\n",
    "    K, n, m = B.shape\n",
    "\n",
    "    if M_bmm is None:\n",
    "        M_bmm = lambda x: x\n",
    "    if X0 is None:\n",
    "        X0 = M_bmm(B)\n",
    "    if maxiter is None:\n",
    "        maxiter = 5 * n\n",
    "\n",
    "    assert B.shape == (K, n, m)\n",
    "    assert X0.shape == (K, n, m)\n",
    "    assert rtol > 0 or atol > 0\n",
    "    assert isinstance(maxiter, int)\n",
    "\n",
    "    X_k = X0\n",
    "    R_k = B - A_bmm(X_k)\n",
    "    Z_k = M_bmm(R_k)\n",
    "\n",
    "    P_k = torch.zeros_like(Z_k)\n",
    "\n",
    "    P_k1 = P_k\n",
    "    R_k1 = R_k\n",
    "    R_k2 = R_k\n",
    "    X_k1 = X0\n",
    "    Z_k1 = Z_k\n",
    "    Z_k2 = Z_k\n",
    "\n",
    "    B_norm = torch.norm(B, dim=1)\n",
    "    stopping_matrix = torch.max(rtol*B_norm, atol*torch.ones_like(B_norm))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"%03s | %010s %06s\" % (\"it\", \"dist\", \"it/s\"))\n",
    "\n",
    "    optimal = False\n",
    "    start = time.perf_counter()\n",
    "    for k in range(1, maxiter + 1):\n",
    "        start_iter = time.perf_counter()\n",
    "        Z_k = M_bmm(R_k)\n",
    "\n",
    "        if k == 1:\n",
    "            P_k = Z_k\n",
    "            R_k1 = R_k\n",
    "            X_k1 = X_k\n",
    "            Z_k1 = Z_k\n",
    "        else:\n",
    "            R_k2 = R_k1\n",
    "            Z_k2 = Z_k1\n",
    "            P_k1 = P_k\n",
    "            R_k1 = R_k\n",
    "            Z_k1 = Z_k\n",
    "            X_k1 = X_k\n",
    "            denominator = (R_k2 * Z_k2).sum(1)\n",
    "            denominator[denominator == 0] = 1e-8\n",
    "            beta = (R_k1 * Z_k1).sum(1) / denominator\n",
    "            P_k = Z_k1 + beta.unsqueeze(1) * P_k1\n",
    "\n",
    "        denominator = (P_k * A_bmm(P_k)).sum(1)\n",
    "        denominator[denominator == 0] = 1e-8\n",
    "        alpha = (R_k1 * Z_k1).sum(1) / denominator\n",
    "        X_k = X_k1 + alpha.unsqueeze(1) * P_k\n",
    "        R_k = R_k1 - alpha.unsqueeze(1) * A_bmm(P_k)\n",
    "        end_iter = time.perf_counter()\n",
    "\n",
    "        residual_norm = torch.norm(A_bmm(X_k) - B, dim=1)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"%03d | %8.4e %4.2f\" %\n",
    "                  (k, torch.max(residual_norm-stopping_matrix),\n",
    "                    1. / (end_iter - start_iter)))\n",
    "\n",
    "        if (residual_norm <= stopping_matrix).all():\n",
    "            optimal = True\n",
    "            break\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    if verbose:\n",
    "        if optimal:\n",
    "            print(\"Terminated in %d steps (reached maxiter). Took %.3f ms.\" %\n",
    "                  (k, (end - start) * 1000))\n",
    "        else:\n",
    "            print(\"Terminated in %d steps (optimal). Took %.3f ms.\" %\n",
    "                  (k, (end - start) * 1000))\n",
    "\n",
    "\n",
    "    info = {\n",
    "        \"niter\": k,\n",
    "        \"optimal\": optimal\n",
    "    }\n",
    "\n",
    "    return X_k, info\n",
    "\n",
    "\n",
    "A = torch.tensor([[4.0,1],[1, 3]])[None, ...]\n",
    "def A_bmm(X):\n",
    "    Y = [(A[i]@X[i]).unsqueeze(0) for i in range(1)]\n",
    "    return torch.cat(Y, dim=0)\n",
    "b = torch.tensor([1.0,2])[None, ...][..., None]\n",
    "cg_batch(A_bmm=A_bmm, B=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "_A = torch.rand((N, N))\n",
    "z = torch.zeros_like(_A)\n",
    "for idx in range(70):\n",
    "    i = idx * 10\n",
    "    j = (idx+1)*10\n",
    "    z[i:j, i:j] = 1.0\n",
    "_A *= z\n",
    "A = torch.sqrt(_A.T @ _A) # positive and symetric\n",
    "# A = torch.diag(torch.diag(A))\n",
    "b = torch.rand((N)) + 2\n",
    "def A_bmm(X):\n",
    "    Y = [(A[None, ...][i]@X[i]).unsqueeze(0) for i in range(1)]\n",
    "    return torch.cat(Y, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "out = cg_batch(A_bmm=A_bmm, B=b[None, ...][..., None], maxiter=10)\n",
    "print(time.time() - s)\n",
    "x_pcg = out[0].squeeze()\n",
    "s = time.time()\n",
    "x_optim = torch.linalg.solve(A, b)\n",
    "print(time.time() - s)\n",
    "pcg_norm = ((A @ x_pcg) - b).norm()\n",
    "optim_norm = ((A @ x_optim) - b).norm()\n",
    "pcg_norm, optim_norm, b.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
