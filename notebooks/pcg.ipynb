{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.optimizer.pcg import conjugate_gradient\n",
    "\n",
    "N = 400\n",
    "_A = torch.rand((N, N))\n",
    "A = torch.sqrt(_A.T @ _A)  # positive semidefinite and symmetric\n",
    "b = torch.rand((N))\n",
    "# x_pcg  = conjugate_gradient(A, b, verbose=True, max_iter=100)\n",
    "# x_optim= torch.linalg.solve(A, b)\n",
    "# pcg_norm = ((A @ x_pcg) - b).norm()\n",
    "# optim_norm  = ((A @ x_optim) - b).norm()\n",
    "# print(pcg_norm, optim_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "t = 100\n",
    "A = A.to(\"cuda\")\n",
    "b = b.to(\"cuda\")\n",
    "\n",
    "\n",
    "def foo():\n",
    "    conjugate_gradient(A, b, verbose=True, max_iter=5)\n",
    "    # torch.linalg.solve(A, b)\n",
    "\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=False\n",
    ") as prof:\n",
    "    # foo()\n",
    "    timeit.timeit(foo, number=t) / t\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lib.optimizer.pcg import conjugate_gradient\n",
    "import torch\n",
    "import timeit\n",
    "\n",
    "\n",
    "def conjugate_gradient(\n",
    "    A: torch.Tensor,  # dim (N,N)\n",
    "    b: torch.Tensor,  # dim (N)\n",
    "    x0: torch.Tensor | None = None,  # dim (N)\n",
    "    max_iter: int = 20,\n",
    "    verbose: bool = False,\n",
    "    tol: float = 1e-08,\n",
    "):\n",
    "    k = 0\n",
    "    converged = False\n",
    "\n",
    "    xk = torch.zeros_like(b) if x0 is None else x0  # (N)\n",
    "    rk = b - A @ xk  # column vector (N)\n",
    "    pk = rk\n",
    "\n",
    "    if torch.norm(rk) < tol:\n",
    "        converged = True\n",
    "\n",
    "    while k < max_iter and not converged:\n",
    "\n",
    "        # compute step size\n",
    "        ak = (rk[None] @ rk) / (pk[None] @ A @ pk)\n",
    "        # update unknowns\n",
    "        xk_1 = xk + ak * pk\n",
    "        # compute residuals\n",
    "        rk_1 = rk - ak * A @ pk\n",
    "        # compute new pk\n",
    "        bk = (rk_1[None] @ rk_1) / (rk[None] @ rk)\n",
    "        pk_1 = rk_1 + bk * pk\n",
    "        # update the next stateprint\n",
    "        xk = xk_1\n",
    "        pk = pk_1\n",
    "        rk = rk_1\n",
    "\n",
    "        k += 1\n",
    "        if torch.norm(rk) < tol:\n",
    "            converged = True\n",
    "\n",
    "    return xk\n",
    "\n",
    "\n",
    "t = 1000\n",
    "N = 400\n",
    "_A = torch.rand((N, N))\n",
    "A = torch.sqrt(_A.T @ _A).to(\"cuda\")  # positive semidefinite and symmetric\n",
    "b = torch.rand((N)).to(\"cuda\")\n",
    "\n",
    "\n",
    "def foo():\n",
    "    # conjugate_gradient(A, b, max_iter=5)\n",
    "    torch.linalg.solve(A, b)\n",
    "\n",
    "\n",
    "print((timeit.timeit(foo, number=t) / t) * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LU, pivots = torch.linalg.lu_factor(A)\n",
    "X = torch.linalg.lu_solve(LU, pivots, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = 1000\n",
    "N = 1000\n",
    "_A = torch.rand((N, N))\n",
    "A = torch.sqrt(_A.T @ _A).to(\"cuda\")  # positive semidefinite and symmetric\n",
    "b = torch.rand((N)).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_A = A.clone()\n",
    "_b = b.clone()\n",
    "_A.requires_grad = True\n",
    "_b.requires_grad = True\n",
    "x = conjugate_gradient(_A, _b, max_iter=5)\n",
    "x_gt = torch.linalg.solve(A, b)\n",
    "(x - x_gt).norm().backward()\n",
    "_A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import ConjugateGradient\n",
    "\n",
    "_A = A.clone()\n",
    "_b = b.clone()\n",
    "_A.requires_grad = True\n",
    "_b.requires_grad = True\n",
    "x = ConjugateGradient.apply(_A, _b)\n",
    "x_gt = torch.linalg.solve(A, b)\n",
    "(x - x_gt).norm().backward()\n",
    "_A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for i in range(9):\n",
    "    out = torch.load(\n",
    "        f\"/home/borth/GuidedResearch/logs/2024-07-14_20-16-20_pcg_sampling/linsys/000000{i}.pt\"\n",
    "    )\n",
    "    A = out[\"A\"]\n",
    "    b = out[\"b\"]\n",
    "    x = out[\"x\"]\n",
    "\n",
    "    print(torch.linalg.solve(A, b).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import JaccobiConditionNet\n",
    "\n",
    "j = JaccobiConditionNet()\n",
    "M = j(A)\n",
    "torch.linalg.cond(M @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from lib.optimizer.pcg import PCGSolver\n",
    "from lib.optimizer.pcg import conjugate_gradient\n",
    "from torch.optim import Adam, SGD\n",
    "import torch\n",
    "\n",
    "# define the matrixes\n",
    "\n",
    "\n",
    "def generate_data(N, x_eps=1e-02, a_eps=1e-01):\n",
    "    E = torch.rand((N, N)) * a_eps\n",
    "    A = torch.eye(N) + (E.T @ E)\n",
    "    x_gt = torch.rand((N)) * x_eps\n",
    "    b = A @ x_gt\n",
    "    A.requires_grad = True\n",
    "    b.requires_grad = True\n",
    "    return A, b, x_gt\n",
    "\n",
    "\n",
    "def eval_loss(x_pcg, x_gt, verbose=True):\n",
    "    l_pcg = (x_pcg - x_gt).norm()\n",
    "    if verbose:\n",
    "        print(\"Loss:\", l_pcg.item())\n",
    "    return l_pcg\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "N = 6\n",
    "# A, b, x_gt = generate_data(N)\n",
    "max_steps = 1000\n",
    "lr = 1e-03\n",
    "max_iter = 1\n",
    "verbose = True\n",
    "tol = 1e-08\n",
    "\n",
    "out = torch.load(\n",
    "    \"/home/borth/GuidedResearch/logs/2024-07-04_12-17-20_pcg_sampling/linsys/0000000.pt\"\n",
    ")\n",
    "A = out[\"A\"]\n",
    "b = out[\"b\"]\n",
    "# A.requires_grad = True\n",
    "# b.requires_grad = True\n",
    "x_gt = torch.linalg.solve(A, b)\n",
    "\n",
    "pcg = PCGSolver(\n",
    "    dim=N,\n",
    "    max_iter=max_iter,\n",
    "    verbose=verbose,\n",
    "    tol=tol,\n",
    "    mode=\"diagonal_offset\",\n",
    "    gradients=\"backprop\",\n",
    ")\n",
    "# optimizer = SGD(pcg.parameters(), lr=lr, momentum=0.90)\n",
    "optimizer = Adam(pcg.parameters(), lr=lr)\n",
    "\n",
    "init_loss = None\n",
    "for step in (pbar := tqdm(range(max_steps), total=max_steps)):\n",
    "    pbar.set_description(f\"{step}/{max_steps}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    A, b, x_gt = generate_data(N)\n",
    "    M = pcg.condition_net(A)\n",
    "    C_m = torch.linalg.cond(M @ A).item()\n",
    "    C_a = torch.linalg.cond(A).item()\n",
    "    x = pcg(A, b)\n",
    "    loss = eval_loss(x, x_gt, verbose=False)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if init_loss is None:\n",
    "        init_loss = loss.item()\n",
    "\n",
    "    pbar.set_postfix(\n",
    "        {\"init_loss\": init_loss, \"loss\": loss.item(), \"C_m\": C_m, \"C_a\": C_a}\n",
    "    )\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.data.datamodule import PCGDataModule\n",
    "from lib.data.dataset import PCGDataset\n",
    "\n",
    "path = \"/home/borth/GuidedResearch/data/linsys_pose\"\n",
    "dataset = PCGDataset(data_dir=path)\n",
    "out = torch.load(\"/home/borth/GuidedResearch/data/linsys_pose/0000000.pt\")\n",
    "A = out[\"A\"]\n",
    "b = out[\"b\"]\n",
    "x_gt = torch.linalg.solve(A, b)\n",
    "x_gt.norm(), x_gt, x_gt[:3].norm(), x_gt[3:6].norm(), x_gt[:3].norm() / x_gt[3:6].norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.norm(), A.inverse().norm(), torch.linalg.cond(A) / A.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gt = out[\"x_gt\"]\n",
    "eps = torch.rand_like(x_gt)\n",
    "eps /= torch.linalg.vector_norm(eps)\n",
    "eps *= 1e-02\n",
    "l1_solution = torch.abs(eps).mean()\n",
    "residual = torch.linalg.vector_norm(A @ (x_gt + eps) - b)\n",
    "# residual = torch.linalg.vector_norm(A @ x_gt - b)\n",
    "l1_solution, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import (\n",
    "    JaccobiConditionNet,\n",
    "    PCGSolver,\n",
    "    IdentityConditionNet,\n",
    "    ConditionNet,\n",
    "    DenseConditionNet,\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class JaccobiConditionNet1(ConditionNet):\n",
    "    name: str = \"JaccobiConditionNet\"\n",
    "\n",
    "    def forward(self, A: torch.Tensor):\n",
    "        diagonals = A.diagonal(dim1=-2, dim2=-1)\n",
    "        # diagonals = torch.nn.functional.softmax(1 / diagonals, dim=-1)\n",
    "        diagonals = 1 / diagonals\n",
    "        return torch.diag_embed(diagonals)\n",
    "\n",
    "\n",
    "condition_net = partial(DenseConditionNet, unknowns=6)\n",
    "solver = PCGSolver(check_convergence=True, condition_net=condition_net)\n",
    "x, info = solver(A, b)\n",
    "x, info[\"k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.inverse().diag() / A.inverse().diag().sum(), (1 / A.diag()) / (1 / A.diag()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import (\n",
    "    JaccobiConditionNet,\n",
    "    PCGSolver,\n",
    "    IdentityConditionNet,\n",
    "    ConditionNet,\n",
    "    DenseConditionNet,\n",
    ")\n",
    "\n",
    "cond = DenseConditionNet(unknowns=6)\n",
    "cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def foo0():\n",
    "#     batched_conjugate_gradient(batch[\"A\"], batch[\"b\"], max_iter=max_iter)\n",
    "\n",
    "\n",
    "# def foo1():\n",
    "#     for i in range(batch_size):\n",
    "#         conjugate_gradient(batch[\"A\"][i], batch[\"b\"][i], max_iter=max_iter)\n",
    "\n",
    "\n",
    "# print(timeit.timeit(foo0, number=n) / n)\n",
    "# print(timeit.timeit(foo1, number=n) / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from lib.optimizer.pcg import JaccobiConditionNet, preconditioned_conjugate_gradient\n",
    "import timeit\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from lib.data.dataset import PCGDataset\n",
    "from lib.optimizer.pcg import PCGSolver\n",
    "\n",
    "ckpt_path = (\n",
    "    \"/home/borth/GuidedResearch/logs/2024-07-11_12-23-43_pcg/checkpoints/last.ckpt\"\n",
    ")\n",
    "solver = PCGSolver.load_from_checkpoint(ckpt_path).to(\"cpu\")\n",
    "\n",
    "batch_size = 2\n",
    "path = \"/home/borth/GuidedResearch/data/linsys_pose\"\n",
    "dataset = PCGDataset(data_dir=path, split=\"test\")\n",
    "loader = DataLoader(dataset, batch_size=batch_size)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "jaccobi = JaccobiConditionNet()\n",
    "\n",
    "A = batch[\"A\"][0]\n",
    "b = batch[\"b\"][0]\n",
    "\n",
    "M = jaccobi(A)\n",
    "M_A = torch.matmul(M, A)  # (B, N, N) or (N, N)\n",
    "M_b = torch.matmul(M, b.unsqueeze(-1)).squeeze(-1)  # (B, N, N) or (N, N)\n",
    "x, info_jaccobi = preconditioned_conjugate_gradient(\n",
    "    A=A,\n",
    "    b=b,\n",
    "    M=M,\n",
    "    max_iter=20,\n",
    "    rel_tol=1e-08,\n",
    "    verbose=True,\n",
    "    check_convergence=False,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    M = solver.condition_net(A)\n",
    "M_A = torch.matmul(M, A)  # (B, N, N) or (N, N)\n",
    "M_b = torch.matmul(M, b.unsqueeze(-1)).squeeze(-1)  # (B, N, N) or (N, N)\n",
    "x, info_pcg = preconditioned_conjugate_gradient(\n",
    "    A=A,\n",
    "    b=b,\n",
    "    M=M,\n",
    "    max_iter=20,\n",
    "    rel_tol=1e-08,\n",
    "    verbose=True,\n",
    "    check_convergence=False,\n",
    ")\n",
    "\n",
    "x, info_wo = preconditioned_conjugate_gradient(\n",
    "    A=A,\n",
    "    b=b,\n",
    "    max_iter=20,\n",
    "    rel_tol=1e-08,\n",
    "    verbose=True,\n",
    "    check_convergence=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import Trainer\n",
    "\n",
    "trainer = Trainer()\n",
    "solver.max_iter = 20\n",
    "out = trainer.predict(solver, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "stats = defaultdict(list)\n",
    "for batch in out:\n",
    "    for key, value in batch.items():\n",
    "        stats[key].append(value)\n",
    "for key, value in stats.items():\n",
    "    stats[key] = torch.stack(value, dim=-1)\n",
    "cond = stats[\"cond\"].mean()\n",
    "iters = stats[\"relres_norms\"].size(0)\n",
    "relres_norms = stats[\"relres_norms\"].view(iters, -1).mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relres_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the relative residual norms\n",
    "relres = [v.detach() for v in info_pcg[\"relres_norms\"]]\n",
    "relres_jaccobi = info_jaccobi[\"relres_norms\"]\n",
    "relres_wo = info_wo[\"relres_norms\"]\n",
    "\n",
    "# Create a range for the x-axis based on the length of the data\n",
    "iterations_pcg = range(len(relres_pcg))\n",
    "iterations_jaccobi = range(len(relres_jaccobi))\n",
    "iterations_wo = range(len(relres_wo))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations_pcg, relres_pcg, label=\"PCG\", marker=\"o\")\n",
    "plt.plot(iterations_jaccobi, relres_jaccobi, label=\"Jaccobi\", marker=\"o\")\n",
    "plt.plot(iterations_wo, relres_wo, label=\"Without Optimization\", marker=\"s\")\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title(\"Relative Residual Norms Comparison\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Relative Residual Norms\")\n",
    "\n",
    "# Set y-axis to log scale\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "# Set y-axis limit to start from 10^-8\n",
    "plt.ylim(bottom=1e-7)\n",
    "\n",
    "# Add a horizontal red line at 10^-6\n",
    "plt.axhline(y=1e-6, color=\"red\", linestyle=\"--\", label=\"Convergence at $10^{-6}$\")\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(batch[\"A\"][0], batch[\"b\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import conjugate_gradient, log\n",
    "import logging\n",
    "\n",
    "log.setLevel(logging.INFO)\n",
    "conjugate_gradient(batch[\"A\"][0], batch[\"b\"][0], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag_embed(1 / batch[\"A\"][0].diagonal(dim1=-2, dim2=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = batch[\"A\"]\n",
    "ones = torch.ones((A.shape[0], A.shape[1]), device=A.device)\n",
    "torch.diag_embed(ones).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.solve(batch[\"A\"], batch[\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones_like(A)[0]\n",
    "x.expand(A[0].shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.bmm(A[None], b[None, ..., None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "\n",
    "def foo():\n",
    "    b[None] @ b\n",
    "\n",
    "\n",
    "n = 100000\n",
    "timeit.timeit(foo, number=n) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    if torch.norm(b) > 0.1:\n",
    "        pass\n",
    "\n",
    "\n",
    "n = 100000\n",
    "timeit.timeit(foo, number=n) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcg = PCGSolver(dim=N, max_iter=max_iter, verbose=verbose, tol=tol, mode=\"dense\")\n",
    "A, b, x_gt = generate_data(N)\n",
    "M = pcg.condition_net(A)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.cond(pcg.condition_net(A) @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import preconditioned_conjugate_gradient\n",
    "\n",
    "A, b, x_gt = generate_data(N)\n",
    "x_cg = preconditioned_conjugate_gradient(A, b, max_iter=2)\n",
    "pcg.max_iter = 2\n",
    "x = pcg(A, b)\n",
    "loss = eval_loss(x, x_gt, verbose=True)\n",
    "loss = eval_loss(x_cg, x_gt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(p.size() for p in pcg.condition_net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import conjugate_gradient\n",
    "\n",
    "torch.manual_seed(42)\n",
    "N = 100\n",
    "E = torch.rand((N, N)) * 1e-02\n",
    "A = torch.eye(N) + (E.T @ E)\n",
    "x = torch.rand((N)) * 1e-02\n",
    "b = A @ x\n",
    "\n",
    "x_pcg = conjugate_gradient(A, b, max_iter=1)\n",
    "r_pcg = (A @ x_pcg - b).mean()\n",
    "r_pcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "tri_N = ((N * N - N) // 2) + N\n",
    "L = torch.zeros((N, N))\n",
    "tril_indices = torch.tril_indices(row=N, col=N, offset=0)\n",
    "L[tril_indices[0], tril_indices[1]] = torch.rand(tri_N)\n",
    "L[4, 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "def cg_batch(\n",
    "    A_bmm, B, M_bmm=None, X0=None, rtol=1e-3, atol=0.0, maxiter=None, verbose=False\n",
    "):\n",
    "    \"\"\"Solves a batch of PD matrix linear systems using the preconditioned CG algorithm.\n",
    "\n",
    "    This function solves a batch of matrix linear systems of the form\n",
    "\n",
    "        A_i X_i = B_i,  i=1,...,K,\n",
    "\n",
    "    where A_i is a n x n positive definite matrix and B_i is a n x m matrix,\n",
    "    and X_i is the n x m matrix representing the solution for the ith system.\n",
    "\n",
    "    Args:\n",
    "        A_bmm: A callable that performs a batch matrix multiply of A and a K x n x m matrix.\n",
    "        B: A K x n x m matrix representing the right hand sides.\n",
    "        M_bmm: (optional) A callable that performs a batch matrix multiply of the preconditioning\n",
    "            matrices M and a K x n x m matrix. (default=identity matrix)\n",
    "        X0: (optional) Initial guess for X, defaults to M_bmm(B). (default=None)\n",
    "        rtol: (optional) Relative tolerance for norm of residual. (default=1e-3)\n",
    "        atol: (optional) Absolute tolerance for norm of residual. (default=0)\n",
    "        maxiter: (optional) Maximum number of iterations to perform. (default=5*n)\n",
    "        verbose: (optional) Whether or not to print status messages. (default=False)\n",
    "    \"\"\"\n",
    "    K, n, m = B.shape\n",
    "\n",
    "    if M_bmm is None:\n",
    "        M_bmm = lambda x: x\n",
    "    if X0 is None:\n",
    "        X0 = M_bmm(B)\n",
    "    if maxiter is None:\n",
    "        maxiter = 5 * n\n",
    "\n",
    "    assert B.shape == (K, n, m)\n",
    "    assert X0.shape == (K, n, m)\n",
    "    assert rtol > 0 or atol > 0\n",
    "    assert isinstance(maxiter, int)\n",
    "\n",
    "    X_k = X0\n",
    "    R_k = B - A_bmm(X_k)\n",
    "    Z_k = M_bmm(R_k)\n",
    "\n",
    "    P_k = torch.zeros_like(Z_k)\n",
    "\n",
    "    P_k1 = P_k\n",
    "    R_k1 = R_k\n",
    "    R_k2 = R_k\n",
    "    X_k1 = X0\n",
    "    Z_k1 = Z_k\n",
    "    Z_k2 = Z_k\n",
    "\n",
    "    B_norm = torch.norm(B, dim=1)\n",
    "    stopping_matrix = torch.max(rtol * B_norm, atol * torch.ones_like(B_norm))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"%03s | %010s %06s\" % (\"it\", \"dist\", \"it/s\"))\n",
    "\n",
    "    optimal = False\n",
    "    start = time.perf_counter()\n",
    "    for k in range(1, maxiter + 1):\n",
    "        start_iter = time.perf_counter()\n",
    "        Z_k = M_bmm(R_k)\n",
    "\n",
    "        if k == 1:\n",
    "            P_k = Z_k\n",
    "            R_k1 = R_k\n",
    "            X_k1 = X_k\n",
    "            Z_k1 = Z_k\n",
    "        else:\n",
    "            R_k2 = R_k1\n",
    "            Z_k2 = Z_k1\n",
    "            P_k1 = P_k\n",
    "            R_k1 = R_k\n",
    "            Z_k1 = Z_k\n",
    "            X_k1 = X_k\n",
    "            denominator = (R_k2 * Z_k2).sum(1)\n",
    "            denominator[denominator == 0] = 1e-8\n",
    "            beta = (R_k1 * Z_k1).sum(1) / denominator\n",
    "            P_k = Z_k1 + beta.unsqueeze(1) * P_k1\n",
    "\n",
    "        denominator = (P_k * A_bmm(P_k)).sum(1)\n",
    "        denominator[denominator == 0] = 1e-8\n",
    "        alpha = (R_k1 * Z_k1).sum(1) / denominator\n",
    "        X_k = X_k1 + alpha.unsqueeze(1) * P_k\n",
    "        R_k = R_k1 - alpha.unsqueeze(1) * A_bmm(P_k)\n",
    "        end_iter = time.perf_counter()\n",
    "\n",
    "        residual_norm = torch.norm(A_bmm(X_k) - B, dim=1)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"%03d | %8.4e %4.2f\"\n",
    "                % (\n",
    "                    k,\n",
    "                    torch.max(residual_norm - stopping_matrix),\n",
    "                    1.0 / (end_iter - start_iter),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if (residual_norm <= stopping_matrix).all():\n",
    "            optimal = True\n",
    "            break\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    if verbose:\n",
    "        if optimal:\n",
    "            print(\n",
    "                \"Terminated in %d steps (reached maxiter). Took %.3f ms.\"\n",
    "                % (k, (end - start) * 1000)\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"Terminated in %d steps (optimal). Took %.3f ms.\"\n",
    "                % (k, (end - start) * 1000)\n",
    "            )\n",
    "\n",
    "    info = {\"niter\": k, \"optimal\": optimal}\n",
    "\n",
    "    return X_k, info\n",
    "\n",
    "\n",
    "A = torch.tensor([[4.0, 1], [1, 3]])[None, ...]\n",
    "\n",
    "\n",
    "def A_bmm(X):\n",
    "    Y = [(A[i] @ X[i]).unsqueeze(0) for i in range(1)]\n",
    "    return torch.cat(Y, dim=0)\n",
    "\n",
    "\n",
    "b = torch.tensor([1.0, 2])[None, ...][..., None]\n",
    "cg_batch(A_bmm=A_bmm, B=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "_A = torch.rand((N, N))\n",
    "z = torch.zeros_like(_A)\n",
    "for idx in range(70):\n",
    "    i = idx * 10\n",
    "    j = (idx + 1) * 10\n",
    "    z[i:j, i:j] = 1.0\n",
    "_A *= z\n",
    "A = torch.sqrt(_A.T @ _A)  # positive and symetric\n",
    "# A = torch.diag(torch.diag(A))\n",
    "b = torch.rand((N)) + 2\n",
    "\n",
    "\n",
    "def A_bmm(X):\n",
    "    Y = [(A[None, ...][i] @ X[i]).unsqueeze(0) for i in range(1)]\n",
    "    return torch.cat(Y, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "s = time.time()\n",
    "out = cg_batch(A_bmm=A_bmm, B=b[None, ...][..., None], maxiter=10)\n",
    "print(time.time() - s)\n",
    "x_pcg = out[0].squeeze()\n",
    "s = time.time()\n",
    "x_optim = torch.linalg.solve(A, b)\n",
    "print(time.time() - s)\n",
    "pcg_norm = ((A @ x_pcg) - b).norm()\n",
    "optim_norm = ((A @ x_optim) - b).norm()\n",
    "pcg_norm, optim_norm, b.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: int = 1,\n",
    "        stride: int = 1,\n",
    "        dilation: int = 1,\n",
    "        start_frame: int = 0,\n",
    "        end_frame: int = 126,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.start_frame = start_frame\n",
    "        self.end_frame = end_frame\n",
    "        self.frames = list(range(self.start_frame, self.end_frame))\n",
    "        self.mode = \"sequential\"\n",
    "        self.prev_last_frame_idx = self.start_frame\n",
    "        self.kernal_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "\n",
    "    def frame_idxs_iter(self):\n",
    "        frame_idxs = list(range(self.start_frame, self.end_frame + 1, self.dilation))\n",
    "        for idx in range(0, len(frame_idxs), self.stride):\n",
    "            idxs = frame_idxs[idx : idx + self.kernal_size]\n",
    "            if len(idxs) == self.kernal_size:\n",
    "                yield idxs\n",
    "\n",
    "\n",
    "trainer = SequentialTrainer(\n",
    "    kernel_size=3,\n",
    "    stride=3,\n",
    "    dilation=2,\n",
    "    start_frame=0,\n",
    "    end_frame=10,\n",
    ")\n",
    "\n",
    "for frame_idxs in trainer.frame_idxs_iter():\n",
    "    print(frame_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = 2\n",
    "_A = torch.rand((N, N))\n",
    "A = _A.T @ _A  # positive semidefinite and symmetric\n",
    "b = torch.rand((N))\n",
    "x = torch.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.zeros(N, requires_grad=True)\n",
    "residual = ((A @ x0 - b) ** 2).sum()\n",
    "residual.backward()\n",
    "x0.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.zeros(N, requires_grad=True)\n",
    "error = ((x0 - x) ** 2).sum()\n",
    "error.backward()\n",
    "x0.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1.0, -1], [-1, 1]])\n",
    "A @ x.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = (x0 - x).sum()\n",
    "error.backward()\n",
    "x0.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
