{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.optimizer.pcg import conjugate_gradient\n",
    "\n",
    "N = 400\n",
    "_A = torch.rand((N, N))\n",
    "A = torch.sqrt(_A.T @ _A) # positive semidefinite and symmetric\n",
    "b = torch.rand((N))\n",
    "# x_pcg  = conjugate_gradient(A, b, verbose=True, max_iter=100)\n",
    "# x_optim= torch.linalg.solve(A, b) \n",
    "# pcg_norm = ((A @ x_pcg) - b).norm()\n",
    "# optim_norm  = ((A @ x_optim) - b).norm()\n",
    "# print(pcg_norm, optim_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "t = 100\n",
    "A = A.to(\"cuda\")\n",
    "b = b.to(\"cuda\")\n",
    "\n",
    "\n",
    "def foo():\n",
    "    conjugate_gradient(A, b, verbose=True, max_iter=5)\n",
    "    # torch.linalg.solve(A, b)\n",
    "\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=False\n",
    ") as prof:\n",
    "    # foo()\n",
    "    timeit.timeit(foo, number=t) / t\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import conjugate_gradient\n",
    "\n",
    "def conjugate_gradient(\n",
    "    A: torch.Tensor,  # dim (N,N)\n",
    "    b: torch.Tensor,  # dim (N)\n",
    "    x0: torch.Tensor | None = None,  # dim (N)\n",
    "    max_iter: int = 20,\n",
    "    verbose: bool = False,\n",
    "    tol: float = 1e-08,\n",
    "):\n",
    "    k = 0\n",
    "    converged = False\n",
    "\n",
    "    xk = torch.zeros_like(b) if x0 is None else x0  # (N)\n",
    "    rk = b - A @ xk  # column vector (N)\n",
    "    pk = rk\n",
    "\n",
    "    if torch.norm(rk) < tol:\n",
    "        converged = True\n",
    "\n",
    "    while k < max_iter and not converged:\n",
    "        \n",
    "        # compute step size\n",
    "        ak = (rk[None] @ rk) / (pk[None] @ A @ pk)\n",
    "        # update unknowns\n",
    "        xk_1 = xk + ak * pk\n",
    "        # compute residuals\n",
    "        rk_1 = rk - ak * A @ pk\n",
    "        # compute new pk\n",
    "        bk = (rk_1[None] @ rk_1) / (rk[None] @ rk)\n",
    "        pk_1 = rk_1 + bk * pk\n",
    "        # update the next stateprint\n",
    "        xk = xk_1\n",
    "        pk = pk_1\n",
    "        rk = rk_1\n",
    "\n",
    "        k += 1\n",
    "        if torch.norm(rk) < tol:\n",
    "            converged = True\n",
    "\n",
    "    return xk\n",
    "\n",
    "t = 1000\n",
    "N = 6\n",
    "_A = torch.rand((N, N))\n",
    "A = torch.sqrt(_A.T @ _A).to(\"cuda\") # positive semidefinite and symmetric\n",
    "b = torch.rand((N)).to(\"cuda\")\n",
    "\n",
    "def foo():\n",
    "    conjugate_gradient(A, b, max_iter=4)\n",
    "    # torch.linalg.solve(A,b) \n",
    "\n",
    "print((timeit.timeit(foo, number=t) / t ) * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "out = torch.load(\"/home/borth/GuidedResearch/logs/2024-07-04_09-39-30_optimize/linsys/0000000.pt\")\n",
    "A = out[\"A\"]\n",
    "b = out[\"b\"]\n",
    "x = out[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import JaccobiConditionNet\n",
    "j = JaccobiConditionNet()\n",
    "M = j(A)\n",
    "torch.linalg.cond(M @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from lib.optimizer.pcg import PCGSolver\n",
    "from lib.optimizer.pcg import conjugate_gradient\n",
    "from torch.optim import Adam, SGD\n",
    "import torch\n",
    "\n",
    "# define the matrixes\n",
    "\n",
    "def generate_data(N, x_eps= 1e-02, a_eps=1e-01):\n",
    "    E = torch.rand((N, N)) * a_eps\n",
    "    A = torch.eye(N) + (E.T @ E) \n",
    "    x_gt = torch.rand((N))  * x_eps\n",
    "    b = A @ x_gt\n",
    "    return A, b, x_gt\n",
    "\n",
    "def eval_loss(x_pcg, x_gt, verbose=True):\n",
    "    l_pcg = (x_pcg-x_gt).norm()\n",
    "    if verbose:\n",
    "        print(\"Loss:\", l_pcg.item())\n",
    "    return l_pcg\n",
    "\n",
    "torch.manual_seed(42)\n",
    "N = 6\n",
    "# A, b, x_gt = generate_data(N)\n",
    "max_steps = 1000\n",
    "lr = 1e-06\n",
    "max_iter=1\n",
    "verbose=True\n",
    "tol=1e-08\n",
    "\n",
    "out = torch.load(\"/home/borth/GuidedResearch/logs/2024-07-04_09-39-30_optimize/linsys/0000000.pt\")\n",
    "A = out[\"A\"]\n",
    "b = out[\"b\"]\n",
    "x_gt = torch.linalg.solve(A, b)\n",
    "\n",
    "pcg = PCGSolver(dim=N, max_iter=max_iter, verbose=verbose, tol=tol, mode=\"dense\")\n",
    "optimizer = SGD(pcg.parameters(), lr=lr, momentum=0.90)\n",
    "# optimizer = Adam([M], lr=lr)\n",
    "\n",
    "for step in (pbar:=tqdm(range(max_steps), total=max_steps)):\n",
    "    pbar.set_description(f\"{step}/{max_steps}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "  \n",
    "    A, b, x_gt = generate_data(N)\n",
    "    M = pcg.condition_net(A)\n",
    "    C_m = torch.linalg.cond(M @ A).item()\n",
    "    C_a = torch.linalg.cond(A).item()\n",
    "    x = pcg(A, b)\n",
    "    loss = eval_loss(x, x_gt, verbose=False) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    pbar.set_postfix({\"loss\": loss.item(), \"C_m\": C_m, \"C_a\": C_a})\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcg = PCGSolver(dim=N, max_iter=max_iter, verbose=verbose, tol=tol, mode=\"dense\")\n",
    "A, b, x_gt = generate_data(N)\n",
    "M = pcg.condition_net(A)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.cond(pcg.condition_net(A) @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import preconditioned_conjugate_gradient\n",
    "A, b, x_gt = generate_data(N)\n",
    "x_cg = preconditioned_conjugate_gradient(A, b, max_iter=2)\n",
    "pcg.max_iter=2\n",
    "x = pcg(A, b)\n",
    "loss = eval_loss(x, x_gt, verbose=True) \n",
    "loss = eval_loss(x_cg, x_gt, verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(p.size() for p in pcg.condition_net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import conjugate_gradient\n",
    "torch.manual_seed(42)\n",
    "N = 100\n",
    "E = torch.rand((N, N)) * 1e-02\n",
    "A= torch.eye(N) + (E.T @ E)\n",
    "x = torch.rand((N))  * 1e-02\n",
    "b = A @ x\n",
    "\n",
    "x_pcg = conjugate_gradient(A, b, max_iter=1)\n",
    "r_pcg = (A @ x_pcg - b).mean()\n",
    "r_pcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "tri_N = ((N * N - N) // 2) + N\n",
    "L = torch.zeros((N, N))\n",
    "tril_indices = torch.tril_indices(row=N, col=N, offset=0)\n",
    "L[tril_indices[0], tril_indices[1]] = torch.rand(tri_N)\n",
    "L[4, 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "def cg_batch(A_bmm, B, M_bmm=None, X0=None, rtol=1e-3, atol=0., maxiter=None, verbose=False):\n",
    "    \"\"\"Solves a batch of PD matrix linear systems using the preconditioned CG algorithm.\n",
    "\n",
    "    This function solves a batch of matrix linear systems of the form\n",
    "\n",
    "        A_i X_i = B_i,  i=1,...,K,\n",
    "\n",
    "    where A_i is a n x n positive definite matrix and B_i is a n x m matrix,\n",
    "    and X_i is the n x m matrix representing the solution for the ith system.\n",
    "\n",
    "    Args:\n",
    "        A_bmm: A callable that performs a batch matrix multiply of A and a K x n x m matrix.\n",
    "        B: A K x n x m matrix representing the right hand sides.\n",
    "        M_bmm: (optional) A callable that performs a batch matrix multiply of the preconditioning\n",
    "            matrices M and a K x n x m matrix. (default=identity matrix)\n",
    "        X0: (optional) Initial guess for X, defaults to M_bmm(B). (default=None)\n",
    "        rtol: (optional) Relative tolerance for norm of residual. (default=1e-3)\n",
    "        atol: (optional) Absolute tolerance for norm of residual. (default=0)\n",
    "        maxiter: (optional) Maximum number of iterations to perform. (default=5*n)\n",
    "        verbose: (optional) Whether or not to print status messages. (default=False)\n",
    "    \"\"\"\n",
    "    K, n, m = B.shape\n",
    "\n",
    "    if M_bmm is None:\n",
    "        M_bmm = lambda x: x\n",
    "    if X0 is None:\n",
    "        X0 = M_bmm(B)\n",
    "    if maxiter is None:\n",
    "        maxiter = 5 * n\n",
    "\n",
    "    assert B.shape == (K, n, m)\n",
    "    assert X0.shape == (K, n, m)\n",
    "    assert rtol > 0 or atol > 0\n",
    "    assert isinstance(maxiter, int)\n",
    "\n",
    "    X_k = X0\n",
    "    R_k = B - A_bmm(X_k)\n",
    "    Z_k = M_bmm(R_k)\n",
    "\n",
    "    P_k = torch.zeros_like(Z_k)\n",
    "\n",
    "    P_k1 = P_k\n",
    "    R_k1 = R_k\n",
    "    R_k2 = R_k\n",
    "    X_k1 = X0\n",
    "    Z_k1 = Z_k\n",
    "    Z_k2 = Z_k\n",
    "\n",
    "    B_norm = torch.norm(B, dim=1)\n",
    "    stopping_matrix = torch.max(rtol*B_norm, atol*torch.ones_like(B_norm))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"%03s | %010s %06s\" % (\"it\", \"dist\", \"it/s\"))\n",
    "\n",
    "    optimal = False\n",
    "    start = time.perf_counter()\n",
    "    for k in range(1, maxiter + 1):\n",
    "        start_iter = time.perf_counter()\n",
    "        Z_k = M_bmm(R_k)\n",
    "\n",
    "        if k == 1:\n",
    "            P_k = Z_k\n",
    "            R_k1 = R_k\n",
    "            X_k1 = X_k\n",
    "            Z_k1 = Z_k\n",
    "        else:\n",
    "            R_k2 = R_k1\n",
    "            Z_k2 = Z_k1\n",
    "            P_k1 = P_k\n",
    "            R_k1 = R_k\n",
    "            Z_k1 = Z_k\n",
    "            X_k1 = X_k\n",
    "            denominator = (R_k2 * Z_k2).sum(1)\n",
    "            denominator[denominator == 0] = 1e-8\n",
    "            beta = (R_k1 * Z_k1).sum(1) / denominator\n",
    "            P_k = Z_k1 + beta.unsqueeze(1) * P_k1\n",
    "\n",
    "        denominator = (P_k * A_bmm(P_k)).sum(1)\n",
    "        denominator[denominator == 0] = 1e-8\n",
    "        alpha = (R_k1 * Z_k1).sum(1) / denominator\n",
    "        X_k = X_k1 + alpha.unsqueeze(1) * P_k\n",
    "        R_k = R_k1 - alpha.unsqueeze(1) * A_bmm(P_k)\n",
    "        end_iter = time.perf_counter()\n",
    "\n",
    "        residual_norm = torch.norm(A_bmm(X_k) - B, dim=1)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"%03d | %8.4e %4.2f\" %\n",
    "                  (k, torch.max(residual_norm-stopping_matrix),\n",
    "                    1. / (end_iter - start_iter)))\n",
    "\n",
    "        if (residual_norm <= stopping_matrix).all():\n",
    "            optimal = True\n",
    "            break\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    if verbose:\n",
    "        if optimal:\n",
    "            print(\"Terminated in %d steps (reached maxiter). Took %.3f ms.\" %\n",
    "                  (k, (end - start) * 1000))\n",
    "        else:\n",
    "            print(\"Terminated in %d steps (optimal). Took %.3f ms.\" %\n",
    "                  (k, (end - start) * 1000))\n",
    "\n",
    "\n",
    "    info = {\n",
    "        \"niter\": k,\n",
    "        \"optimal\": optimal\n",
    "    }\n",
    "\n",
    "    return X_k, info\n",
    "\n",
    "\n",
    "A = torch.tensor([[4.0,1],[1, 3]])[None, ...]\n",
    "def A_bmm(X):\n",
    "    Y = [(A[i]@X[i]).unsqueeze(0) for i in range(1)]\n",
    "    return torch.cat(Y, dim=0)\n",
    "b = torch.tensor([1.0,2])[None, ...][..., None]\n",
    "cg_batch(A_bmm=A_bmm, B=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "_A = torch.rand((N, N))\n",
    "z = torch.zeros_like(_A)\n",
    "for idx in range(70):\n",
    "    i = idx * 10\n",
    "    j = (idx+1)*10\n",
    "    z[i:j, i:j] = 1.0\n",
    "_A *= z\n",
    "A = torch.sqrt(_A.T @ _A) # positive and symetric\n",
    "# A = torch.diag(torch.diag(A))\n",
    "b = torch.rand((N)) + 2\n",
    "def A_bmm(X):\n",
    "    Y = [(A[None, ...][i]@X[i]).unsqueeze(0) for i in range(1)]\n",
    "    return torch.cat(Y, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "out = cg_batch(A_bmm=A_bmm, B=b[None, ...][..., None], maxiter=10)\n",
    "print(time.time() - s)\n",
    "x_pcg = out[0].squeeze()\n",
    "s = time.time()\n",
    "x_optim = torch.linalg.solve(A, b)\n",
    "print(time.time() - s)\n",
    "pcg_norm = ((A @ x_pcg) - b).norm()\n",
    "optim_norm = ((A @ x_optim) - b).norm()\n",
    "pcg_norm, optim_norm, b.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
