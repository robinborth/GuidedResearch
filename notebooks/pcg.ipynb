{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from lib.optimizer.pcg import conjugate_gradient\n",
    "\n",
    "N = 400\n",
    "_A = torch.rand((N, N))\n",
    "A = torch.sqrt(_A.T @ _A)  # positive semidefinite and symmetric\n",
    "b = torch.rand((N))\n",
    "# x_pcg  = conjugate_gradient(A, b, verbose=True, max_iter=100)\n",
    "# x_optim= torch.linalg.solve(A, b)\n",
    "# pcg_norm = ((A @ x_pcg) - b).norm()\n",
    "# optim_norm  = ((A @ x_optim) - b).norm()\n",
    "# print(pcg_norm, optim_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2438e-01, 5.7052e-01, 6.4203e-01, 4.5326e-01, 7.8424e-01, 5.8218e-01,\n",
       "        8.4932e-01, 8.3851e-01, 7.7987e-01, 7.6363e-01, 6.9150e-03, 2.6182e-01,\n",
       "        4.7455e-01, 3.4149e-01, 9.7583e-01, 6.9472e-01, 2.7896e-01, 1.6647e-01,\n",
       "        6.8770e-01, 9.6306e-02, 6.2677e-01, 7.4740e-01, 7.6605e-01, 3.6917e-01,\n",
       "        3.0158e-01, 9.6105e-01, 9.5586e-01, 9.0776e-01, 7.0165e-01, 6.9329e-01,\n",
       "        4.2780e-01, 2.1799e-01, 3.7844e-01, 7.7990e-01, 9.4239e-01, 2.6883e-01,\n",
       "        1.3953e-01, 4.6822e-01, 8.4753e-01, 1.8975e-01, 2.7010e-01, 6.4061e-01,\n",
       "        2.5709e-01, 9.5244e-01, 3.8296e-01, 6.9529e-02, 1.8340e-02, 7.4757e-01,\n",
       "        5.0680e-01, 5.1879e-01, 9.9723e-01, 2.7011e-01, 2.0340e-01, 1.5057e-01,\n",
       "        5.1573e-01, 1.9244e-02, 4.8738e-02, 1.5313e-02, 6.5148e-01, 3.6664e-01,\n",
       "        7.2746e-01, 4.3830e-01, 8.3566e-01, 5.4827e-02, 9.9116e-01, 3.3702e-01,\n",
       "        1.7456e-01, 3.8154e-01, 4.1482e-01, 3.9646e-01, 8.7405e-01, 4.5238e-01,\n",
       "        6.3969e-01, 6.8047e-01, 9.5360e-01, 5.1420e-01, 1.8838e-01, 3.5923e-01,\n",
       "        2.9643e-01, 8.5415e-01, 4.6135e-01, 2.9518e-01, 2.5232e-01, 8.1109e-01,\n",
       "        3.6581e-01, 3.5942e-01, 3.2858e-01, 5.8144e-02, 5.7918e-01, 9.6662e-01,\n",
       "        2.4523e-01, 2.3714e-01, 9.2307e-01, 4.3043e-01, 6.4201e-01, 8.0147e-01,\n",
       "        2.7052e-01, 5.7151e-01, 3.2147e-01, 2.7161e-02, 2.9936e-01, 4.8127e-02,\n",
       "        5.0448e-01, 9.8805e-01, 1.7955e-01, 6.8463e-01, 8.9856e-01, 4.2263e-02,\n",
       "        1.9416e-01, 9.6893e-01, 9.7313e-01, 2.0847e-01, 3.2366e-01, 7.5048e-01,\n",
       "        4.4021e-01, 9.9899e-01, 9.8810e-01, 5.5388e-01, 4.1829e-02, 7.1953e-02,\n",
       "        2.9335e-01, 9.1796e-01, 2.3075e-01, 7.0079e-01, 4.5447e-01, 2.3137e-02,\n",
       "        9.3840e-01, 8.5561e-01, 4.5518e-01, 1.0086e-01, 9.6821e-02, 8.7223e-02,\n",
       "        2.3215e-01, 6.2988e-01, 7.7320e-01, 9.0824e-01, 6.0830e-01, 9.6593e-01,\n",
       "        9.3184e-01, 5.0947e-01, 9.1697e-01, 1.8034e-01, 8.8497e-01, 5.2748e-01,\n",
       "        4.8687e-02, 9.9863e-01, 7.8317e-01, 6.4471e-01, 3.9623e-01, 8.1559e-01,\n",
       "        3.0127e-01, 7.7299e-01, 3.0920e-01, 1.7608e-01, 2.1348e-01, 9.1833e-02,\n",
       "        6.4970e-01, 6.2400e-01, 1.9165e-01, 9.9102e-03, 6.0976e-01, 5.7284e-01,\n",
       "        8.4531e-01, 6.4545e-01, 4.7387e-01, 1.4359e-01, 3.7379e-02, 4.2058e-02,\n",
       "        7.1595e-01, 3.5957e-01, 9.7416e-01, 6.4365e-01, 6.1079e-01, 4.8270e-01,\n",
       "        3.9416e-01, 8.1338e-01, 2.7134e-01, 8.8912e-01, 1.5955e-01, 6.2060e-01,\n",
       "        6.1427e-01, 6.3152e-01, 5.4588e-01, 9.9907e-01, 5.5230e-01, 7.3060e-01,\n",
       "        7.4799e-02, 9.8761e-01, 4.2707e-01, 5.7869e-01, 7.0369e-01, 7.2067e-01,\n",
       "        9.2243e-01, 9.2040e-01, 5.7005e-01, 6.9272e-01, 1.0993e-01, 2.3781e-01,\n",
       "        4.8960e-02, 1.4405e-01, 3.9682e-01, 4.6868e-02, 8.8285e-01, 5.7725e-01,\n",
       "        5.3007e-01, 2.2797e-01, 3.8042e-01, 4.4789e-01, 7.9663e-01, 7.5725e-01,\n",
       "        1.6652e-01, 4.2649e-01, 8.6389e-01, 4.1565e-01, 1.5605e-01, 4.7533e-02,\n",
       "        7.2264e-01, 6.9693e-01, 4.3594e-01, 5.5917e-01, 8.7829e-01, 9.9065e-01,\n",
       "        2.4188e-01, 6.0654e-01, 7.5064e-01, 1.6408e-01, 4.3782e-01, 5.5132e-01,\n",
       "        7.6298e-02, 3.6275e-01, 3.3491e-03, 9.6039e-01, 8.5008e-01, 1.0668e-01,\n",
       "        1.5730e-01, 1.2480e-01, 8.1240e-01, 4.5931e-01, 6.6405e-01, 5.0677e-01,\n",
       "        6.5449e-01, 5.0833e-02, 4.8251e-01, 6.2351e-01, 8.7889e-01, 2.7004e-01,\n",
       "        5.0005e-01, 4.1999e-01, 8.0479e-01, 2.8487e-01, 1.0923e-01, 9.6814e-01,\n",
       "        4.6827e-01, 8.7143e-01, 4.6433e-01, 5.5617e-01, 7.5974e-02, 8.1626e-01,\n",
       "        1.8230e-01, 8.9135e-01, 7.6905e-01, 6.6413e-01, 2.1666e-01, 6.6897e-01,\n",
       "        6.8011e-02, 2.5628e-01, 8.9340e-01, 6.8987e-01, 1.3141e-01, 5.5659e-01,\n",
       "        6.8987e-01, 7.1741e-01, 3.2381e-01, 5.6961e-01, 8.0448e-01, 1.6770e-01,\n",
       "        9.9741e-01, 2.9083e-01, 9.5891e-01, 8.2733e-01, 1.4257e-01, 1.0671e-01,\n",
       "        7.6735e-01, 1.6091e-01, 7.4488e-01, 3.0426e-01, 9.4510e-01, 8.1178e-01,\n",
       "        8.4078e-01, 5.1378e-01, 2.1408e-01, 5.5960e-02, 9.1642e-01, 3.6494e-01,\n",
       "        2.8769e-01, 9.7704e-01, 6.8650e-01, 2.1783e-01, 8.7703e-01, 3.3331e-01,\n",
       "        4.5480e-01, 9.9817e-01, 3.0397e-02, 7.8293e-01, 4.4627e-01, 6.6369e-01,\n",
       "        4.1358e-01, 6.0494e-01, 9.7551e-01, 2.3531e-01, 8.7466e-01, 2.2011e-01,\n",
       "        6.6647e-01, 4.5765e-01, 7.1812e-01, 7.5066e-01, 6.3755e-01, 4.6215e-02,\n",
       "        7.4473e-01, 2.9068e-01, 1.1388e-01, 9.2622e-01, 8.3383e-01, 7.3755e-01,\n",
       "        5.8467e-01, 7.0826e-01, 4.2539e-01, 2.7272e-01, 6.7786e-01, 4.3178e-01,\n",
       "        8.5367e-01, 7.7189e-01, 9.4134e-01, 3.4029e-01, 1.0951e-01, 1.5079e-01,\n",
       "        6.7967e-02, 8.8202e-01, 6.8082e-02, 6.3058e-02, 7.5379e-01, 6.4590e-01,\n",
       "        1.5036e-01, 5.7785e-02, 5.2966e-01, 1.0518e-02, 1.1362e-01, 5.2864e-01,\n",
       "        2.4482e-01, 9.6669e-01, 7.1981e-01, 8.5981e-01, 3.1614e-01, 1.3238e-01,\n",
       "        3.1655e-01, 8.1120e-01, 6.5060e-01, 7.9879e-01, 2.7853e-01, 1.9293e-01,\n",
       "        8.8440e-02, 9.3512e-01, 6.1485e-01, 7.7287e-01, 4.6103e-01, 8.4256e-01,\n",
       "        3.4256e-01, 1.8058e-01, 6.3764e-01, 7.4743e-01, 9.5661e-01, 6.4684e-01,\n",
       "        1.2519e-01, 4.2701e-01, 3.5276e-01, 8.3862e-01, 4.1310e-01, 1.6117e-01,\n",
       "        9.3333e-01, 4.8127e-01, 7.8405e-01, 4.6600e-01, 7.8203e-01, 2.3103e-01,\n",
       "        9.5234e-01, 7.0536e-02, 6.8968e-01, 2.7869e-01, 2.8645e-01, 6.9141e-04,\n",
       "        7.6444e-02, 7.3313e-01, 5.8319e-01, 4.1547e-01, 9.4401e-01, 7.6260e-01,\n",
       "        9.5187e-01, 8.6686e-01, 9.8614e-01, 7.6105e-01], requires_grad=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N = 400\n",
    "_A = torch.rand((N, N))\n",
    "A = torch.sqrt(_A.T @ _A)  # positive semidefinite and symmetric\n",
    "# A[0, :] = 0 \n",
    "b = torch.rand((N))\n",
    "A.requires_grad_(True)\n",
    "b.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6803e+00, -1.7749e+01,  2.0542e+01,  1.4515e+01, -6.2911e+00,\n",
       "        -2.0368e+01, -5.7855e+00, -2.7434e+01,  1.8531e+01,  3.1229e+01,\n",
       "        -2.8406e+01, -9.3719e+00,  3.8461e+01,  5.3640e+00, -1.8339e+01,\n",
       "         2.7294e+01, -4.3588e+00, -2.2495e+01,  2.3489e+01, -2.4251e+01,\n",
       "        -2.0043e+01,  2.5185e+01, -8.9638e-01, -2.3563e+01,  1.0631e+01,\n",
       "        -3.7528e+01,  1.0943e+01,  2.4246e+01, -2.2069e+01,  9.9090e+00,\n",
       "        -5.1356e+01, -2.8521e+01,  3.4079e+00,  1.0082e+01, -4.6613e+00,\n",
       "         5.1847e+00, -2.4961e+01, -2.3284e+01, -4.6678e+00,  1.2965e+01,\n",
       "        -8.1331e+00, -4.3669e+01, -2.1586e+01,  3.2282e+01,  3.6784e+01,\n",
       "        -2.5019e+01, -3.7939e+01, -1.3676e+01, -1.2139e+01,  4.8061e+00,\n",
       "        -3.8730e-01, -4.4804e+00, -1.5009e+01, -1.0015e+00, -7.1264e+00,\n",
       "        -1.8777e+01, -1.4093e+01,  4.0635e+00, -3.7914e+00, -1.1552e+00,\n",
       "         1.2839e+01, -7.3916e+00, -1.3182e+01,  5.0343e+00,  2.6203e+01,\n",
       "        -2.2059e+01, -1.1255e+01,  1.5756e+01, -3.0228e+01,  9.5227e+00,\n",
       "        -9.8639e+00, -2.9030e+01, -1.3916e+01,  1.5736e+01,  1.0619e+01,\n",
       "        -6.5935e+00, -1.6862e+01, -3.1647e+01,  5.3578e+01,  2.7859e-01,\n",
       "         1.0197e+01,  7.6702e+01, -5.9766e+00, -1.0487e+01, -3.8082e+01,\n",
       "        -6.0242e+00, -2.8762e+00, -7.2209e-01, -1.6399e+01,  9.1397e+00,\n",
       "        -2.3881e+00, -6.1110e+00,  2.1714e+01, -3.9992e+01, -4.4111e+00,\n",
       "        -2.5479e+01,  2.5128e+01,  2.2253e+01, -1.2154e+01, -7.7111e+00,\n",
       "        -1.3837e+01, -3.2085e+01, -1.0198e+01, -4.0536e+01, -3.5564e+00,\n",
       "         7.9151e+00, -2.7814e+01,  7.4365e+00, -8.3053e+00, -3.4894e+01,\n",
       "         2.7133e+00, -1.1982e+01, -8.1714e+00, -1.6157e+01, -7.2110e+00,\n",
       "        -1.5509e+01, -3.4727e+01, -3.6692e+01, -4.2849e+01,  4.9065e+00,\n",
       "        -3.0848e+01,  1.8657e+01,  4.1000e+01,  1.5976e+01,  5.9758e+00,\n",
       "         2.3401e+01, -5.1506e+00, -2.4141e+01,  4.1509e+00,  2.7883e+01,\n",
       "        -4.2092e+01,  4.3426e+01, -4.7909e+00, -9.4360e+00, -3.2591e+01,\n",
       "        -1.4693e+01, -2.5293e+01,  3.4832e+00, -1.3588e+01,  2.4761e+00,\n",
       "         2.3126e+01,  2.1847e+00,  1.4453e+01,  1.1212e+01, -1.9617e+01,\n",
       "         1.3232e+01,  4.9403e+01,  1.0161e+01, -1.5508e+01, -3.7976e+01,\n",
       "        -1.1141e+01,  1.0512e+00,  3.8922e+01,  2.9562e+01,  1.3190e+01,\n",
       "        -3.0114e+00, -2.0063e+00, -5.3014e+01, -1.9919e+01,  3.0790e+01,\n",
       "        -3.0273e+01,  2.7642e+01, -9.5745e+00, -9.4658e+00, -5.6371e+00,\n",
       "         4.1120e+00,  1.7228e+01, -1.1439e+01, -4.9399e+00,  1.9281e+01,\n",
       "         2.8399e+01,  1.3207e+00, -1.0945e+01,  1.1429e+01, -1.4272e+01,\n",
       "        -7.6574e+00,  9.9097e+00,  1.0286e+01,  2.5237e+01, -8.9142e+00,\n",
       "        -2.1277e+01,  5.8835e+01,  3.2938e+01,  1.7192e+01,  6.1999e+00,\n",
       "         3.2302e+00, -1.2363e+00,  1.4332e+01, -2.7539e+01,  1.1230e+01,\n",
       "         5.2769e+00,  4.5142e+00, -1.0341e+01, -4.3372e+00,  4.7358e+00,\n",
       "        -5.5635e+00,  6.4039e-01,  1.7068e+01,  6.5048e+00,  2.1915e+01,\n",
       "         2.3296e+01,  1.6588e+01, -2.2996e+01,  3.1708e+01, -1.2142e+01,\n",
       "        -2.4953e+01, -8.2438e+00,  2.1284e+01,  2.7068e+01,  9.3132e-01,\n",
       "         1.3773e+01, -3.0121e+01,  1.9845e+01,  3.6723e+01, -3.6217e+00,\n",
       "        -5.4012e+00, -8.8321e+00,  5.0957e+00, -1.7298e+01, -4.5019e+01,\n",
       "        -7.3576e+00,  1.5507e+00, -1.0522e+01,  1.4178e+01,  2.5656e+01,\n",
       "        -3.0219e+01, -3.7695e+00, -2.3178e+01,  2.3365e+01, -7.0634e+00,\n",
       "        -1.7122e+01,  4.0595e+01,  1.0034e+01, -2.1539e+01,  2.8186e+00,\n",
       "        -3.3792e+01, -5.0864e+00,  1.0342e+01, -6.3387e+01,  1.1699e-01,\n",
       "         7.3499e+01, -1.8750e+01,  4.6540e+00,  3.4451e+01, -5.3339e+01,\n",
       "         3.4351e+01,  1.6656e+01, -2.1380e+01, -2.9672e+01,  5.1120e+01,\n",
       "         9.2953e+00, -1.3744e+00, -2.1649e+01, -1.2453e+01, -1.6537e+01,\n",
       "         2.0031e+00, -3.0701e+01,  2.2965e+01,  3.4765e-01,  3.2745e+01,\n",
       "        -2.4807e+01, -9.2444e-01,  4.0888e+01,  1.3016e+01,  1.1832e+01,\n",
       "         5.3082e+00,  2.4033e+01, -1.5642e+00,  2.0981e+01, -2.8422e+01,\n",
       "         1.0616e+01,  1.9998e+01,  3.2278e+01, -1.2659e+01, -1.0376e+01,\n",
       "        -1.0023e+00, -2.0077e+00, -1.0949e+01,  3.2894e+01,  9.1657e+00,\n",
       "         2.5691e+01, -2.4601e+01,  3.0608e+01,  1.3107e+01,  3.5888e+01,\n",
       "         9.6784e+00, -8.2706e-01,  2.3482e+01,  3.6652e-01, -4.9809e+00,\n",
       "        -2.0673e+01, -9.3226e+00, -6.5482e+00,  1.3795e+01,  6.7936e+00,\n",
       "        -1.9756e+01,  4.4575e-01, -1.9808e+01, -1.2020e+01,  5.0522e+00,\n",
       "        -1.1411e+01,  3.8206e+01, -1.3455e+01, -2.2111e+01,  8.6083e+00,\n",
       "         1.0799e+01,  7.0021e+00,  2.3623e+01, -1.4927e+00, -3.5334e+01,\n",
       "         1.5798e+00, -3.1725e+01, -3.9600e+00, -3.6424e+01,  1.2096e+01,\n",
       "        -3.6279e+00, -1.9674e+01,  1.6581e+01, -1.0429e+00,  2.3491e+01,\n",
       "         1.2138e+01,  6.6128e+00, -1.3652e+01,  1.2948e+01,  6.9044e-03,\n",
       "        -4.0450e+01,  4.6204e+01, -2.9025e+00, -3.3001e+00, -1.6284e+01,\n",
       "         2.3192e+01, -1.0036e+01, -1.2063e+01, -2.9823e+01, -1.6427e+01,\n",
       "         4.0041e+00,  4.9512e+01,  2.5738e+01,  1.1329e+01,  1.8547e+01,\n",
       "        -2.2157e+01, -2.7909e+00,  4.8985e+01,  1.4354e+01,  1.1204e+01,\n",
       "        -8.5651e+00,  7.3920e+00,  1.8703e+01, -2.2761e+01, -2.6752e+01,\n",
       "         1.5871e+01, -8.9522e+00, -3.5240e+01,  1.4035e+01,  1.9271e+01,\n",
       "        -3.4270e+01,  1.4973e+01, -1.1301e+01,  2.0182e+01, -8.8067e+00,\n",
       "        -2.4972e+01, -2.2567e+01, -2.2952e+01, -6.3421e+00,  2.3966e+00,\n",
       "        -1.3093e+01,  2.2104e+00, -1.4932e-02,  2.4584e+01,  3.8248e+01,\n",
       "         4.2787e+00,  1.2142e+00, -7.2326e+00,  2.5943e+01, -2.1041e+01,\n",
       "        -1.0706e+00, -7.2957e+00,  2.4068e+01, -1.2472e-01, -2.2489e+01,\n",
       "         1.8148e+01, -4.2774e-01, -1.7973e+01,  1.8526e+00, -3.8164e+00,\n",
       "         2.2224e+01,  1.1029e+01,  3.8222e+01,  2.2744e+01, -9.5820e+00,\n",
       "         2.4595e+01,  6.6794e+00,  1.2424e+01,  3.3820e+01,  4.6769e+01,\n",
       "         3.6310e+01,  3.5788e+01,  6.2698e+00, -9.6078e+00, -3.0103e+01],\n",
       "       grad_fn=<LinalgSolveExBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.linalg.lstsq(A, b).solution\n",
    "# e = torch.randn_like(A) * 1e-08\n",
    "x1 = torch.linalg.solve(A, b)\n",
    "# x2 = torch.linalg.solve(A + e, b)\n",
    "# (x1-x2).sum()\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = torch.randn_like(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.3757, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.linalg.lstsq(A, b).solution\n",
    "_e = e * 1e-10\n",
    "_A = A.clone()\n",
    "_A[:, 0] = 0\n",
    "x2 = torch.linalg.solve(_A + _e, b)\n",
    "\n",
    "# (x1-x2).sum()\n",
    "x2[1:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  6.9797,   4.3023,  -4.9190,  -2.7390,  19.1866, -14.5113,  -7.5664,\n",
       "         -1.8227,   3.5572,   6.9848,  -1.2781,  -7.2631,  -5.4850,  -2.9036,\n",
       "          7.6349,   3.5105,   7.4000,  -5.4340,  14.6066,  14.2608,  12.8030,\n",
       "        -15.5773,   4.3446,  -3.5358,   5.5228,   5.4398,   0.5586,  -9.9863,\n",
       "         -1.8316,  -3.9367, -12.0415,   4.6134,   8.7477, -21.5122,  -8.7369,\n",
       "          3.2658,  -0.7616,   9.4308,  -4.9033,   4.7647,   8.1350,  -3.6620,\n",
       "        -10.6795,   6.4890,   1.2426,  18.7446, -11.4028,   9.3061,  -4.0421,\n",
       "         -4.8910,  -2.8756,   7.8130,  13.3264, -17.9365,   9.4609,  -4.1456,\n",
       "          4.3616,   3.5165,  -3.3228,  -6.0399,   4.0355,  -6.9790,  16.0464,\n",
       "        -20.3430, -15.8175,   2.9459,  -1.0137,  11.1447,  10.7613,  -3.5677,\n",
       "         -1.0943,   8.6522,   6.9326,  -4.5626,   1.2370,   5.1883,  -7.3631,\n",
       "         -7.0756,   4.2590,   3.6379,  15.3663,   6.4727, -11.9880,  -1.3369,\n",
       "         -9.7993,  -2.9827,  -1.7972,   4.1405,  -7.2526,   9.6171,   4.0853,\n",
       "         -3.2639,   5.4806,   0.5579,  18.5286,  -3.5568,  13.5908,   4.7582,\n",
       "         -5.5176,   0.4494,  -3.8597,   0.3495,   7.0871,   0.4089,  -2.8695,\n",
       "         -4.4643,  -3.0898,  -0.7245,   4.8890,  -2.5742,  -6.9160,  11.8527,\n",
       "        -22.7231,  -8.9501, -10.2512,  -2.3676,  -1.1060, -13.1295,  -0.7399,\n",
       "         -0.2144,  14.1155,  -3.9408,   9.4880,  11.8858, -18.4913,   2.4616,\n",
       "         -4.4079,   0.9690,  -3.8310,  12.3652,  16.2925,  -7.3743, -11.3490,\n",
       "          4.9117,  -1.7923,   1.5632,  21.4739,   5.2220,  -3.7104,   2.5559,\n",
       "         -9.9796,   7.0898,  10.7431,   3.3796,  -0.8528,  -0.4276,  -0.2573,\n",
       "          5.2509, -10.3560,  -2.8235,   0.5373,   3.7517,   6.1562,   1.2801,\n",
       "         11.4617,   4.0393,  -4.1346,   2.0743,   2.3361, -14.3013,  -6.2845,\n",
       "          7.9428,  11.9284,  -2.0097,  -0.8552,  -4.9949, -11.6637,  -6.7515,\n",
       "         -1.6035,   4.3863,  -4.9335,  -2.6410,  -6.1428,  10.7101,  -0.9520,\n",
       "          7.1581,   1.4057,  -2.6919,  -0.9443,   2.0326,  -8.9724,  -7.6044,\n",
       "         -6.4182,  -0.7658,   7.8228,   3.1572,  -7.1047,   1.0050,  -0.8602,\n",
       "          6.0775,   1.5166, -10.2791,   3.4010,   1.9790,   7.8921,   4.2621,\n",
       "          7.0970,  -6.5072, -17.5700,   7.3409,  10.5357,  -4.8780,   0.9708,\n",
       "          0.2758,   4.4003,   1.9879,   1.0060,   1.8568, -13.6060,  11.4716,\n",
       "          6.3542,  -7.1080, -11.4101,  13.3984,  -6.5273,  -0.6408,   7.3111,\n",
       "        -11.6682,  -0.9249,   0.3130,  -4.4424,  -1.6230, -14.1048,  -4.9072,\n",
       "         -5.6595, -11.0686,   2.1008,  10.3164,   2.5338, -22.0807,  -1.0493,\n",
       "         -6.4769,  -1.5247,   8.6281,   7.7882,   0.9924, -11.5361,   7.9926,\n",
       "        -15.0863, -12.7263,   5.2278, -12.4320, -12.7291,   5.9660,   0.5771,\n",
       "         -5.9678,   2.0537,   9.1108,  -2.1791,   6.7075,  -2.1848,   9.1410,\n",
       "          0.4743,   2.6293,  -4.2876,  -5.5302,  -6.4279,  -4.4237,  -2.4417,\n",
       "          5.3825, -13.8634,  -3.3574,  10.4169,  13.5859,  -2.7429,   1.3687,\n",
       "          1.7679,   2.3212,   1.6559,  14.3278, -14.0093,   1.9819,   9.0446,\n",
       "          3.3842,   1.3069, -15.0092,  -1.2731,  -0.4391,  -1.3338,  -3.4013,\n",
       "         -1.6656,  -9.3665,  15.8259,   1.6970,   2.0128,   0.6180,   4.4029,\n",
       "         14.2059,   0.4388,  -7.0201,  11.2967,  10.1083,   4.6235,  -8.3360,\n",
       "          5.4401,   2.7205,  -1.7403, -12.3828,   3.2136,   4.9189,  -6.5261,\n",
       "         -7.2193,  10.8143, -10.3737,   2.5583,   4.6426,  -1.1022,  -6.0773,\n",
       "          7.9604,  13.4570,   1.8925,  -4.2575, -15.4559,   6.7535,   2.1441,\n",
       "         20.2806,  -1.4640,  10.6853,  -7.6194,   3.7369,  -3.7113, -11.5180,\n",
       "         -9.7337,   7.1018,   6.8409,  -7.1525,  -6.5662,  -4.0636,   5.2557,\n",
       "          2.6704,  14.1937, -14.0147,  -4.6701,   4.1494,   6.7680,  -7.4357,\n",
       "        -10.8317, -14.0977,   7.0309,  -4.2752,  -2.0222,   0.0605,  -4.9919,\n",
       "         -2.5835,   2.5672,   2.6166,   1.1790,  -5.8822,   7.1799, -15.3968,\n",
       "         -4.2344,   6.2703,  15.3108,  10.1370,  -9.1172,  -0.3383,   0.8286,\n",
       "          3.1487,  -9.4862,  -4.1386,  -3.3967,   2.1056,   5.0403,  -6.0818,\n",
       "          4.3631, -13.0567,   4.1453,   7.9196,   6.6029,   4.2646,   2.0967,\n",
       "         -5.9936,   6.3180,  -0.4209,  -8.0518,  -2.9138,   5.7355,  -6.4426,\n",
       "          0.2758,   8.1443, -13.4703,  -2.1619,  12.0914,  -3.3613,  11.5405,\n",
       "        -17.6106,  -1.0909,  -6.8906,   1.7592,   5.2988,   6.2665,  -0.7212,\n",
       "          8.2019,   6.0927, -10.2003,  11.7954,   5.4122, -11.1247,  11.8646,\n",
       "         -8.5400], grad_fn=<LinalgSolveExBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [10.0456, 11.7331,  9.9941,  ..., 10.1602,  9.8357, 10.2872],\n",
       "        [ 9.9623,  9.9941, 11.6465,  ..., 10.1315,  9.8894, 10.0983],\n",
       "        ...,\n",
       "        [ 9.9489, 10.1602, 10.1315,  ..., 11.6621,  9.9030, 10.3025],\n",
       "        [ 9.7626,  9.8357,  9.8894,  ...,  9.9030, 11.2234, 10.0703],\n",
       "        [10.2303, 10.2872, 10.0983,  ..., 10.3025, 10.0703, 11.7982]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "t = 100\n",
    "A = A.to(\"cuda\")\n",
    "b = b.to(\"cuda\")\n",
    "\n",
    "\n",
    "def foo():\n",
    "    conjugate_gradient(A, b, verbose=True, max_iter=5)\n",
    "    # torch.linalg.solve(A, b)\n",
    "\n",
    "\n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=False\n",
    ") as prof:\n",
    "    # foo()\n",
    "    timeit.timeit(foo, number=t) / t\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lib.optimizer.pcg import conjugate_gradient\n",
    "import torch\n",
    "import timeit\n",
    "\n",
    "\n",
    "def conjugate_gradient(\n",
    "    A: torch.Tensor,  # dim (N,N)\n",
    "    b: torch.Tensor,  # dim (N)\n",
    "    x0: torch.Tensor | None = None,  # dim (N)\n",
    "    max_iter: int = 20,\n",
    "    verbose: bool = False,\n",
    "    tol: float = 1e-08,\n",
    "):\n",
    "    k = 0\n",
    "    converged = False\n",
    "\n",
    "    xk = torch.zeros_like(b) if x0 is None else x0  # (N)\n",
    "    rk = b - A @ xk  # column vector (N)\n",
    "    pk = rk\n",
    "\n",
    "    if torch.norm(rk) < tol:\n",
    "        converged = True\n",
    "\n",
    "    while k < max_iter and not converged:\n",
    "\n",
    "        # compute step size\n",
    "        ak = (rk[None] @ rk) / (pk[None] @ A @ pk)\n",
    "        # update unknowns\n",
    "        xk_1 = xk + ak * pk\n",
    "        # compute residuals\n",
    "        rk_1 = rk - ak * A @ pk\n",
    "        # compute new pk\n",
    "        bk = (rk_1[None] @ rk_1) / (rk[None] @ rk)\n",
    "        pk_1 = rk_1 + bk * pk\n",
    "        # update the next stateprint\n",
    "        xk = xk_1\n",
    "        pk = pk_1\n",
    "        rk = rk_1\n",
    "\n",
    "        k += 1\n",
    "        if torch.norm(rk) < tol:\n",
    "            converged = True\n",
    "\n",
    "    return xk\n",
    "\n",
    "\n",
    "t = 1000\n",
    "N = 400\n",
    "_A = torch.rand((N, N))\n",
    "A = torch.sqrt(_A.T @ _A).to(\"cuda\")  # positive semidefinite and symmetric\n",
    "b = torch.rand((N)).to(\"cuda\")\n",
    "\n",
    "\n",
    "def foo():\n",
    "    # conjugate_gradient(A, b, max_iter=5)\n",
    "    torch.linalg.solve(A, b)\n",
    "\n",
    "\n",
    "print((timeit.timeit(foo, number=t) / t) * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LU, pivots = torch.linalg.lu_factor(A)\n",
    "X = torch.linalg.lu_solve(LU, pivots, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = 1000\n",
    "N = 1000\n",
    "_A = torch.rand((N, N))\n",
    "A = torch.sqrt(_A.T @ _A).to(\"cuda\")  # positive semidefinite and symmetric\n",
    "b = torch.rand((N)).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_A = A.clone()\n",
    "_b = b.clone()\n",
    "_A.requires_grad = True\n",
    "_b.requires_grad = True\n",
    "x = conjugate_gradient(_A, _b, max_iter=5)\n",
    "x_gt = torch.linalg.solve(A, b)\n",
    "(x - x_gt).norm().backward()\n",
    "_A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import ConjugateGradient\n",
    "\n",
    "_A = A.clone()\n",
    "_b = b.clone()\n",
    "_A.requires_grad = True\n",
    "_b.requires_grad = True\n",
    "x = ConjugateGradient.apply(_A, _b)\n",
    "x_gt = torch.linalg.solve(A, b)\n",
    "(x - x_gt).norm().backward()\n",
    "_A.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for i in range(9):\n",
    "    out = torch.load(\n",
    "        f\"/home/borth/GuidedResearch/logs/2024-07-14_20-16-20_pcg_sampling/linsys/000000{i}.pt\"\n",
    "    )\n",
    "    A = out[\"A\"]\n",
    "    b = out[\"b\"]\n",
    "    x = out[\"x\"]\n",
    "\n",
    "    print(torch.linalg.solve(A, b).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import JaccobiConditionNet\n",
    "\n",
    "j = JaccobiConditionNet()\n",
    "M = j(A)\n",
    "torch.linalg.cond(M @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from lib.optimizer.pcg import PCGSolver\n",
    "from lib.optimizer.pcg import conjugate_gradient\n",
    "from torch.optim import Adam, SGD\n",
    "import torch\n",
    "\n",
    "# define the matrixes\n",
    "\n",
    "\n",
    "def generate_data(N, x_eps=1e-02, a_eps=1e-01):\n",
    "    E = torch.rand((N, N)) * a_eps\n",
    "    A = torch.eye(N) + (E.T @ E)\n",
    "    x_gt = torch.rand((N)) * x_eps\n",
    "    b = A @ x_gt\n",
    "    A.requires_grad = True\n",
    "    b.requires_grad = True\n",
    "    return A, b, x_gt\n",
    "\n",
    "\n",
    "def eval_loss(x_pcg, x_gt, verbose=True):\n",
    "    l_pcg = (x_pcg - x_gt).norm()\n",
    "    if verbose:\n",
    "        print(\"Loss:\", l_pcg.item())\n",
    "    return l_pcg\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "N = 6\n",
    "# A, b, x_gt = generate_data(N)\n",
    "max_steps = 1000\n",
    "lr = 1e-03\n",
    "max_iter = 1\n",
    "verbose = True\n",
    "tol = 1e-08\n",
    "\n",
    "out = torch.load(\n",
    "    \"/home/borth/GuidedResearch/logs/2024-07-04_12-17-20_pcg_sampling/linsys/0000000.pt\"\n",
    ")\n",
    "A = out[\"A\"]\n",
    "b = out[\"b\"]\n",
    "# A.requires_grad = True\n",
    "# b.requires_grad = True\n",
    "x_gt = torch.linalg.solve(A, b)\n",
    "\n",
    "pcg = PCGSolver(\n",
    "    dim=N,\n",
    "    max_iter=max_iter,\n",
    "    verbose=verbose,\n",
    "    tol=tol,\n",
    "    mode=\"diagonal_offset\",\n",
    "    gradients=\"backprop\",\n",
    ")\n",
    "# optimizer = SGD(pcg.parameters(), lr=lr, momentum=0.90)\n",
    "optimizer = Adam(pcg.parameters(), lr=lr)\n",
    "\n",
    "init_loss = None\n",
    "for step in (pbar := tqdm(range(max_steps), total=max_steps)):\n",
    "    pbar.set_description(f\"{step}/{max_steps}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    A, b, x_gt = generate_data(N)\n",
    "    M = pcg.condition_net(A)\n",
    "    C_m = torch.linalg.cond(M @ A).item()\n",
    "    C_a = torch.linalg.cond(A).item()\n",
    "    x = pcg(A, b)\n",
    "    loss = eval_loss(x, x_gt, verbose=False)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if init_loss is None:\n",
    "        init_loss = loss.item()\n",
    "\n",
    "    pbar.set_postfix(\n",
    "        {\"init_loss\": init_loss, \"loss\": loss.item(), \"C_m\": C_m, \"C_a\": C_a}\n",
    "    )\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.data.datamodule import PCGDataModule\n",
    "from lib.data.dataset import PCGDataset\n",
    "\n",
    "path = \"/home/borth/GuidedResearch/data/linsys_pose\"\n",
    "dataset = PCGDataset(data_dir=path)\n",
    "out = torch.load(\"/home/borth/GuidedResearch/data/linsys_pose/0000000.pt\")\n",
    "A = out[\"A\"]\n",
    "b = out[\"b\"]\n",
    "x_gt = torch.linalg.solve(A, b)\n",
    "x_gt.norm(), x_gt, x_gt[:3].norm(), x_gt[3:6].norm(), x_gt[:3].norm() / x_gt[3:6].norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.norm(), A.inverse().norm(), torch.linalg.cond(A) / A.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gt = out[\"x_gt\"]\n",
    "eps = torch.rand_like(x_gt)\n",
    "eps /= torch.linalg.vector_norm(eps)\n",
    "eps *= 1e-02\n",
    "l1_solution = torch.abs(eps).mean()\n",
    "residual = torch.linalg.vector_norm(A @ (x_gt + eps) - b)\n",
    "# residual = torch.linalg.vector_norm(A @ x_gt - b)\n",
    "l1_solution, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import (\n",
    "    JaccobiConditionNet,\n",
    "    PCGSolver,\n",
    "    IdentityConditionNet,\n",
    "    ConditionNet,\n",
    "    DenseConditionNet,\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class JaccobiConditionNet1(ConditionNet):\n",
    "    name: str = \"JaccobiConditionNet\"\n",
    "\n",
    "    def forward(self, A: torch.Tensor):\n",
    "        diagonals = A.diagonal(dim1=-2, dim2=-1)\n",
    "        # diagonals = torch.nn.functional.softmax(1 / diagonals, dim=-1)\n",
    "        diagonals = 1 / diagonals\n",
    "        return torch.diag_embed(diagonals)\n",
    "\n",
    "\n",
    "condition_net = partial(DenseConditionNet, unknowns=6)\n",
    "solver = PCGSolver(check_convergence=True, condition_net=condition_net)\n",
    "x, info = solver(A, b)\n",
    "x, info[\"k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.inverse().diag() / A.inverse().diag().sum(), (1 / A.diag()) / (1 / A.diag()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import (\n",
    "    JaccobiConditionNet,\n",
    "    PCGSolver,\n",
    "    IdentityConditionNet,\n",
    "    ConditionNet,\n",
    "    DenseConditionNet,\n",
    ")\n",
    "\n",
    "cond = DenseConditionNet(unknowns=6)\n",
    "cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def foo0():\n",
    "#     batched_conjugate_gradient(batch[\"A\"], batch[\"b\"], max_iter=max_iter)\n",
    "\n",
    "\n",
    "# def foo1():\n",
    "#     for i in range(batch_size):\n",
    "#         conjugate_gradient(batch[\"A\"][i], batch[\"b\"][i], max_iter=max_iter)\n",
    "\n",
    "\n",
    "# print(timeit.timeit(foo0, number=n) / n)\n",
    "# print(timeit.timeit(foo1, number=n) / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from lib.optimizer.pcg import JaccobiConditionNet, preconditioned_conjugate_gradient\n",
    "import timeit\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from lib.data.dataset import PCGDataset\n",
    "from lib.optimizer.pcg import PCGSolver\n",
    "\n",
    "ckpt_path = (\n",
    "    \"/home/borth/GuidedResearch/logs/2024-07-11_12-23-43_pcg/checkpoints/last.ckpt\"\n",
    ")\n",
    "solver = PCGSolver.load_from_checkpoint(ckpt_path).to(\"cpu\")\n",
    "\n",
    "batch_size = 2\n",
    "path = \"/home/borth/GuidedResearch/data/linsys_pose\"\n",
    "dataset = PCGDataset(data_dir=path, split=\"test\")\n",
    "loader = DataLoader(dataset, batch_size=batch_size)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "jaccobi = JaccobiConditionNet()\n",
    "\n",
    "A = batch[\"A\"][0]\n",
    "b = batch[\"b\"][0]\n",
    "\n",
    "M = jaccobi(A)\n",
    "M_A = torch.matmul(M, A)  # (B, N, N) or (N, N)\n",
    "M_b = torch.matmul(M, b.unsqueeze(-1)).squeeze(-1)  # (B, N, N) or (N, N)\n",
    "x, info_jaccobi = preconditioned_conjugate_gradient(\n",
    "    A=A,\n",
    "    b=b,\n",
    "    M=M,\n",
    "    max_iter=20,\n",
    "    rel_tol=1e-08,\n",
    "    verbose=True,\n",
    "    check_convergence=False,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    M = solver.condition_net(A)\n",
    "M_A = torch.matmul(M, A)  # (B, N, N) or (N, N)\n",
    "M_b = torch.matmul(M, b.unsqueeze(-1)).squeeze(-1)  # (B, N, N) or (N, N)\n",
    "x, info_pcg = preconditioned_conjugate_gradient(\n",
    "    A=A,\n",
    "    b=b,\n",
    "    M=M,\n",
    "    max_iter=20,\n",
    "    rel_tol=1e-08,\n",
    "    verbose=True,\n",
    "    check_convergence=False,\n",
    ")\n",
    "\n",
    "x, info_wo = preconditioned_conjugate_gradient(\n",
    "    A=A,\n",
    "    b=b,\n",
    "    max_iter=20,\n",
    "    rel_tol=1e-08,\n",
    "    verbose=True,\n",
    "    check_convergence=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import Trainer\n",
    "\n",
    "trainer = Trainer()\n",
    "solver.max_iter = 20\n",
    "out = trainer.predict(solver, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "stats = defaultdict(list)\n",
    "for batch in out:\n",
    "    for key, value in batch.items():\n",
    "        stats[key].append(value)\n",
    "for key, value in stats.items():\n",
    "    stats[key] = torch.stack(value, dim=-1)\n",
    "cond = stats[\"cond\"].mean()\n",
    "iters = stats[\"relres_norms\"].size(0)\n",
    "relres_norms = stats[\"relres_norms\"].view(iters, -1).mean(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relres_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the relative residual norms\n",
    "relres = [v.detach() for v in info_pcg[\"relres_norms\"]]\n",
    "relres_jaccobi = info_jaccobi[\"relres_norms\"]\n",
    "relres_wo = info_wo[\"relres_norms\"]\n",
    "\n",
    "# Create a range for the x-axis based on the length of the data\n",
    "iterations_pcg = range(len(relres_pcg))\n",
    "iterations_jaccobi = range(len(relres_jaccobi))\n",
    "iterations_wo = range(len(relres_wo))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations_pcg, relres_pcg, label=\"PCG\", marker=\"o\")\n",
    "plt.plot(iterations_jaccobi, relres_jaccobi, label=\"Jaccobi\", marker=\"o\")\n",
    "plt.plot(iterations_wo, relres_wo, label=\"Without Optimization\", marker=\"s\")\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title(\"Relative Residual Norms Comparison\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Relative Residual Norms\")\n",
    "\n",
    "# Set y-axis to log scale\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "# Set y-axis limit to start from 10^-8\n",
    "plt.ylim(bottom=1e-7)\n",
    "\n",
    "# Add a horizontal red line at 10^-6\n",
    "plt.axhline(y=1e-6, color=\"red\", linestyle=\"--\", label=\"Convergence at $10^{-6}$\")\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(batch[\"A\"][0], batch[\"b\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import conjugate_gradient, log\n",
    "import logging\n",
    "\n",
    "log.setLevel(logging.INFO)\n",
    "conjugate_gradient(batch[\"A\"][0], batch[\"b\"][0], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag_embed(1 / batch[\"A\"][0].diagonal(dim1=-2, dim2=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = batch[\"A\"]\n",
    "ones = torch.ones((A.shape[0], A.shape[1]), device=A.device)\n",
    "torch.diag_embed(ones).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.solve(batch[\"A\"], batch[\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones_like(A)[0]\n",
    "x.expand(A[0].shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.bmm(A[None], b[None, ..., None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "\n",
    "def foo():\n",
    "    b[None] @ b\n",
    "\n",
    "\n",
    "n = 100000\n",
    "timeit.timeit(foo, number=n) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    if torch.norm(b) > 0.1:\n",
    "        pass\n",
    "\n",
    "\n",
    "n = 100000\n",
    "timeit.timeit(foo, number=n) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcg = PCGSolver(dim=N, max_iter=max_iter, verbose=verbose, tol=tol, mode=\"dense\")\n",
    "A, b, x_gt = generate_data(N)\n",
    "M = pcg.condition_net(A)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linalg.cond(pcg.condition_net(A) @ A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import preconditioned_conjugate_gradient\n",
    "\n",
    "A, b, x_gt = generate_data(N)\n",
    "x_cg = preconditioned_conjugate_gradient(A, b, max_iter=2)\n",
    "pcg.max_iter = 2\n",
    "x = pcg(A, b)\n",
    "loss = eval_loss(x, x_gt, verbose=True)\n",
    "loss = eval_loss(x_cg, x_gt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(p.size() for p in pcg.condition_net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.optimizer.pcg import conjugate_gradient\n",
    "\n",
    "torch.manual_seed(42)\n",
    "N = 100\n",
    "E = torch.rand((N, N)) * 1e-02\n",
    "A = torch.eye(N) + (E.T @ E)\n",
    "x = torch.rand((N)) * 1e-02\n",
    "b = A @ x\n",
    "\n",
    "x_pcg = conjugate_gradient(A, b, max_iter=1)\n",
    "r_pcg = (A @ x_pcg - b).mean()\n",
    "r_pcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "tri_N = ((N * N - N) // 2) + N\n",
    "L = torch.zeros((N, N))\n",
    "tril_indices = torch.tril_indices(row=N, col=N, offset=0)\n",
    "L[tril_indices[0], tril_indices[1]] = torch.rand(tri_N)\n",
    "L[4, 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "def cg_batch(\n",
    "    A_bmm, B, M_bmm=None, X0=None, rtol=1e-3, atol=0.0, maxiter=None, verbose=False\n",
    "):\n",
    "    \"\"\"Solves a batch of PD matrix linear systems using the preconditioned CG algorithm.\n",
    "\n",
    "    This function solves a batch of matrix linear systems of the form\n",
    "\n",
    "        A_i X_i = B_i,  i=1,...,K,\n",
    "\n",
    "    where A_i is a n x n positive definite matrix and B_i is a n x m matrix,\n",
    "    and X_i is the n x m matrix representing the solution for the ith system.\n",
    "\n",
    "    Args:\n",
    "        A_bmm: A callable that performs a batch matrix multiply of A and a K x n x m matrix.\n",
    "        B: A K x n x m matrix representing the right hand sides.\n",
    "        M_bmm: (optional) A callable that performs a batch matrix multiply of the preconditioning\n",
    "            matrices M and a K x n x m matrix. (default=identity matrix)\n",
    "        X0: (optional) Initial guess for X, defaults to M_bmm(B). (default=None)\n",
    "        rtol: (optional) Relative tolerance for norm of residual. (default=1e-3)\n",
    "        atol: (optional) Absolute tolerance for norm of residual. (default=0)\n",
    "        maxiter: (optional) Maximum number of iterations to perform. (default=5*n)\n",
    "        verbose: (optional) Whether or not to print status messages. (default=False)\n",
    "    \"\"\"\n",
    "    K, n, m = B.shape\n",
    "\n",
    "    if M_bmm is None:\n",
    "        M_bmm = lambda x: x\n",
    "    if X0 is None:\n",
    "        X0 = M_bmm(B)\n",
    "    if maxiter is None:\n",
    "        maxiter = 5 * n\n",
    "\n",
    "    assert B.shape == (K, n, m)\n",
    "    assert X0.shape == (K, n, m)\n",
    "    assert rtol > 0 or atol > 0\n",
    "    assert isinstance(maxiter, int)\n",
    "\n",
    "    X_k = X0\n",
    "    R_k = B - A_bmm(X_k)\n",
    "    Z_k = M_bmm(R_k)\n",
    "\n",
    "    P_k = torch.zeros_like(Z_k)\n",
    "\n",
    "    P_k1 = P_k\n",
    "    R_k1 = R_k\n",
    "    R_k2 = R_k\n",
    "    X_k1 = X0\n",
    "    Z_k1 = Z_k\n",
    "    Z_k2 = Z_k\n",
    "\n",
    "    B_norm = torch.norm(B, dim=1)\n",
    "    stopping_matrix = torch.max(rtol * B_norm, atol * torch.ones_like(B_norm))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"%03s | %010s %06s\" % (\"it\", \"dist\", \"it/s\"))\n",
    "\n",
    "    optimal = False\n",
    "    start = time.perf_counter()\n",
    "    for k in range(1, maxiter + 1):\n",
    "        start_iter = time.perf_counter()\n",
    "        Z_k = M_bmm(R_k)\n",
    "\n",
    "        if k == 1:\n",
    "            P_k = Z_k\n",
    "            R_k1 = R_k\n",
    "            X_k1 = X_k\n",
    "            Z_k1 = Z_k\n",
    "        else:\n",
    "            R_k2 = R_k1\n",
    "            Z_k2 = Z_k1\n",
    "            P_k1 = P_k\n",
    "            R_k1 = R_k\n",
    "            Z_k1 = Z_k\n",
    "            X_k1 = X_k\n",
    "            denominator = (R_k2 * Z_k2).sum(1)\n",
    "            denominator[denominator == 0] = 1e-8\n",
    "            beta = (R_k1 * Z_k1).sum(1) / denominator\n",
    "            P_k = Z_k1 + beta.unsqueeze(1) * P_k1\n",
    "\n",
    "        denominator = (P_k * A_bmm(P_k)).sum(1)\n",
    "        denominator[denominator == 0] = 1e-8\n",
    "        alpha = (R_k1 * Z_k1).sum(1) / denominator\n",
    "        X_k = X_k1 + alpha.unsqueeze(1) * P_k\n",
    "        R_k = R_k1 - alpha.unsqueeze(1) * A_bmm(P_k)\n",
    "        end_iter = time.perf_counter()\n",
    "\n",
    "        residual_norm = torch.norm(A_bmm(X_k) - B, dim=1)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"%03d | %8.4e %4.2f\"\n",
    "                % (\n",
    "                    k,\n",
    "                    torch.max(residual_norm - stopping_matrix),\n",
    "                    1.0 / (end_iter - start_iter),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if (residual_norm <= stopping_matrix).all():\n",
    "            optimal = True\n",
    "            break\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    if verbose:\n",
    "        if optimal:\n",
    "            print(\n",
    "                \"Terminated in %d steps (reached maxiter). Took %.3f ms.\"\n",
    "                % (k, (end - start) * 1000)\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                \"Terminated in %d steps (optimal). Took %.3f ms.\"\n",
    "                % (k, (end - start) * 1000)\n",
    "            )\n",
    "\n",
    "    info = {\"niter\": k, \"optimal\": optimal}\n",
    "\n",
    "    return X_k, info\n",
    "\n",
    "\n",
    "A = torch.tensor([[4.0, 1], [1, 3]])[None, ...]\n",
    "\n",
    "\n",
    "def A_bmm(X):\n",
    "    Y = [(A[i] @ X[i]).unsqueeze(0) for i in range(1)]\n",
    "    return torch.cat(Y, dim=0)\n",
    "\n",
    "\n",
    "b = torch.tensor([1.0, 2])[None, ...][..., None]\n",
    "cg_batch(A_bmm=A_bmm, B=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "_A = torch.rand((N, N))\n",
    "z = torch.zeros_like(_A)\n",
    "for idx in range(70):\n",
    "    i = idx * 10\n",
    "    j = (idx + 1) * 10\n",
    "    z[i:j, i:j] = 1.0\n",
    "_A *= z\n",
    "A = torch.sqrt(_A.T @ _A)  # positive and symetric\n",
    "# A = torch.diag(torch.diag(A))\n",
    "b = torch.rand((N)) + 2\n",
    "\n",
    "\n",
    "def A_bmm(X):\n",
    "    Y = [(A[None, ...][i] @ X[i]).unsqueeze(0) for i in range(1)]\n",
    "    return torch.cat(Y, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "s = time.time()\n",
    "out = cg_batch(A_bmm=A_bmm, B=b[None, ...][..., None], maxiter=10)\n",
    "print(time.time() - s)\n",
    "x_pcg = out[0].squeeze()\n",
    "s = time.time()\n",
    "x_optim = torch.linalg.solve(A, b)\n",
    "print(time.time() - s)\n",
    "pcg_norm = ((A @ x_pcg) - b).norm()\n",
    "optim_norm = ((A @ x_optim) - b).norm()\n",
    "pcg_norm, optim_norm, b.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_size: int = 1,\n",
    "        stride: int = 1,\n",
    "        dilation: int = 1,\n",
    "        start_frame: int = 0,\n",
    "        end_frame: int = 126,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.start_frame = start_frame\n",
    "        self.end_frame = end_frame\n",
    "        self.frames = list(range(self.start_frame, self.end_frame))\n",
    "        self.mode = \"sequential\"\n",
    "        self.prev_last_frame_idx = self.start_frame\n",
    "        self.kernal_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "\n",
    "    def frame_idxs_iter(self):\n",
    "        frame_idxs = list(range(self.start_frame, self.end_frame + 1, self.dilation))\n",
    "        for idx in range(0, len(frame_idxs), self.stride):\n",
    "            idxs = frame_idxs[idx : idx + self.kernal_size]\n",
    "            if len(idxs) == self.kernal_size:\n",
    "                yield idxs\n",
    "\n",
    "\n",
    "trainer = SequentialTrainer(\n",
    "    kernel_size=3,\n",
    "    stride=3,\n",
    "    dilation=2,\n",
    "    start_frame=0,\n",
    "    end_frame=10,\n",
    ")\n",
    "\n",
    "for frame_idxs in trainer.frame_idxs_iter():\n",
    "    print(frame_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = 2\n",
    "_A = torch.rand((N, N))\n",
    "A = _A.T @ _A  # positive semidefinite and symmetric\n",
    "b = torch.rand((N))\n",
    "x = torch.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.zeros(N, requires_grad=True)\n",
    "residual = ((A @ x0 - b) ** 2).sum()\n",
    "residual.backward()\n",
    "x0.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.zeros(N, requires_grad=True)\n",
    "error = ((x0 - x) ** 2).sum()\n",
    "error.backward()\n",
    "x0.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1.0, -1], [-1, 1]])\n",
    "A @ x.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = (x0 - x).sum()\n",
    "error.backward()\n",
    "x0.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guided",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
