# @package _global_

defaults:
  - model: flame 
  - logger: flame_wandb
  - data: dphm # synthetic, dphm
  - residuals: face2face 
  - optimizer: gauss_newton
  - framework : weighted_icp
  - correspondence: projective 
  - weighting: residual # dummy, residual
  - hydra: default
  - paths: default
  - _self_

seed: 123
device: cuda
task_name: train
tags: 
  - ${task_name}

eval: False
train: True

weighting:
  hidden_channels: 100
  kernal_size: 3
  num_layers: 1
  mode: point_normal
  max_weight: 100.0

data:
  train_dataset:
    start_frame: null
    end_frame: null
    jump_size: 1
    mode: fix
    datasets: 
      - ali_kocal_mouthmove
      - ali_kocal_rotatemouth
      - aria_talebizadeh_mouthmove
      - aria_talebizadeh_rotatemouth
      - arnefucks_mouthmove
      - arnefucks_rotatemouth
      - changluo_rotatemouth
      - christoph_mouthmove
      - elias_wohlgemuth_mouthmove
      - felix_mouthmove
      - honglixu_mouthmove
      - honglixu_rotatemouth
      - innocenzo_fulgintl_mouthmove
      - leni_rohe_mouthmove
      - leni_rohe_rotatemouth
      - madhav_agarwal_mouthmove
      - madhav_agarwal_rotatemouth
  val_dataset:
    start_frame: 100
    end_frame: 20
    jump_size: 1
    mode: fix
    datasets: 
      - innocenzo_fulgintl_rotatemouth

framework:
  lr: 8e-04
  max_iters: 1
  max_optims: 1
  param_weight: 1.0
  geometric_weight: 1.0
  log_frame_idx: 50
  log_interval: 20
  params:
    global_pose: 100.0
    transl: 500.0
    neck_pose: 20.0
    expression_params: 1.0
  verbose: False

trainer:
  _target_: lightning.pytorch.trainer.Trainer
  default_root_dir: ${paths.output_dir}
  accelerator: ${device}
  # training
  min_epochs: 1 
  max_epochs: 100 
  # debugging
  # overfit_batches: 8
  accumulate_grad_batches: 16
  num_sanity_val_steps: 0
  check_val_every_n_epoch: 1
  log_every_n_steps: 1