# @package _global_

defaults:
  - model: flame 
  - logger: flame_wandb
  - data: kinect_train   # synthetic, kinect_train
  - residuals: face2face 
  - optimizer: gauss_newton
  - framework : weighted_icp
  - correspondence: projective 
  - weighting: residual # dummy, residual
  - hydra: default
  - paths: default
  - _self_

seed: 123
device: cuda
task_name: train
tags: 
  - ${task_name}

eval: False
train: True

# eval: True
# train: False

data:
  dataset_name: dphm_christoph_mouthmove  # dphm_christoph_mouthmove, dphm_fulgitl_rotatemouth
  train_dataset:
    jump_size: 2
    mode: fix
    start_frame: 20
    end_frame: 100
  val_dataset:
    jump_size: 1
    mode: fix
    start_frame: 110
    end_frame: 120

framework:
  lr: 5e-03  # 3e-04
  max_iters: 1
  max_optims: 1
  param_weight: 1.0
  geometric_weight: 1.0
  log_frame_idx: 52
  log_interval: 20
  params:
    global_pose: 100.0
    transl: 500.0
    neck_pose: 20.0
    expression_params: 1.0
  verbose: False

trainer:
  _target_: lightning.pytorch.trainer.Trainer
  default_root_dir: ${paths.output_dir}
  accelerator: ${device}
  # training
  min_epochs: 1 
  max_epochs: 2000
  # debugging
  # overfit_batches: 32
  accumulate_grad_batches: 16
  num_sanity_val_steps: 0
  check_val_every_n_epoch: 1
  log_every_n_steps: 1