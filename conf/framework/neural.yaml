# @package framework 

_target_: lib.optimizer.framework.NeuralOptimizer

flame: ???
logger: ???
renderer: ???
correspondence: ???
residuals: ???
optimizer: ???
weighting: ???

# optimization settings
max_iters: 1
max_optims: 1

# loss settings
lr: 1e-03
residual_weight: 0.1
geometric_weight: 0.0
param_weight: 1.0
vertices_weight: 0.0
params:
  global_pose: 1.0
  transl: 1.0
  neck_pose: 1.0
  expression_params: 1.0

# logging settings
log_train_frame_idx: ${data.train_dataset.log_frame_idx} 
log_train_dataset: ${data.train_dataset.log_dataset} 
log_train_interval: ${data.train_dataset.log_interval} 
log_val_frame_idx: ${data.val_dataset.log_frame_idx} 
log_val_dataset: ${data.val_dataset.log_dataset} 
log_val_interval: ${data.val_dataset.log_frame_idx} 
verbose: False

# training settings
monitor: train/loss
# scheduler: null
scheduler:
  _partial_: True
  _target_: torch.optim.lr_scheduler.LinearLR
  total_iters: 500 
  start_factor: 1.0
  end_factor: 1e-01
train_optimizer:
  _target_: torch.optim.Adam 
  _partial_: True
  lr: ${framework.lr}

